{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fxSvDoSPQC1Z",
        "outputId": "99a3c9d5-3243-45a3-a7e8-353351be2023"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zwiCtQeqQ1yY",
        "outputId": "889bd2a8-e0b3-4200-ce02-f0afe5581f01"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.layers as layers\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "#随机旋转，裁切等数据增强\n",
        "def data_enhace(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_hue(x, max_delta=0.2)\n",
        "    x = tf.image.random_brightness(x, max_delta=0.7)\n",
        "    result = tf.image.random_contrast(x, lower=0.2, upper=1.8)\n",
        "    return result\n",
        "\n",
        "#mixup数据增强\n",
        "def criterion(batch_x, batch_y, batch_size,alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    index = tf.random_shuffle(batch_size)\n",
        "    mixed_batchx = lam * batch_x + (1 - lam) * batch_x[index, :]\n",
        "\n",
        "    batch_y_= lam*batch_y+(1-lam)*batch_y[index]\n",
        "    return mixed_batchx,batch_y_\n",
        "\n",
        "#本地载入cifar10数据集\n",
        "def load_cifar10_batch(cifar10_dataset_folder_path,batch_id):\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id),mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding = 'latin1')\n",
        "\n",
        "    # features and labels\n",
        "    features = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return  features, labels\n",
        "\n",
        "#在线载入cifar10数据集\n",
        "def load_cifar10_batch_online():\n",
        "    (x_Train, y_Train), (x_Test, y_Test)=cifar10.load_data()\n",
        "    return x_Train,y_Train,x_Test,y_Test\n",
        "\n",
        "#cifar10数据集读取器\n",
        "class Cifar10DataReader():\n",
        "    def __init__(self, cifar_folder=None, onehot=True):\n",
        "        self.cifar_folder = cifar_folder\n",
        "        self.onehot = onehot\n",
        "        self.data_label_train = None\n",
        "        self.data_label_test = None\n",
        "        self.batch_index = 0\n",
        "\n",
        "        if cifar_folder==None:#在线\n",
        "            x_train, y_train,x_test,y_test=load_cifar10_batch_online()\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            self.data_label_test = list(zip(x_test, y_test))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "        else:#本地\n",
        "            x_train, y_train = load_cifar10_batch(self.cifar_folder, 1)\n",
        "            for i in range(2,6):\n",
        "                features,labels = load_cifar10_batch(self.cifar_folder, i)\n",
        "                x_train, y_train = np.concatenate([x_train, features]),np.concatenate([y_train, labels])\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "    #批次读取训练集数据\n",
        "    def next_train_data(self, batch_size=100):\n",
        "        if self.batch_index < len(self.data_label_train) // batch_size:\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有训练集数据\n",
        "    def all_train_data(self):\n",
        "        np.random.shuffle(self.data_label_train)\n",
        "        datum = self.data_label_train\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #独热编码\n",
        "    def _decode(self, datum, onehot):\n",
        "        rdata = list()\n",
        "        rlabel = list()\n",
        "        if onehot:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                hot = np.zeros(10)\n",
        "                hot[int(l)] = 1\n",
        "                rlabel.append(hot)\n",
        "        else:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                rlabel.append(int(l))\n",
        "        return rdata, rlabel\n",
        "\n",
        "    #批次读取测试集数据\n",
        "    def next_test_data(self, batch_size=100):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test[0:batch_size]\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有测试集数据\n",
        "    def all_test_data(self):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "\n",
        "#网络结构\n",
        "def weight_variable(shape):\n",
        "    weights = tf.get_variable(\"weights\", shape, initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    #加入l2正则化\n",
        "    regular=tf.multiply(tf.nn.l2_loss(weights),0.005,name='regular')\n",
        "    tf.add_to_collection('losses',regular)\n",
        "    return weights\n",
        "def bias_variable(shape):\n",
        "    biases = tf.get_variable(\"biases\", shape, initializer=tf.constant_initializer(0.0))\n",
        "    return biases\n",
        "def conv_layer(x, w, b, name, strides, padding = 'SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = weight_variable(w)\n",
        "        b = bias_variable(b)\n",
        "        conv_and_biased = tf.nn.conv2d(x, w, strides = strides, padding = padding, name = name) + b\n",
        "    return conv_and_biased\n",
        "def batch_normalization(inputs, scope, is_training=True, need_relu=True):\n",
        "    bn = tf.contrib.layers.batch_norm(inputs,\n",
        "        decay=0.999,\n",
        "        center=True,\n",
        "        scale=True,  # 可以让学生实验True和False的区别。\n",
        "        epsilon=0.001,\n",
        "        activation_fn=None,\n",
        "        param_initializers=None,\n",
        "        param_regularizers=None,\n",
        "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
        "        is_training=is_training,  # 可以让学生实验True和False的区别。\n",
        "        reuse=None,\n",
        "        variables_collections=None,\n",
        "        outputs_collections=None,\n",
        "        trainable=True,\n",
        "        batch_weights=None,\n",
        "        fused=False,\n",
        "        data_format='NHWC',\n",
        "        zero_debias_moving_mean=False,\n",
        "        scope=scope,\n",
        "        renorm=False,\n",
        "        renorm_clipping=None,\n",
        "        renorm_decay=0.99)\n",
        "    if need_relu:\n",
        "        bn = tf.nn.relu(bn, name='relu')\n",
        "    return bn\n",
        "def maxpooling(x,kernal_size, strides, name):  # 最大池化，前面的是核大小，一般为[1, 2, 2, 1]，后面的strides指的是步长，如[1, 2, 2, 1]。\n",
        "    return tf.nn.max_pool(x, ksize=kernal_size, strides=strides, padding='SAME',name = name)\n",
        "def avg_pool(input_feats, k):\n",
        "    ksize = [1, k, k, 1]\n",
        "    strides = [1, k, k, 1]\n",
        "    padding = 'VALID'\n",
        "    output = tf.nn.avg_pool(input_feats, ksize, strides, padding)  # 平均池化\n",
        "    return output\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, train_flag,stride2=False):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters  # nb_filter1/2/3分别是64/64/256，三个整数。\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'  # 'res2a_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'  # 'bn2a_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    if stride2==False:\n",
        "        stride_for_first_conv = [1, 1, 1, 1]  # 。\n",
        "    else:\n",
        "        stride_for_first_conv = [1, 2, 2, 1]\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    shortcut = conv_layer(input_tensor, [1, 1, input_layers, nb_filter3], [nb_filter3], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '1')\n",
        "    shortcut = batch_normalization(shortcut, scope=bn_name_base + '1', is_training=train_flag)\n",
        "    x = tf.add(x, shortcut)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block, train_flag):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x =  batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    x = tf.add(x, input_tensor)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def resnet_graph(input_image, architecture,train_flag=True, stage5=False,ttf=True):  # 残差网络函数\n",
        "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
        "    # Stage 1\n",
        "    paddings = tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])  # 上下左右各补3个0。\n",
        "    x = tf.pad(input_image, paddings, \"CONSTANT\")\n",
        "    w = weight_variable([7, 7, 3, 64])\n",
        "    b = bias_variable([64])  #\n",
        "    x = tf.nn.conv2d(x, w, strides = [1, 2, 2, 1], padding = 'VALID', name = 'conv_1') + b\n",
        "    x = batch_normalization(x, scope='bn_conv1', is_training=train_flag, need_relu=True)\n",
        "    C1 = x = maxpooling(x, [1, 3, 3, 1], [1, 2, 2, 1], name='stage1')\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', train_flag=train_flag)  # 结构块。\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_flag=train_flag)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_flag=train_flag, stride2=True)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_flag=train_flag)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_flag=train_flag)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_flag=train_flag, stride2=True)\n",
        "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  # block_count=22，即，下句for循环的range是0~22，正好是加23个层。\n",
        "    for i in range(block_count):  # 加22个层，都是用identity_block函数加。\n",
        "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_flag=train_flag)\n",
        "    C4 = x  #\n",
        "    # Stage 5\n",
        "    if stage5:\n",
        "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_flag=train_flag, stride2=True)\n",
        "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_flag=train_flag)\n",
        "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_flag=train_flag)\n",
        "        output=avg_pool(C5,1)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)#加入dropout\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "    else:\n",
        "        C5 = None\n",
        "        output=avg_pool(C4,2)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "\n",
        "    return [C1, C2, C3, C4, C5,output]\n",
        "\n",
        "\n",
        "\n",
        "filepath='C:/Users/zy109/Desktop/1/data/cifar-10-batches-py'\n",
        "model_save_path='C:/Users/zy109/Desktop/1/cifar10'\n",
        "data=Cifar10DataReader()\n",
        "sess=tf.InteractiveSession()\n",
        "\n",
        "#指数衰减学习率\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(learning_rate=0.001, global_step=global_step, decay_steps=50, decay_rate=0.99, staircase=False)\n",
        "\n",
        "#占位符\n",
        "input_image=tf.placeholder(tf.float32,[None,32,32,3])\n",
        "y_=tf.placeholder(tf.float32,[None,10])\n",
        "tf_is_train=tf.placeholder(tf.bool,None)\n",
        "input_image=data_enhace(input_image)#数据增强\n",
        "input_image,y_=criterion(input_image,y_,batch_size)#mixup\n",
        "\n",
        "[C1,C2,C3,C4,C5,output]=resnet_graph(input_image,'resnet50',ttf=tf_is_train)\n",
        "\n",
        "#交叉熵损失\n",
        "cross_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_,logits=output))\n",
        "\n",
        "tf.add_to_collection('losses',cross_loss)\n",
        "loss=tf.add_n(tf.get_collection('losses'))\n",
        "train_step=tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "\n",
        "#正确率\n",
        "correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y_,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "sess.run(tf.initialize_all_variables())\n",
        "saver = tf.train.Saver()\n",
        "#读取已经训练好的权重\n",
        "ckpt = tf.train.get_checkpoint_state(model_save_path)\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "#加载测试集数据\n",
        "test_data=data.all_test_data()\n",
        "x_test=np.array(test_data[0])/255\n",
        "y_test=np.array(test_data[1])\n",
        "\n",
        "total_loss=[]\n",
        "total_acc=[]\n",
        "total_step=[]\n",
        "\n",
        "\n",
        "# 训练\n",
        "for i in range(20000):\n",
        "\n",
        "    #逐批加载训练集数据\n",
        "    train_data=data.next_train_data(batch_size)\n",
        "    x_train=np.array(train_data[0])/255\n",
        "    y_train=np.array(train_data[1])\n",
        "\n",
        "    train_step.run(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        # 测试准确率\n",
        "        train_accuracy = accuracy.eval(feed_dict={input_image: x_train,y_: y_train,tf_is_train:False})\n",
        "        test_accuracy = accuracy.eval(feed_dict={input_image: x_test,y_: y_test,tf_is_train:False})\n",
        "        # 该次训练的损失\n",
        "        loss_value = loss.eval(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "        global_var=sess.run(global_step)\n",
        "        lr_val=sess.run(lr)\n",
        "        print(\"step %d, trainning accuracy， %g , testing accuracy， %g , loss %g  ,lr %g\" % (global_var, train_accuracy,test_accuracy, loss_value,lr_val))\n",
        "        total_step.append(i)\n",
        "        total_loss.append(loss_value)\n",
        "        total_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "save_path = saver.save(sess,\"./cifar10/model.ckpt\")\n",
        "print(\"save model:{0} Finished\".format(save_path))\n",
        "np.save(\"cifar10_step\",total_step)\n",
        "np.save(\"cifar10_loss\",total_loss)\n",
        "np.save(\"cifar10_acc\",total_acc)\n",
        "\n",
        "#测试\n",
        "y_log_predict=output.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "test_accuracy = accuracy.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "print(\"test accuracy %g\" % test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test.argmax(axis=1), y_log_predict.argmax(axis=1))\n",
        "\n",
        "\n",
        "for i in range(len(y_log_predict)):\n",
        "        max_value=max(y_log_predict[i])\n",
        "        for j in range(len(y_log_predict[i])):\n",
        "            if max_value==y_log_predict[i][j]:\n",
        "                y_log_predict[i][j]=1\n",
        "            else:\n",
        "                y_log_predict[i][j]=0\n",
        "\n",
        "# 精确率\n",
        "from sklearn.metrics import precision_score\n",
        "ps=precision_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_precision_score\",ps)\n",
        "\n",
        "#召回率\n",
        "from sklearn.metrics import recall_score\n",
        "rs=recall_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_recall_score\",rs)\n",
        "\n",
        "print(\"precision score:\",ps)\n",
        "print(\"recall score:\",rs)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "ax1.plot(total_step, total_loss, color=\"blue\", label=\"loss\")\n",
        "ax1.set_xlabel(\"step\")\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_step, total_acc, color=\"red\", label=\"accuary\")\n",
        "ax2.set_ylabel(\"accuary\")\n",
        "\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:650: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-aa76e5cc59b9>:248: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-aa76e5cc59b9>:279: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "step 1, trainning accuracy， 0.113281 , testing accuracy， 0.1019 , loss 81.2704  ,lr 0.000999799\n",
            "step 101, trainning accuracy， 0.382812 , testing accuracy， 0.3214 , loss 51.2619  ,lr 0.000979903\n",
            "step 201, trainning accuracy， 0.363281 , testing accuracy， 0.366 , loss 30.4797  ,lr 0.000960403\n",
            "step 301, trainning accuracy， 0.394531 , testing accuracy， 0.4058 , loss 20.1196  ,lr 0.000941291\n",
            "step 401, trainning accuracy， 0.496094 , testing accuracy， 0.4217 , loss 13.9808  ,lr 0.000922559\n",
            "step 501, trainning accuracy， 0.441406 , testing accuracy， 0.4346 , loss 10.9794  ,lr 0.0009042\n",
            "step 601, trainning accuracy， 0.441406 , testing accuracy， 0.473 , loss 8.78181  ,lr 0.000886207\n",
            "step 701, trainning accuracy， 0.523438 , testing accuracy， 0.5012 , loss 7.0633  ,lr 0.000868571\n",
            "step 801, trainning accuracy， 0.507812 , testing accuracy， 0.5066 , loss 6.36174  ,lr 0.000851287\n",
            "step 901, trainning accuracy， 0.566406 , testing accuracy， 0.5057 , loss 5.48804  ,lr 0.000834346\n",
            "step 1001, trainning accuracy， 0.59375 , testing accuracy， 0.5457 , loss 5.14836  ,lr 0.000817743\n",
            "step 1101, trainning accuracy， 0.578125 , testing accuracy， 0.5645 , loss 4.41168  ,lr 0.00080147\n",
            "step 1201, trainning accuracy， 0.582031 , testing accuracy， 0.5767 , loss 3.97648  ,lr 0.00078552\n",
            "step 1301, trainning accuracy， 0.65625 , testing accuracy， 0.6057 , loss 3.68495  ,lr 0.000769889\n",
            "step 1401, trainning accuracy， 0.625 , testing accuracy， 0.5857 , loss 3.63598  ,lr 0.000754568\n",
            "step 1501, trainning accuracy， 0.601562 , testing accuracy， 0.5995 , loss 3.15671  ,lr 0.000739552\n",
            "step 1601, trainning accuracy， 0.667969 , testing accuracy， 0.6213 , loss 2.89448  ,lr 0.000724835\n",
            "step 1701, trainning accuracy， 0.671875 , testing accuracy， 0.6367 , loss 2.7349  ,lr 0.000710411\n",
            "step 1801, trainning accuracy， 0.734375 , testing accuracy， 0.6502 , loss 2.44053  ,lr 0.000696274\n",
            "step 1901, trainning accuracy， 0.667969 , testing accuracy， 0.65 , loss 2.38529  ,lr 0.000682418\n",
            "step 2001, trainning accuracy， 0.65625 , testing accuracy， 0.6537 , loss 2.37497  ,lr 0.000668838\n",
            "step 2101, trainning accuracy， 0.738281 , testing accuracy， 0.67 , loss 2.08469  ,lr 0.000655528\n",
            "step 2201, trainning accuracy， 0.636719 , testing accuracy， 0.5993 , loss 2.41362  ,lr 0.000642483\n",
            "step 2301, trainning accuracy， 0.664062 , testing accuracy， 0.6509 , loss 2.16494  ,lr 0.000629697\n",
            "step 2401, trainning accuracy， 0.621094 , testing accuracy， 0.6442 , loss 2.34215  ,lr 0.000617166\n",
            "step 2501, trainning accuracy， 0.664062 , testing accuracy， 0.6667 , loss 2.10383  ,lr 0.000604885\n",
            "step 2601, trainning accuracy， 0.714844 , testing accuracy， 0.6658 , loss 2.02345  ,lr 0.000592848\n",
            "step 2701, trainning accuracy， 0.691406 , testing accuracy， 0.638 , loss 2.01158  ,lr 0.00058105\n",
            "step 2801, trainning accuracy， 0.703125 , testing accuracy， 0.6841 , loss 1.90062  ,lr 0.000569487\n",
            "step 2901, trainning accuracy， 0.78125 , testing accuracy， 0.7182 , loss 1.63946  ,lr 0.000558154\n",
            "step 3001, trainning accuracy， 0.769531 , testing accuracy， 0.7079 , loss 1.66269  ,lr 0.000547047\n",
            "step 3101, trainning accuracy， 0.742188 , testing accuracy， 0.7052 , loss 1.76581  ,lr 0.000536161\n",
            "step 3201, trainning accuracy， 0.761719 , testing accuracy， 0.7028 , loss 1.61709  ,lr 0.000525491\n",
            "step 3301, trainning accuracy， 0.78125 , testing accuracy， 0.7324 , loss 1.4841  ,lr 0.000515034\n",
            "step 3401, trainning accuracy， 0.796875 , testing accuracy， 0.7299 , loss 1.33211  ,lr 0.000504785\n",
            "step 3501, trainning accuracy， 0.832031 , testing accuracy， 0.7359 , loss 1.34708  ,lr 0.00049474\n",
            "step 3601, trainning accuracy， 0.730469 , testing accuracy， 0.702 , loss 1.66499  ,lr 0.000484894\n",
            "step 3701, trainning accuracy， 0.769531 , testing accuracy， 0.7246 , loss 1.60818  ,lr 0.000475245\n",
            "step 3801, trainning accuracy， 0.78125 , testing accuracy， 0.7333 , loss 1.37543  ,lr 0.000465788\n",
            "step 3901, trainning accuracy， 0.734375 , testing accuracy， 0.6933 , loss 1.7374  ,lr 0.000456518\n",
            "step 4001, trainning accuracy， 0.71875 , testing accuracy， 0.7192 , loss 1.70919  ,lr 0.000447434\n",
            "step 4101, trainning accuracy， 0.804688 , testing accuracy， 0.7307 , loss 1.43749  ,lr 0.00043853\n",
            "step 4201, trainning accuracy， 0.808594 , testing accuracy， 0.75 , loss 1.25478  ,lr 0.000429803\n",
            "step 4301, trainning accuracy， 0.8125 , testing accuracy， 0.7536 , loss 1.27749  ,lr 0.00042125\n",
            "step 4401, trainning accuracy， 0.820312 , testing accuracy， 0.7418 , loss 1.34743  ,lr 0.000412867\n",
            "step 4501, trainning accuracy， 0.808594 , testing accuracy， 0.7451 , loss 1.31697  ,lr 0.000404651\n",
            "step 4601, trainning accuracy， 0.808594 , testing accuracy， 0.7516 , loss 1.24077  ,lr 0.000396598\n",
            "step 4701, trainning accuracy， 0.804688 , testing accuracy， 0.7498 , loss 1.29023  ,lr 0.000388706\n",
            "step 4801, trainning accuracy， 0.828125 , testing accuracy， 0.7598 , loss 1.19056  ,lr 0.000380971\n",
            "step 4901, trainning accuracy， 0.863281 , testing accuracy， 0.7644 , loss 1.05661  ,lr 0.00037339\n",
            "step 5001, trainning accuracy， 0.851562 , testing accuracy， 0.7678 , loss 1.05686  ,lr 0.000365959\n",
            "step 5101, trainning accuracy， 0.902344 , testing accuracy， 0.7633 , loss 0.931543  ,lr 0.000358677\n",
            "step 5201, trainning accuracy， 0.878906 , testing accuracy， 0.7645 , loss 0.936834  ,lr 0.000351539\n",
            "step 5301, trainning accuracy， 0.90625 , testing accuracy， 0.7661 , loss 0.862651  ,lr 0.000344543\n",
            "step 5401, trainning accuracy， 0.855469 , testing accuracy， 0.7753 , loss 1.0337  ,lr 0.000337687\n",
            "step 5501, trainning accuracy， 0.859375 , testing accuracy， 0.7746 , loss 0.923486  ,lr 0.000330967\n",
            "step 5601, trainning accuracy， 0.84375 , testing accuracy， 0.7644 , loss 0.972498  ,lr 0.000324381\n",
            "step 5701, trainning accuracy， 0.871094 , testing accuracy， 0.7521 , loss 1.0504  ,lr 0.000317926\n",
            "step 5801, trainning accuracy， 0.84375 , testing accuracy， 0.7616 , loss 1.07171  ,lr 0.000311599\n",
            "step 5901, trainning accuracy， 0.886719 , testing accuracy， 0.7675 , loss 0.983785  ,lr 0.000305398\n",
            "step 6001, trainning accuracy， 0.851562 , testing accuracy， 0.7695 , loss 0.991543  ,lr 0.000299321\n",
            "step 6101, trainning accuracy， 0.929688 , testing accuracy， 0.7796 , loss 0.819966  ,lr 0.000293364\n",
            "step 6201, trainning accuracy， 0.867188 , testing accuracy， 0.7757 , loss 0.871737  ,lr 0.000287526\n",
            "step 6301, trainning accuracy， 0.953125 , testing accuracy， 0.7772 , loss 0.732729  ,lr 0.000281804\n",
            "step 6401, trainning accuracy， 0.917969 , testing accuracy， 0.7787 , loss 0.857042  ,lr 0.000276196\n",
            "step 6501, trainning accuracy， 0.929688 , testing accuracy， 0.7788 , loss 0.792086  ,lr 0.0002707\n",
            "step 6601, trainning accuracy， 0.945312 , testing accuracy， 0.7835 , loss 0.755926  ,lr 0.000265313\n",
            "step 6701, trainning accuracy， 0.933594 , testing accuracy， 0.7683 , loss 0.756038  ,lr 0.000260034\n",
            "step 6801, trainning accuracy， 0.910156 , testing accuracy， 0.7831 , loss 0.748463  ,lr 0.000254859\n",
            "step 6901, trainning accuracy， 0.902344 , testing accuracy， 0.7781 , loss 0.832976  ,lr 0.000249787\n",
            "step 7001, trainning accuracy， 0.941406 , testing accuracy， 0.7802 , loss 0.684016  ,lr 0.000244816\n",
            "step 7101, trainning accuracy， 0.917969 , testing accuracy， 0.7764 , loss 0.806513  ,lr 0.000239945\n",
            "step 7201, trainning accuracy， 0.9375 , testing accuracy， 0.785 , loss 0.70198  ,lr 0.00023517\n",
            "step 7301, trainning accuracy， 0.921875 , testing accuracy， 0.7823 , loss 0.663742  ,lr 0.00023049\n",
            "step 7401, trainning accuracy， 0.945312 , testing accuracy， 0.7768 , loss 0.77353  ,lr 0.000225903\n",
            "step 7501, trainning accuracy， 0.953125 , testing accuracy， 0.7826 , loss 0.729364  ,lr 0.000221408\n",
            "step 7601, trainning accuracy， 0.933594 , testing accuracy， 0.7826 , loss 0.677198  ,lr 0.000217002\n",
            "step 7701, trainning accuracy， 0.964844 , testing accuracy， 0.785 , loss 0.666099  ,lr 0.000212683\n",
            "step 7801, trainning accuracy， 0.9375 , testing accuracy， 0.7617 , loss 0.787505  ,lr 0.000208451\n",
            "step 7901, trainning accuracy， 0.980469 , testing accuracy， 0.782 , loss 0.60832  ,lr 0.000204303\n",
            "step 8001, trainning accuracy， 0.960938 , testing accuracy， 0.7795 , loss 0.676018  ,lr 0.000200237\n",
            "step 8101, trainning accuracy， 0.972656 , testing accuracy， 0.7848 , loss 0.615602  ,lr 0.000196252\n",
            "step 8201, trainning accuracy， 0.960938 , testing accuracy， 0.7802 , loss 0.608023  ,lr 0.000192347\n",
            "step 8301, trainning accuracy， 0.980469 , testing accuracy， 0.7856 , loss 0.540706  ,lr 0.000188519\n",
            "step 8401, trainning accuracy， 0.980469 , testing accuracy， 0.7823 , loss 0.531674  ,lr 0.000184768\n",
            "step 8501, trainning accuracy， 0.96875 , testing accuracy， 0.7802 , loss 0.577011  ,lr 0.000181091\n",
            "step 8601, trainning accuracy， 0.972656 , testing accuracy， 0.7791 , loss 0.574459  ,lr 0.000177487\n",
            "step 8701, trainning accuracy， 0.980469 , testing accuracy， 0.7806 , loss 0.535182  ,lr 0.000173955\n",
            "step 8801, trainning accuracy， 0.988281 , testing accuracy， 0.784 , loss 0.558762  ,lr 0.000170493\n",
            "step 8901, trainning accuracy， 0.992188 , testing accuracy， 0.7775 , loss 0.484848  ,lr 0.000167101\n",
            "step 9001, trainning accuracy， 0.988281 , testing accuracy， 0.786 , loss 0.554183  ,lr 0.000163775\n",
            "step 9101, trainning accuracy， 0.957031 , testing accuracy， 0.7769 , loss 0.626357  ,lr 0.000160516\n",
            "step 9201, trainning accuracy， 0.992188 , testing accuracy， 0.7846 , loss 0.512539  ,lr 0.000157322\n",
            "step 9301, trainning accuracy， 0.984375 , testing accuracy， 0.7818 , loss 0.499534  ,lr 0.000154191\n",
            "step 9401, trainning accuracy， 0.992188 , testing accuracy， 0.7811 , loss 0.456591  ,lr 0.000151123\n",
            "step 9501, trainning accuracy， 0.964844 , testing accuracy， 0.7708 , loss 0.621592  ,lr 0.000148115\n",
            "step 9601, trainning accuracy， 0.992188 , testing accuracy， 0.7837 , loss 0.470567  ,lr 0.000145168\n",
            "step 9701, trainning accuracy， 0.980469 , testing accuracy， 0.781 , loss 0.478986  ,lr 0.000142279\n",
            "step 9801, trainning accuracy， 0.984375 , testing accuracy， 0.7839 , loss 0.485953  ,lr 0.000139448\n",
            "step 9901, trainning accuracy， 0.980469 , testing accuracy， 0.776 , loss 0.494134  ,lr 0.000136673\n",
            "step 10001, trainning accuracy， 0.996094 , testing accuracy， 0.7833 , loss 0.450177  ,lr 0.000133953\n",
            "step 10101, trainning accuracy， 0.988281 , testing accuracy， 0.7786 , loss 0.454854  ,lr 0.000131287\n",
            "step 10201, trainning accuracy， 0.984375 , testing accuracy， 0.7829 , loss 0.450206  ,lr 0.000128675\n",
            "step 10301, trainning accuracy， 0.984375 , testing accuracy， 0.7809 , loss 0.472458  ,lr 0.000126114\n",
            "step 10401, trainning accuracy， 0.996094 , testing accuracy， 0.7825 , loss 0.429128  ,lr 0.000123604\n",
            "step 10501, trainning accuracy， 0.980469 , testing accuracy， 0.7872 , loss 0.443424  ,lr 0.000121145\n",
            "step 10601, trainning accuracy， 1 , testing accuracy， 0.7857 , loss 0.412657  ,lr 0.000118734\n",
            "step 10701, trainning accuracy， 1 , testing accuracy， 0.7832 , loss 0.406475  ,lr 0.000116371\n",
            "step 10801, trainning accuracy， 0.996094 , testing accuracy， 0.7847 , loss 0.42887  ,lr 0.000114055\n",
            "step 10901, trainning accuracy， 1 , testing accuracy， 0.7832 , loss 0.396573  ,lr 0.000111786\n",
            "step 11001, trainning accuracy， 0.996094 , testing accuracy， 0.7794 , loss 0.405029  ,lr 0.000109561\n",
            "step 11101, trainning accuracy， 1 , testing accuracy， 0.7867 , loss 0.389456  ,lr 0.000107381\n",
            "step 11201, trainning accuracy， 1 , testing accuracy， 0.7861 , loss 0.395445  ,lr 0.000105244\n",
            "step 11301, trainning accuracy， 1 , testing accuracy， 0.7872 , loss 0.402779  ,lr 0.00010315\n",
            "step 11401, trainning accuracy， 0.996094 , testing accuracy， 0.7866 , loss 0.388987  ,lr 0.000101097\n",
            "step 11501, trainning accuracy， 0.996094 , testing accuracy， 0.7838 , loss 0.429692  ,lr 9.90851e-05\n",
            "step 11601, trainning accuracy， 1 , testing accuracy， 0.7847 , loss 0.377026  ,lr 9.71133e-05\n",
            "step 11701, trainning accuracy， 1 , testing accuracy， 0.7834 , loss 0.360474  ,lr 9.51808e-05\n",
            "step 11801, trainning accuracy， 0.996094 , testing accuracy， 0.7858 , loss 0.374807  ,lr 9.32867e-05\n",
            "step 11901, trainning accuracy， 1 , testing accuracy， 0.7875 , loss 0.373267  ,lr 9.14303e-05\n",
            "step 12001, trainning accuracy， 0.992188 , testing accuracy， 0.7872 , loss 0.378496  ,lr 8.96108e-05\n",
            "step 12101, trainning accuracy， 1 , testing accuracy， 0.7872 , loss 0.355624  ,lr 8.78276e-05\n",
            "step 12201, trainning accuracy， 1 , testing accuracy， 0.7829 , loss 0.403312  ,lr 8.60798e-05\n",
            "step 12301, trainning accuracy， 1 , testing accuracy， 0.7843 , loss 0.338362  ,lr 8.43668e-05\n",
            "step 12401, trainning accuracy， 1 , testing accuracy， 0.7882 , loss 0.33549  ,lr 8.26879e-05\n",
            "step 12501, trainning accuracy， 0.996094 , testing accuracy， 0.785 , loss 0.393078  ,lr 8.10424e-05\n",
            "step 12601, trainning accuracy， 0.992188 , testing accuracy， 0.7844 , loss 0.347338  ,lr 7.94297e-05\n",
            "step 12701, trainning accuracy， 1 , testing accuracy， 0.7854 , loss 0.332911  ,lr 7.7849e-05\n",
            "step 12801, trainning accuracy， 1 , testing accuracy， 0.7865 , loss 0.358263  ,lr 7.62998e-05\n",
            "step 12901, trainning accuracy， 1 , testing accuracy， 0.7849 , loss 0.340381  ,lr 7.47815e-05\n",
            "step 13001, trainning accuracy， 1 , testing accuracy， 0.787 , loss 0.335769  ,lr 7.32933e-05\n",
            "step 13101, trainning accuracy， 1 , testing accuracy， 0.7859 , loss 0.316277  ,lr 7.18348e-05\n",
            "step 13201, trainning accuracy， 1 , testing accuracy， 0.7876 , loss 0.319322  ,lr 7.04053e-05\n",
            "step 13301, trainning accuracy， 1 , testing accuracy， 0.7935 , loss 0.339043  ,lr 6.90042e-05\n",
            "step 13401, trainning accuracy， 0.996094 , testing accuracy， 0.7863 , loss 0.36037  ,lr 6.7631e-05\n",
            "step 13501, trainning accuracy， 1 , testing accuracy， 0.7826 , loss 0.319404  ,lr 6.62852e-05\n",
            "step 13601, trainning accuracy， 1 , testing accuracy， 0.7876 , loss 0.310397  ,lr 6.49661e-05\n",
            "step 13701, trainning accuracy， 0.992188 , testing accuracy， 0.7887 , loss 0.331712  ,lr 6.36733e-05\n",
            "step 13801, trainning accuracy， 1 , testing accuracy， 0.7863 , loss 0.312268  ,lr 6.24062e-05\n",
            "step 13901, trainning accuracy， 1 , testing accuracy， 0.787 , loss 0.339516  ,lr 6.11643e-05\n",
            "step 14001, trainning accuracy， 0.996094 , testing accuracy， 0.7858 , loss 0.30996  ,lr 5.99471e-05\n",
            "step 14101, trainning accuracy， 1 , testing accuracy， 0.7865 , loss 0.307317  ,lr 5.87542e-05\n",
            "step 14201, trainning accuracy， 1 , testing accuracy， 0.7839 , loss 0.31536  ,lr 5.7585e-05\n",
            "step 14301, trainning accuracy， 1 , testing accuracy， 0.7882 , loss 0.30145  ,lr 5.6439e-05\n",
            "step 14401, trainning accuracy， 1 , testing accuracy， 0.7892 , loss 0.303173  ,lr 5.53159e-05\n",
            "step 14501, trainning accuracy， 1 , testing accuracy， 0.7849 , loss 0.297059  ,lr 5.42151e-05\n",
            "step 14601, trainning accuracy， 1 , testing accuracy， 0.7881 , loss 0.292465  ,lr 5.31362e-05\n",
            "step 14701, trainning accuracy， 1 , testing accuracy， 0.7889 , loss 0.299817  ,lr 5.20788e-05\n",
            "step 14801, trainning accuracy， 1 , testing accuracy， 0.7906 , loss 0.307071  ,lr 5.10425e-05\n",
            "step 14901, trainning accuracy， 1 , testing accuracy， 0.7914 , loss 0.307252  ,lr 5.00267e-05\n",
            "step 15001, trainning accuracy， 1 , testing accuracy， 0.7896 , loss 0.294541  ,lr 4.90312e-05\n",
            "step 15101, trainning accuracy， 1 , testing accuracy， 0.788 , loss 0.323725  ,lr 4.80555e-05\n",
            "step 15201, trainning accuracy， 0.996094 , testing accuracy， 0.7885 , loss 0.305184  ,lr 4.70992e-05\n",
            "step 15301, trainning accuracy， 1 , testing accuracy， 0.7916 , loss 0.289634  ,lr 4.61619e-05\n",
            "step 15401, trainning accuracy， 1 , testing accuracy， 0.7911 , loss 0.302392  ,lr 4.52433e-05\n",
            "step 15501, trainning accuracy， 1 , testing accuracy， 0.7903 , loss 0.291265  ,lr 4.43429e-05\n",
            "step 15601, trainning accuracy， 1 , testing accuracy， 0.7907 , loss 0.341846  ,lr 4.34605e-05\n",
            "step 15701, trainning accuracy， 1 , testing accuracy， 0.789 , loss 0.284647  ,lr 4.25956e-05\n",
            "step 15801, trainning accuracy， 1 , testing accuracy， 0.7918 , loss 0.282443  ,lr 4.1748e-05\n",
            "step 15901, trainning accuracy， 1 , testing accuracy， 0.7891 , loss 0.275561  ,lr 4.09172e-05\n",
            "step 16001, trainning accuracy， 1 , testing accuracy， 0.7915 , loss 0.29772  ,lr 4.0103e-05\n",
            "step 16101, trainning accuracy， 1 , testing accuracy， 0.7911 , loss 0.283445  ,lr 3.93049e-05\n",
            "step 16201, trainning accuracy， 1 , testing accuracy， 0.7893 , loss 0.274315  ,lr 3.85227e-05\n",
            "step 16301, trainning accuracy， 1 , testing accuracy， 0.7892 , loss 0.270312  ,lr 3.77561e-05\n",
            "step 16401, trainning accuracy， 1 , testing accuracy， 0.7876 , loss 0.285203  ,lr 3.70048e-05\n",
            "step 16501, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.283292  ,lr 3.62684e-05\n",
            "step 16601, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.283944  ,lr 3.55467e-05\n",
            "step 16701, trainning accuracy， 1 , testing accuracy， 0.7897 , loss 0.269185  ,lr 3.48393e-05\n",
            "step 16801, trainning accuracy， 1 , testing accuracy， 0.7942 , loss 0.266457  ,lr 3.4146e-05\n",
            "step 16901, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.272467  ,lr 3.34665e-05\n",
            "step 17001, trainning accuracy， 1 , testing accuracy， 0.7938 , loss 0.274483  ,lr 3.28005e-05\n",
            "step 17101, trainning accuracy， 1 , testing accuracy， 0.7918 , loss 0.282718  ,lr 3.21478e-05\n",
            "step 17201, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.259916  ,lr 3.1508e-05\n",
            "step 17301, trainning accuracy， 1 , testing accuracy， 0.7944 , loss 0.258523  ,lr 3.0881e-05\n",
            "step 17401, trainning accuracy， 1 , testing accuracy， 0.7923 , loss 0.260893  ,lr 3.02665e-05\n",
            "step 17501, trainning accuracy， 1 , testing accuracy， 0.7941 , loss 0.258578  ,lr 2.96642e-05\n",
            "step 17601, trainning accuracy， 1 , testing accuracy， 0.791 , loss 0.256528  ,lr 2.90739e-05\n",
            "step 17701, trainning accuracy， 1 , testing accuracy， 0.7907 , loss 0.260822  ,lr 2.84953e-05\n",
            "step 17801, trainning accuracy， 1 , testing accuracy， 0.7907 , loss 0.252376  ,lr 2.79282e-05\n",
            "step 17901, trainning accuracy， 1 , testing accuracy， 0.7915 , loss 0.257686  ,lr 2.73725e-05\n",
            "step 18001, trainning accuracy， 1 , testing accuracy， 0.7893 , loss 0.251157  ,lr 2.68278e-05\n",
            "step 18101, trainning accuracy， 1 , testing accuracy， 0.7908 , loss 0.252122  ,lr 2.62939e-05\n",
            "step 18201, trainning accuracy， 1 , testing accuracy， 0.7902 , loss 0.26604  ,lr 2.57706e-05\n",
            "step 18301, trainning accuracy， 1 , testing accuracy， 0.7907 , loss 0.252428  ,lr 2.52578e-05\n",
            "step 18401, trainning accuracy， 1 , testing accuracy， 0.7921 , loss 0.253276  ,lr 2.47552e-05\n",
            "step 18501, trainning accuracy， 1 , testing accuracy， 0.7931 , loss 0.254597  ,lr 2.42625e-05\n",
            "step 18601, trainning accuracy， 1 , testing accuracy， 0.7922 , loss 0.249679  ,lr 2.37797e-05\n",
            "step 18701, trainning accuracy， 1 , testing accuracy， 0.7887 , loss 0.26239  ,lr 2.33065e-05\n",
            "step 18801, trainning accuracy， 0.996094 , testing accuracy， 0.7952 , loss 0.246112  ,lr 2.28427e-05\n",
            "step 18901, trainning accuracy， 1 , testing accuracy， 0.7928 , loss 0.262609  ,lr 2.23881e-05\n",
            "step 19001, trainning accuracy， 1 , testing accuracy， 0.7926 , loss 0.242188  ,lr 2.19426e-05\n",
            "step 19101, trainning accuracy， 1 , testing accuracy， 0.7939 , loss 0.242185  ,lr 2.1506e-05\n",
            "step 19201, trainning accuracy， 1 , testing accuracy， 0.7939 , loss 0.240947  ,lr 2.1078e-05\n",
            "step 19301, trainning accuracy， 1 , testing accuracy， 0.7941 , loss 0.241699  ,lr 2.06585e-05\n",
            "step 19401, trainning accuracy， 1 , testing accuracy， 0.7967 , loss 0.239858  ,lr 2.02474e-05\n",
            "step 19501, trainning accuracy， 1 , testing accuracy， 0.7979 , loss 0.252742  ,lr 1.98445e-05\n",
            "step 19601, trainning accuracy， 1 , testing accuracy， 0.7938 , loss 0.240401  ,lr 1.94496e-05\n",
            "step 19701, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.241272  ,lr 1.90626e-05\n",
            "step 19801, trainning accuracy， 1 , testing accuracy， 0.7943 , loss 0.237256  ,lr 1.86832e-05\n",
            "step 19901, trainning accuracy， 1 , testing accuracy， 0.7957 , loss 0.246981  ,lr 1.83114e-05\n",
            "save model:./cifar10/model.ckpt Finished\n",
            "test accuracy 0.795\n",
            "precision score: [0.81854839 0.886      0.75553214 0.63645621 0.77037773 0.69979508\n",
            " 0.80864765 0.83991895 0.86717998 0.85845214]\n",
            "recall score: [0.812 0.886 0.717 0.625 0.775 0.683 0.879 0.829 0.901 0.843]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fyK7OIoQXy_A",
        "outputId": "a95de6f9-c2b9-4671-8a3c-e506415f9bee"
      },
      "source": [
        "# is_training=False 37min\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.layers as layers\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "#随机旋转，裁切等数据增强\n",
        "def data_enhace(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_hue(x, max_delta=0.2)\n",
        "    x = tf.image.random_brightness(x, max_delta=0.7)\n",
        "    result = tf.image.random_contrast(x, lower=0.2, upper=1.8)\n",
        "    return result\n",
        "\n",
        "#mixup数据增强\n",
        "def criterion(batch_x, batch_y, batch_size,alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    index = tf.random_shuffle(batch_size)\n",
        "    mixed_batchx = lam * batch_x + (1 - lam) * batch_x[index, :]\n",
        "\n",
        "    batch_y_= lam*batch_y+(1-lam)*batch_y[index]\n",
        "    return mixed_batchx,batch_y_\n",
        "\n",
        "#本地载入cifar10数据集\n",
        "def load_cifar10_batch(cifar10_dataset_folder_path,batch_id):\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id),mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding = 'latin1')\n",
        "\n",
        "    # features and labels\n",
        "    features = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return  features, labels\n",
        "\n",
        "#在线载入cifar10数据集\n",
        "def load_cifar10_batch_online():\n",
        "    (x_Train, y_Train), (x_Test, y_Test)=cifar10.load_data()\n",
        "    return x_Train,y_Train,x_Test,y_Test\n",
        "\n",
        "#cifar10数据集读取器\n",
        "class Cifar10DataReader():\n",
        "    def __init__(self, cifar_folder=None, onehot=True):\n",
        "        self.cifar_folder = cifar_folder\n",
        "        self.onehot = onehot\n",
        "        self.data_label_train = None\n",
        "        self.data_label_test = None\n",
        "        self.batch_index = 0\n",
        "\n",
        "        if cifar_folder==None:#在线\n",
        "            x_train, y_train,x_test,y_test=load_cifar10_batch_online()\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            self.data_label_test = list(zip(x_test, y_test))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "        else:#本地\n",
        "            x_train, y_train = load_cifar10_batch(self.cifar_folder, 1)\n",
        "            for i in range(2,6):\n",
        "                features,labels = load_cifar10_batch(self.cifar_folder, i)\n",
        "                x_train, y_train = np.concatenate([x_train, features]),np.concatenate([y_train, labels])\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "    #批次读取训练集数据\n",
        "    def next_train_data(self, batch_size=100):\n",
        "        if self.batch_index < len(self.data_label_train) // batch_size:\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有训练集数据\n",
        "    def all_train_data(self):\n",
        "        np.random.shuffle(self.data_label_train)\n",
        "        datum = self.data_label_train\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #独热编码\n",
        "    def _decode(self, datum, onehot):\n",
        "        rdata = list()\n",
        "        rlabel = list()\n",
        "        if onehot:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                hot = np.zeros(10)\n",
        "                hot[int(l)] = 1\n",
        "                rlabel.append(hot)\n",
        "        else:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                rlabel.append(int(l))\n",
        "        return rdata, rlabel\n",
        "\n",
        "    #批次读取测试集数据\n",
        "    def next_test_data(self, batch_size=100):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test[0:batch_size]\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有测试集数据\n",
        "    def all_test_data(self):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "\n",
        "#网络结构\n",
        "def weight_variable(shape):\n",
        "    weights = tf.get_variable(\"weights\", shape, initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    #加入l2正则化\n",
        "    regular=tf.multiply(tf.nn.l2_loss(weights),0.005,name='regular')\n",
        "    tf.add_to_collection('losses',regular)\n",
        "    return weights\n",
        "def bias_variable(shape):\n",
        "    biases = tf.get_variable(\"biases\", shape, initializer=tf.constant_initializer(0.0))\n",
        "    return biases\n",
        "def conv_layer(x, w, b, name, strides, padding = 'SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = weight_variable(w)\n",
        "        b = bias_variable(b)\n",
        "        conv_and_biased = tf.nn.conv2d(x, w, strides = strides, padding = padding, name = name) + b\n",
        "    return conv_and_biased\n",
        "def batch_normalization(inputs, scope, is_training=False, need_relu=True):\n",
        "    bn = tf.contrib.layers.batch_norm(inputs,\n",
        "        decay=0.999,\n",
        "        center=True,\n",
        "        scale=True,  # 可以让学生实验True和False的区别。\n",
        "        epsilon=0.001,\n",
        "        activation_fn=None,\n",
        "        param_initializers=None,\n",
        "        param_regularizers=None,\n",
        "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
        "        is_training=is_training,  # 可以让学生实验True和False的区别。\n",
        "        reuse=None,\n",
        "        variables_collections=None,\n",
        "        outputs_collections=None,\n",
        "        trainable=True,\n",
        "        batch_weights=None,\n",
        "        fused=False,\n",
        "        data_format='NHWC',\n",
        "        zero_debias_moving_mean=False,\n",
        "        scope=scope,\n",
        "        renorm=False,\n",
        "        renorm_clipping=None,\n",
        "        renorm_decay=0.99)\n",
        "    if need_relu:\n",
        "        bn = tf.nn.relu(bn, name='relu')\n",
        "    return bn\n",
        "def maxpooling(x,kernal_size, strides, name):  # 最大池化，前面的是核大小，一般为[1, 2, 2, 1]，后面的strides指的是步长，如[1, 2, 2, 1]。\n",
        "    return tf.nn.max_pool(x, ksize=kernal_size, strides=strides, padding='SAME',name = name)\n",
        "def avg_pool(input_feats, k):\n",
        "    ksize = [1, k, k, 1]\n",
        "    strides = [1, k, k, 1]\n",
        "    padding = 'VALID'\n",
        "    output = tf.nn.avg_pool(input_feats, ksize, strides, padding)  # 平均池化\n",
        "    return output\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, train_flag,stride2=False):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters  # nb_filter1/2/3分别是64/64/256，三个整数。\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'  # 'res2a_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'  # 'bn2a_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    if stride2==False:\n",
        "        stride_for_first_conv = [1, 1, 1, 1]  # 。\n",
        "    else:\n",
        "        stride_for_first_conv = [1, 2, 2, 1]\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    shortcut = conv_layer(input_tensor, [1, 1, input_layers, nb_filter3], [nb_filter3], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '1')\n",
        "    shortcut = batch_normalization(shortcut, scope=bn_name_base + '1', is_training=train_flag)\n",
        "    x = tf.add(x, shortcut)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block, train_flag):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x =  batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    x = tf.add(x, input_tensor)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def resnet_graph(input_image, architecture,train_flag=True, stage5=False,ttf=True):  # 残差网络函数\n",
        "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
        "    # Stage 1\n",
        "    paddings = tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])  # 上下左右各补3个0。\n",
        "    x = tf.pad(input_image, paddings, \"CONSTANT\")\n",
        "    w = weight_variable([7, 7, 3, 64])\n",
        "    b = bias_variable([64])  #\n",
        "    x = tf.nn.conv2d(x, w, strides = [1, 2, 2, 1], padding = 'VALID', name = 'conv_1') + b\n",
        "    x = batch_normalization(x, scope='bn_conv1', is_training=train_flag, need_relu=True)\n",
        "    C1 = x = maxpooling(x, [1, 3, 3, 1], [1, 2, 2, 1], name='stage1')\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', train_flag=train_flag)  # 结构块。\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_flag=train_flag)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_flag=train_flag, stride2=True)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_flag=train_flag)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_flag=train_flag)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_flag=train_flag, stride2=True)\n",
        "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  # block_count=22，即，下句for循环的range是0~22，正好是加23个层。\n",
        "    for i in range(block_count):  # 加22个层，都是用identity_block函数加。\n",
        "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_flag=train_flag)\n",
        "    C4 = x  #\n",
        "    # Stage 5\n",
        "    if stage5:\n",
        "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_flag=train_flag, stride2=True)\n",
        "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_flag=train_flag)\n",
        "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_flag=train_flag)\n",
        "        output=avg_pool(C5,1)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)#加入dropout\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "    else:\n",
        "        C5 = None\n",
        "        output=avg_pool(C4,2)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "\n",
        "    return [C1, C2, C3, C4, C5,output]\n",
        "\n",
        "\n",
        "\n",
        "filepath='C:/Users/zy109/Desktop/1/data/cifar-10-batches-py'\n",
        "model_save_path='C:/Users/zy109/Desktop/1/cifar10'\n",
        "data=Cifar10DataReader()\n",
        "sess=tf.InteractiveSession()\n",
        "\n",
        "#指数衰减学习率\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(learning_rate=0.001, global_step=global_step, decay_steps=50, decay_rate=0.99, staircase=False)\n",
        "\n",
        "#占位符\n",
        "input_image=tf.placeholder(tf.float32,[None,32,32,3])\n",
        "y_=tf.placeholder(tf.float32,[None,10])\n",
        "tf_is_train=tf.placeholder(tf.bool,None)\n",
        "input_image=data_enhace(input_image)#数据增强\n",
        "input_image,y_=criterion(input_image,y_,batch_size)#mixup\n",
        "\n",
        "[C1,C2,C3,C4,C5,output]=resnet_graph(input_image,'resnet50',ttf=tf_is_train)\n",
        "\n",
        "#交叉熵损失\n",
        "cross_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_,logits=output))\n",
        "\n",
        "tf.add_to_collection('losses',cross_loss)\n",
        "loss=tf.add_n(tf.get_collection('losses'))\n",
        "train_step=tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "\n",
        "#正确率\n",
        "correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y_,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "sess.run(tf.initialize_all_variables())\n",
        "saver = tf.train.Saver()\n",
        "#读取已经训练好的权重\n",
        "ckpt = tf.train.get_checkpoint_state(model_save_path)\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "#加载测试集数据\n",
        "test_data=data.all_test_data()\n",
        "x_test=np.array(test_data[0])/255\n",
        "y_test=np.array(test_data[1])\n",
        "\n",
        "total_loss=[]\n",
        "total_acc=[]\n",
        "total_step=[]\n",
        "\n",
        "\n",
        "# 训练\n",
        "for i in range(20000):\n",
        "\n",
        "    #逐批加载训练集数据\n",
        "    train_data=data.next_train_data(batch_size)\n",
        "    x_train=np.array(train_data[0])/255\n",
        "    y_train=np.array(train_data[1])\n",
        "\n",
        "    train_step.run(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        # 测试准确率\n",
        "        train_accuracy = accuracy.eval(feed_dict={input_image: x_train,y_: y_train,tf_is_train:False})\n",
        "        test_accuracy = accuracy.eval(feed_dict={input_image: x_test,y_: y_test,tf_is_train:False})\n",
        "        # 该次训练的损失\n",
        "        loss_value = loss.eval(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "        global_var=sess.run(global_step)\n",
        "        lr_val=sess.run(lr)\n",
        "        print(\"step %d, trainning accuracy， %g , testing accuracy， %g , loss %g  ,lr %g\" % (global_var, train_accuracy,test_accuracy, loss_value,lr_val))\n",
        "        total_step.append(i)\n",
        "        total_loss.append(loss_value)\n",
        "        total_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "save_path = saver.save(sess,\"./cifar10/model.ckpt\")\n",
        "print(\"save model:{0} Finished\".format(save_path))\n",
        "np.save(\"cifar10_step\",total_step)\n",
        "np.save(\"cifar10_loss\",total_loss)\n",
        "np.save(\"cifar10_acc\",total_acc)\n",
        "\n",
        "#测试\n",
        "y_log_predict=output.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "test_accuracy = accuracy.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "print(\"test accuracy %g\" % test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test.argmax(axis=1), y_log_predict.argmax(axis=1))\n",
        "\n",
        "\n",
        "for i in range(len(y_log_predict)):\n",
        "        max_value=max(y_log_predict[i])\n",
        "        for j in range(len(y_log_predict[i])):\n",
        "            if max_value==y_log_predict[i][j]:\n",
        "                y_log_predict[i][j]=1\n",
        "            else:\n",
        "                y_log_predict[i][j]=0\n",
        "\n",
        "# 精确率\n",
        "from sklearn.metrics import precision_score\n",
        "ps=precision_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_precision_score\",ps)\n",
        "\n",
        "#召回率\n",
        "from sklearn.metrics import recall_score\n",
        "rs=recall_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_recall_score\",rs)\n",
        "\n",
        "print(\"precision score:\",ps)\n",
        "print(\"recall score:\",rs)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "ax1.plot(total_step, total_loss, color=\"blue\", label=\"loss\")\n",
        "ax1.set_xlabel(\"step\")\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_step, total_acc, color=\"red\", label=\"accuary\")\n",
        "ax2.set_ylabel(\"accuary\")\n",
        "\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:650: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-41dfbd2f548e>:249: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-41dfbd2f548e>:280: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "step 1, trainning accuracy， 0.171875 , testing accuracy， 0.1221 , loss 80.561  ,lr 0.000999799\n",
            "step 101, trainning accuracy， 0.332031 , testing accuracy， 0.2949 , loss 52.5471  ,lr 0.000979903\n",
            "step 201, trainning accuracy， 0.367188 , testing accuracy， 0.3905 , loss 31.7227  ,lr 0.000960403\n",
            "step 301, trainning accuracy， 0.382812 , testing accuracy， 0.4163 , loss 20.7978  ,lr 0.000941291\n",
            "step 401, trainning accuracy， 0.433594 , testing accuracy， 0.4417 , loss 14.8903  ,lr 0.000922559\n",
            "step 501, trainning accuracy， 0.492188 , testing accuracy， 0.4824 , loss 10.6072  ,lr 0.0009042\n",
            "step 601, trainning accuracy， 0.476562 , testing accuracy， 0.4771 , loss 8.73625  ,lr 0.000886207\n",
            "step 701, trainning accuracy， 0.515625 , testing accuracy， 0.5234 , loss 7.09204  ,lr 0.000868571\n",
            "step 801, trainning accuracy， 0.53125 , testing accuracy， 0.5513 , loss 5.91643  ,lr 0.000851287\n",
            "step 901, trainning accuracy， 0.605469 , testing accuracy， 0.5737 , loss 5.03351  ,lr 0.000834346\n",
            "step 1001, trainning accuracy， 0.609375 , testing accuracy， 0.5662 , loss 4.37791  ,lr 0.000817743\n",
            "step 1101, trainning accuracy， 0.582031 , testing accuracy， 0.6107 , loss 3.93391  ,lr 0.00080147\n",
            "step 1201, trainning accuracy， 0.660156 , testing accuracy， 0.6203 , loss 3.45449  ,lr 0.00078552\n",
            "step 1301, trainning accuracy， 0.660156 , testing accuracy， 0.6231 , loss 3.10459  ,lr 0.000769889\n",
            "step 1401, trainning accuracy， 0.644531 , testing accuracy， 0.644 , loss 2.94461  ,lr 0.000754568\n",
            "step 1501, trainning accuracy， 0.667969 , testing accuracy， 0.6338 , loss 2.73426  ,lr 0.000739552\n",
            "step 1601, trainning accuracy， 0.707031 , testing accuracy， 0.6487 , loss 2.68009  ,lr 0.000724835\n",
            "step 1701, trainning accuracy， 0.703125 , testing accuracy， 0.6713 , loss 2.38068  ,lr 0.000710411\n",
            "step 1801, trainning accuracy， 0.734375 , testing accuracy， 0.6524 , loss 2.32415  ,lr 0.000696274\n",
            "step 1901, trainning accuracy， 0.753906 , testing accuracy， 0.676 , loss 2.06093  ,lr 0.000682418\n",
            "step 2001, trainning accuracy， 0.738281 , testing accuracy， 0.6851 , loss 2.03446  ,lr 0.000668838\n",
            "step 2101, trainning accuracy， 0.730469 , testing accuracy， 0.6696 , loss 2.02023  ,lr 0.000655528\n",
            "step 2201, trainning accuracy， 0.6875 , testing accuracy， 0.6941 , loss 1.92981  ,lr 0.000642483\n",
            "step 2301, trainning accuracy， 0.722656 , testing accuracy， 0.6753 , loss 1.82954  ,lr 0.000629697\n",
            "step 2401, trainning accuracy， 0.699219 , testing accuracy， 0.6302 , loss 2.05701  ,lr 0.000617166\n",
            "step 2501, trainning accuracy， 0.726562 , testing accuracy， 0.689 , loss 2.18478  ,lr 0.000604885\n",
            "step 2601, trainning accuracy， 0.765625 , testing accuracy， 0.6963 , loss 1.95633  ,lr 0.000592848\n",
            "step 2701, trainning accuracy， 0.75 , testing accuracy， 0.6969 , loss 1.83125  ,lr 0.00058105\n",
            "step 2801, trainning accuracy， 0.726562 , testing accuracy， 0.6507 , loss 1.89284  ,lr 0.000569487\n",
            "step 2901, trainning accuracy， 0.730469 , testing accuracy， 0.7014 , loss 1.82147  ,lr 0.000558154\n",
            "step 3001, trainning accuracy， 0.753906 , testing accuracy， 0.7032 , loss 1.86107  ,lr 0.000547047\n",
            "step 3101, trainning accuracy， 0.75 , testing accuracy， 0.7235 , loss 1.62097  ,lr 0.000536161\n",
            "step 3201, trainning accuracy， 0.769531 , testing accuracy， 0.7207 , loss 1.51845  ,lr 0.000525491\n",
            "step 3301, trainning accuracy， 0.742188 , testing accuracy， 0.7366 , loss 1.55161  ,lr 0.000515034\n",
            "step 3401, trainning accuracy， 0.769531 , testing accuracy， 0.7323 , loss 1.42951  ,lr 0.000504785\n",
            "step 3501, trainning accuracy， 0.796875 , testing accuracy， 0.7423 , loss 1.2999  ,lr 0.00049474\n",
            "step 3601, trainning accuracy， 0.835938 , testing accuracy， 0.7477 , loss 1.16426  ,lr 0.000484894\n",
            "step 3701, trainning accuracy， 0.769531 , testing accuracy， 0.6894 , loss 1.5646  ,lr 0.000475245\n",
            "step 3801, trainning accuracy， 0.824219 , testing accuracy， 0.7466 , loss 1.25179  ,lr 0.000465788\n",
            "step 3901, trainning accuracy， 0.847656 , testing accuracy， 0.7445 , loss 1.21986  ,lr 0.000456518\n",
            "step 4001, trainning accuracy， 0.828125 , testing accuracy， 0.744 , loss 1.31096  ,lr 0.000447434\n",
            "step 4101, trainning accuracy， 0.824219 , testing accuracy， 0.7383 , loss 1.3342  ,lr 0.00043853\n",
            "step 4201, trainning accuracy， 0.824219 , testing accuracy， 0.7504 , loss 1.26347  ,lr 0.000429803\n",
            "step 4301, trainning accuracy， 0.859375 , testing accuracy， 0.7645 , loss 1.03758  ,lr 0.00042125\n",
            "step 4401, trainning accuracy， 0.875 , testing accuracy， 0.7558 , loss 1.05247  ,lr 0.000412867\n",
            "step 4501, trainning accuracy， 0.824219 , testing accuracy， 0.7449 , loss 1.21749  ,lr 0.000404651\n",
            "step 4601, trainning accuracy， 0.84375 , testing accuracy， 0.7649 , loss 1.02457  ,lr 0.000396598\n",
            "step 4701, trainning accuracy， 0.871094 , testing accuracy， 0.7597 , loss 1.05491  ,lr 0.000388706\n",
            "step 4801, trainning accuracy， 0.839844 , testing accuracy， 0.7604 , loss 1.04231  ,lr 0.000380971\n",
            "step 4901, trainning accuracy， 0.886719 , testing accuracy， 0.7586 , loss 0.90098  ,lr 0.00037339\n",
            "step 5001, trainning accuracy， 0.757812 , testing accuracy， 0.714 , loss 1.46182  ,lr 0.000365959\n",
            "step 5101, trainning accuracy， 0.847656 , testing accuracy， 0.7632 , loss 1.13827  ,lr 0.000358677\n",
            "step 5201, trainning accuracy， 0.839844 , testing accuracy， 0.7666 , loss 1.0407  ,lr 0.000351539\n",
            "step 5301, trainning accuracy， 0.878906 , testing accuracy， 0.764 , loss 0.908045  ,lr 0.000344543\n",
            "step 5401, trainning accuracy， 0.882812 , testing accuracy， 0.7698 , loss 0.921542  ,lr 0.000337687\n",
            "step 5501, trainning accuracy， 0.898438 , testing accuracy， 0.7676 , loss 0.88217  ,lr 0.000330967\n",
            "step 5601, trainning accuracy， 0.910156 , testing accuracy， 0.7692 , loss 0.787486  ,lr 0.000324381\n",
            "step 5701, trainning accuracy， 0.832031 , testing accuracy， 0.7351 , loss 1.29589  ,lr 0.000317926\n",
            "step 5801, trainning accuracy， 0.851562 , testing accuracy， 0.7665 , loss 1.12018  ,lr 0.000311599\n",
            "step 5901, trainning accuracy， 0.886719 , testing accuracy， 0.7682 , loss 0.895461  ,lr 0.000305398\n",
            "step 6001, trainning accuracy， 0.886719 , testing accuracy， 0.7776 , loss 0.895258  ,lr 0.000299321\n",
            "step 6101, trainning accuracy， 0.890625 , testing accuracy， 0.7697 , loss 0.817493  ,lr 0.000293364\n",
            "step 6201, trainning accuracy， 0.925781 , testing accuracy， 0.7748 , loss 0.798996  ,lr 0.000287526\n",
            "step 6301, trainning accuracy， 0.929688 , testing accuracy， 0.7784 , loss 0.706163  ,lr 0.000281804\n",
            "step 6401, trainning accuracy， 0.941406 , testing accuracy， 0.7763 , loss 0.725904  ,lr 0.000276196\n",
            "step 6501, trainning accuracy， 0.949219 , testing accuracy， 0.773 , loss 0.74043  ,lr 0.0002707\n",
            "step 6601, trainning accuracy， 0.890625 , testing accuracy， 0.7722 , loss 0.863892  ,lr 0.000265313\n",
            "step 6701, trainning accuracy， 0.957031 , testing accuracy， 0.7749 , loss 0.637798  ,lr 0.000260034\n",
            "step 6801, trainning accuracy， 0.921875 , testing accuracy， 0.7735 , loss 0.696092  ,lr 0.000254859\n",
            "step 6901, trainning accuracy， 0.957031 , testing accuracy， 0.7806 , loss 0.678972  ,lr 0.000249787\n",
            "step 7001, trainning accuracy， 0.941406 , testing accuracy， 0.7782 , loss 0.719967  ,lr 0.000244816\n",
            "step 7101, trainning accuracy， 0.964844 , testing accuracy， 0.7735 , loss 0.62493  ,lr 0.000239945\n",
            "step 7201, trainning accuracy， 0.945312 , testing accuracy， 0.7817 , loss 0.676905  ,lr 0.00023517\n",
            "step 7301, trainning accuracy， 0.941406 , testing accuracy， 0.7773 , loss 0.645288  ,lr 0.00023049\n",
            "step 7401, trainning accuracy， 0.945312 , testing accuracy， 0.7746 , loss 0.641508  ,lr 0.000225903\n",
            "step 7501, trainning accuracy， 0.917969 , testing accuracy， 0.7661 , loss 0.832156  ,lr 0.000221408\n",
            "step 7601, trainning accuracy， 0.945312 , testing accuracy， 0.7773 , loss 0.758911  ,lr 0.000217002\n",
            "step 7701, trainning accuracy， 0.949219 , testing accuracy， 0.7771 , loss 0.643362  ,lr 0.000212683\n",
            "step 7801, trainning accuracy， 0.976562 , testing accuracy， 0.7724 , loss 0.556879  ,lr 0.000208451\n",
            "step 7901, trainning accuracy， 0.976562 , testing accuracy， 0.777 , loss 0.624866  ,lr 0.000204303\n",
            "step 8001, trainning accuracy， 0.980469 , testing accuracy， 0.7763 , loss 0.558369  ,lr 0.000200237\n",
            "step 8101, trainning accuracy， 0.976562 , testing accuracy， 0.7792 , loss 0.529382  ,lr 0.000196252\n",
            "step 8201, trainning accuracy， 0.964844 , testing accuracy， 0.776 , loss 0.564218  ,lr 0.000192347\n",
            "step 8301, trainning accuracy， 0.964844 , testing accuracy， 0.7781 , loss 0.528224  ,lr 0.000188519\n",
            "step 8401, trainning accuracy， 0.980469 , testing accuracy， 0.7743 , loss 0.512641  ,lr 0.000184768\n",
            "step 8501, trainning accuracy， 0.964844 , testing accuracy， 0.7749 , loss 0.62874  ,lr 0.000181091\n",
            "step 8601, trainning accuracy， 0.996094 , testing accuracy， 0.7821 , loss 0.481303  ,lr 0.000177487\n",
            "step 8701, trainning accuracy， 0.988281 , testing accuracy， 0.7689 , loss 0.490175  ,lr 0.000173955\n",
            "step 8801, trainning accuracy， 0.992188 , testing accuracy， 0.7755 , loss 0.490665  ,lr 0.000170493\n",
            "step 8901, trainning accuracy， 0.972656 , testing accuracy， 0.7815 , loss 0.521708  ,lr 0.000167101\n",
            "step 9001, trainning accuracy， 0.992188 , testing accuracy， 0.7771 , loss 0.464894  ,lr 0.000163775\n",
            "step 9101, trainning accuracy， 0.988281 , testing accuracy， 0.7797 , loss 0.466965  ,lr 0.000160516\n",
            "step 9201, trainning accuracy， 0.984375 , testing accuracy， 0.7786 , loss 0.48208  ,lr 0.000157322\n",
            "step 9301, trainning accuracy， 0.996094 , testing accuracy， 0.7764 , loss 0.469138  ,lr 0.000154191\n",
            "step 9401, trainning accuracy， 0.992188 , testing accuracy， 0.7772 , loss 0.436146  ,lr 0.000151123\n",
            "step 9501, trainning accuracy， 0.988281 , testing accuracy， 0.7794 , loss 0.424636  ,lr 0.000148115\n",
            "step 9601, trainning accuracy， 0.992188 , testing accuracy， 0.7768 , loss 0.423621  ,lr 0.000145168\n",
            "step 9701, trainning accuracy， 0.976562 , testing accuracy， 0.7772 , loss 0.486419  ,lr 0.000142279\n",
            "step 9801, trainning accuracy， 0.996094 , testing accuracy， 0.7775 , loss 0.399165  ,lr 0.000139448\n",
            "step 9901, trainning accuracy， 0.996094 , testing accuracy， 0.7868 , loss 0.413764  ,lr 0.000136673\n",
            "step 10001, trainning accuracy， 0.996094 , testing accuracy， 0.7851 , loss 0.393748  ,lr 0.000133953\n",
            "step 10101, trainning accuracy， 0.992188 , testing accuracy， 0.7833 , loss 0.469159  ,lr 0.000131287\n",
            "step 10201, trainning accuracy， 0.996094 , testing accuracy， 0.7849 , loss 0.39001  ,lr 0.000128675\n",
            "step 10301, trainning accuracy， 0.992188 , testing accuracy， 0.7756 , loss 0.432182  ,lr 0.000126114\n",
            "step 10401, trainning accuracy， 1 , testing accuracy， 0.7845 , loss 0.388008  ,lr 0.000123604\n",
            "step 10501, trainning accuracy， 0.996094 , testing accuracy， 0.7807 , loss 0.402996  ,lr 0.000121145\n",
            "step 10601, trainning accuracy， 0.984375 , testing accuracy， 0.7775 , loss 0.421033  ,lr 0.000118734\n",
            "step 10701, trainning accuracy， 0.996094 , testing accuracy， 0.783 , loss 0.403796  ,lr 0.000116371\n",
            "step 10801, trainning accuracy， 0.984375 , testing accuracy， 0.7705 , loss 0.400652  ,lr 0.000114055\n",
            "step 10901, trainning accuracy， 1 , testing accuracy， 0.7756 , loss 0.41034  ,lr 0.000111786\n",
            "step 11001, trainning accuracy， 1 , testing accuracy， 0.7794 , loss 0.394085  ,lr 0.000109561\n",
            "step 11101, trainning accuracy， 0.980469 , testing accuracy， 0.7837 , loss 0.433748  ,lr 0.000107381\n",
            "step 11201, trainning accuracy， 1 , testing accuracy， 0.7863 , loss 0.384779  ,lr 0.000105244\n",
            "step 11301, trainning accuracy， 0.992188 , testing accuracy， 0.779 , loss 0.385375  ,lr 0.00010315\n",
            "step 11401, trainning accuracy， 1 , testing accuracy， 0.7825 , loss 0.358572  ,lr 0.000101097\n",
            "step 11501, trainning accuracy， 0.996094 , testing accuracy， 0.7834 , loss 0.376181  ,lr 9.90851e-05\n",
            "step 11601, trainning accuracy， 1 , testing accuracy， 0.7853 , loss 0.340137  ,lr 9.71133e-05\n",
            "step 11701, trainning accuracy， 1 , testing accuracy， 0.7794 , loss 0.362657  ,lr 9.51808e-05\n",
            "step 11801, trainning accuracy， 0.988281 , testing accuracy， 0.7819 , loss 0.381561  ,lr 9.32867e-05\n",
            "step 11901, trainning accuracy， 0.992188 , testing accuracy， 0.7848 , loss 0.3801  ,lr 9.14303e-05\n",
            "step 12001, trainning accuracy， 0.992188 , testing accuracy， 0.7835 , loss 0.380102  ,lr 8.96108e-05\n",
            "step 12101, trainning accuracy， 0.996094 , testing accuracy， 0.783 , loss 0.369962  ,lr 8.78276e-05\n",
            "step 12201, trainning accuracy， 1 , testing accuracy， 0.7801 , loss 0.345307  ,lr 8.60798e-05\n",
            "step 12301, trainning accuracy， 1 , testing accuracy， 0.7815 , loss 0.339816  ,lr 8.43668e-05\n",
            "step 12401, trainning accuracy， 1 , testing accuracy， 0.7772 , loss 0.370438  ,lr 8.26879e-05\n",
            "step 12501, trainning accuracy， 0.996094 , testing accuracy， 0.7827 , loss 0.344811  ,lr 8.10424e-05\n",
            "step 12601, trainning accuracy， 1 , testing accuracy， 0.7815 , loss 0.33363  ,lr 7.94297e-05\n",
            "step 12701, trainning accuracy， 0.996094 , testing accuracy， 0.7833 , loss 0.341969  ,lr 7.7849e-05\n",
            "step 12801, trainning accuracy， 1 , testing accuracy， 0.783 , loss 0.303654  ,lr 7.62998e-05\n",
            "step 12901, trainning accuracy， 1 , testing accuracy， 0.7816 , loss 0.316804  ,lr 7.47815e-05\n",
            "step 13001, trainning accuracy， 0.996094 , testing accuracy， 0.7826 , loss 0.331098  ,lr 7.32933e-05\n",
            "step 13101, trainning accuracy， 0.992188 , testing accuracy， 0.7853 , loss 0.338132  ,lr 7.18348e-05\n",
            "step 13201, trainning accuracy， 1 , testing accuracy， 0.7763 , loss 0.313462  ,lr 7.04053e-05\n",
            "step 13301, trainning accuracy， 0.996094 , testing accuracy， 0.7822 , loss 0.317855  ,lr 6.90042e-05\n",
            "step 13401, trainning accuracy， 0.996094 , testing accuracy， 0.7793 , loss 0.342832  ,lr 6.7631e-05\n",
            "step 13501, trainning accuracy， 1 , testing accuracy， 0.7795 , loss 0.320509  ,lr 6.62852e-05\n",
            "step 13601, trainning accuracy， 1 , testing accuracy， 0.7841 , loss 0.292378  ,lr 6.49661e-05\n",
            "step 13701, trainning accuracy， 0.996094 , testing accuracy， 0.7852 , loss 0.325889  ,lr 6.36733e-05\n",
            "step 13801, trainning accuracy， 1 , testing accuracy， 0.7827 , loss 0.334967  ,lr 6.24062e-05\n",
            "step 13901, trainning accuracy， 1 , testing accuracy， 0.7818 , loss 0.291756  ,lr 6.11643e-05\n",
            "step 14001, trainning accuracy， 1 , testing accuracy， 0.7814 , loss 0.311681  ,lr 5.99471e-05\n",
            "step 14101, trainning accuracy， 1 , testing accuracy， 0.7778 , loss 0.295272  ,lr 5.87542e-05\n",
            "step 14201, trainning accuracy， 1 , testing accuracy， 0.7841 , loss 0.317909  ,lr 5.7585e-05\n",
            "step 14301, trainning accuracy， 1 , testing accuracy， 0.7822 , loss 0.28188  ,lr 5.6439e-05\n",
            "step 14401, trainning accuracy， 1 , testing accuracy， 0.7808 , loss 0.284696  ,lr 5.53159e-05\n",
            "step 14501, trainning accuracy， 1 , testing accuracy， 0.7816 , loss 0.282908  ,lr 5.42151e-05\n",
            "step 14601, trainning accuracy， 1 , testing accuracy， 0.7793 , loss 0.279319  ,lr 5.31362e-05\n",
            "step 14701, trainning accuracy， 1 , testing accuracy， 0.7826 , loss 0.27984  ,lr 5.20788e-05\n",
            "step 14801, trainning accuracy， 1 , testing accuracy， 0.7877 , loss 0.291759  ,lr 5.10425e-05\n",
            "step 14901, trainning accuracy， 1 , testing accuracy， 0.7835 , loss 0.283453  ,lr 5.00267e-05\n",
            "step 15001, trainning accuracy， 1 , testing accuracy， 0.7825 , loss 0.270265  ,lr 4.90312e-05\n",
            "step 15101, trainning accuracy， 1 , testing accuracy， 0.7825 , loss 0.286797  ,lr 4.80555e-05\n",
            "step 15201, trainning accuracy， 1 , testing accuracy， 0.7821 , loss 0.276074  ,lr 4.70992e-05\n",
            "step 15301, trainning accuracy， 1 , testing accuracy， 0.7846 , loss 0.289566  ,lr 4.61619e-05\n",
            "step 15401, trainning accuracy， 1 , testing accuracy， 0.7813 , loss 0.280295  ,lr 4.52433e-05\n",
            "step 15501, trainning accuracy， 1 , testing accuracy， 0.7824 , loss 0.282079  ,lr 4.43429e-05\n",
            "step 15601, trainning accuracy， 1 , testing accuracy， 0.7818 , loss 0.271006  ,lr 4.34605e-05\n",
            "step 15701, trainning accuracy， 1 , testing accuracy， 0.7839 , loss 0.265818  ,lr 4.25956e-05\n",
            "step 15801, trainning accuracy， 1 , testing accuracy， 0.7844 , loss 0.274769  ,lr 4.1748e-05\n",
            "step 15901, trainning accuracy， 1 , testing accuracy， 0.7836 , loss 0.286156  ,lr 4.09172e-05\n",
            "step 16001, trainning accuracy， 1 , testing accuracy， 0.7876 , loss 0.276868  ,lr 4.0103e-05\n",
            "step 16101, trainning accuracy， 1 , testing accuracy， 0.7857 , loss 0.265413  ,lr 3.93049e-05\n",
            "step 16201, trainning accuracy， 1 , testing accuracy， 0.7863 , loss 0.262729  ,lr 3.85227e-05\n",
            "step 16301, trainning accuracy， 1 , testing accuracy， 0.7837 , loss 0.259835  ,lr 3.77561e-05\n",
            "step 16401, trainning accuracy， 1 , testing accuracy， 0.7828 , loss 0.260754  ,lr 3.70048e-05\n",
            "step 16501, trainning accuracy， 1 , testing accuracy， 0.7819 , loss 0.268932  ,lr 3.62684e-05\n",
            "step 16601, trainning accuracy， 1 , testing accuracy， 0.7862 , loss 0.258055  ,lr 3.55467e-05\n",
            "step 16701, trainning accuracy， 1 , testing accuracy， 0.786 , loss 0.255066  ,lr 3.48393e-05\n",
            "step 16801, trainning accuracy， 1 , testing accuracy， 0.7848 , loss 0.266265  ,lr 3.4146e-05\n",
            "step 16901, trainning accuracy， 1 , testing accuracy， 0.7859 , loss 0.252778  ,lr 3.34665e-05\n",
            "step 17001, trainning accuracy， 1 , testing accuracy， 0.7844 , loss 0.250314  ,lr 3.28005e-05\n",
            "step 17101, trainning accuracy， 1 , testing accuracy， 0.7875 , loss 0.249406  ,lr 3.21478e-05\n",
            "step 17201, trainning accuracy， 1 , testing accuracy， 0.7863 , loss 0.250426  ,lr 3.1508e-05\n",
            "step 17301, trainning accuracy， 1 , testing accuracy， 0.7856 , loss 0.248104  ,lr 3.0881e-05\n",
            "step 17401, trainning accuracy， 1 , testing accuracy， 0.7872 , loss 0.249987  ,lr 3.02665e-05\n",
            "step 17501, trainning accuracy， 1 , testing accuracy， 0.786 , loss 0.257109  ,lr 2.96642e-05\n",
            "step 17601, trainning accuracy， 1 , testing accuracy， 0.7873 , loss 0.244652  ,lr 2.90739e-05\n",
            "step 17701, trainning accuracy， 1 , testing accuracy， 0.7854 , loss 0.246133  ,lr 2.84953e-05\n",
            "step 17801, trainning accuracy， 1 , testing accuracy， 0.7864 , loss 0.255938  ,lr 2.79282e-05\n",
            "step 17901, trainning accuracy， 1 , testing accuracy， 0.785 , loss 0.251849  ,lr 2.73725e-05\n",
            "step 18001, trainning accuracy， 1 , testing accuracy， 0.7868 , loss 0.246877  ,lr 2.68278e-05\n",
            "step 18101, trainning accuracy， 1 , testing accuracy， 0.7851 , loss 0.241307  ,lr 2.62939e-05\n",
            "step 18201, trainning accuracy， 1 , testing accuracy， 0.7874 , loss 0.239583  ,lr 2.57706e-05\n",
            "step 18301, trainning accuracy， 1 , testing accuracy， 0.7853 , loss 0.245024  ,lr 2.52578e-05\n",
            "step 18401, trainning accuracy， 1 , testing accuracy， 0.7865 , loss 0.249712  ,lr 2.47552e-05\n",
            "step 18501, trainning accuracy， 1 , testing accuracy， 0.7846 , loss 0.239709  ,lr 2.42625e-05\n",
            "step 18601, trainning accuracy， 1 , testing accuracy， 0.7865 , loss 0.239744  ,lr 2.37797e-05\n",
            "step 18701, trainning accuracy， 1 , testing accuracy， 0.7842 , loss 0.237178  ,lr 2.33065e-05\n",
            "step 18801, trainning accuracy， 1 , testing accuracy， 0.788 , loss 0.235579  ,lr 2.28427e-05\n",
            "step 18901, trainning accuracy， 1 , testing accuracy， 0.7832 , loss 0.236459  ,lr 2.23881e-05\n",
            "step 19001, trainning accuracy， 1 , testing accuracy， 0.7862 , loss 0.244708  ,lr 2.19426e-05\n",
            "step 19101, trainning accuracy， 1 , testing accuracy， 0.7831 , loss 0.259468  ,lr 2.1506e-05\n",
            "step 19201, trainning accuracy， 1 , testing accuracy， 0.7858 , loss 0.233016  ,lr 2.1078e-05\n",
            "step 19301, trainning accuracy， 1 , testing accuracy， 0.7874 , loss 0.232527  ,lr 2.06585e-05\n",
            "step 19401, trainning accuracy， 1 , testing accuracy， 0.7863 , loss 0.23693  ,lr 2.02474e-05\n",
            "step 19501, trainning accuracy， 1 , testing accuracy， 0.7861 , loss 0.231958  ,lr 1.98445e-05\n",
            "step 19601, trainning accuracy， 1 , testing accuracy， 0.7867 , loss 0.245174  ,lr 1.94496e-05\n",
            "step 19701, trainning accuracy， 1 , testing accuracy， 0.7869 , loss 0.229771  ,lr 1.90626e-05\n",
            "step 19801, trainning accuracy， 1 , testing accuracy， 0.7868 , loss 0.232794  ,lr 1.86832e-05\n",
            "step 19901, trainning accuracy， 1 , testing accuracy， 0.7879 , loss 0.228682  ,lr 1.83114e-05\n",
            "save model:./cifar10/model.ckpt Finished\n",
            "test accuracy 0.7865\n",
            "precision score: [0.79078695 0.87562189 0.73717277 0.61116552 0.75143403 0.71295337\n",
            " 0.84575569 0.81809145 0.88410256 0.84102061]\n",
            "recall score: [0.824 0.88  0.704 0.624 0.786 0.688 0.817 0.823 0.862 0.857]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEaCAYAAADZvco2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dd5gV9fX/X2cLC7v0IiCooCKCCFIVKaIIYgMNomJDLKSJPVG/msSWX+wajTFBRdGoiFjQ2CUSxKCCSl0QERGWLgLSFlj2/P44c9nLsuXu7q3LeT3PPDPzmXbu3HvnPeecTxFVxXEcx3GiQVqiDXAcx3GqDy4qjuM4TtRwUXEcx3GihouK4ziOEzVcVBzHcZyo4aLiOI7jRI2MeF4sLS1Na9WqFc9LOo7jpDzbtm1TVU0JJyCuolKrVi22bt0az0s6juOkPCKyPdE2REpKKJ/jOI6TGrioOI7jOFHDRcVxHMeJGnHNqTiOs/+xa9cu8vLyyM/PT7QpSU/NmjVp2bIlmZmZiTal0rioOI4TU/Ly8qhTpw6tWrVCRBJtTtKiqqxfv568vDxat25doWNFZCxwBrBWVTuUsF2AvwKnAduAS1X1qyiYvQ8Rhb9E5DoRmS8i80TkJRGpKSKtReRzEVksIi+LSI1YGOg4TmqTn59Po0aNXFDKQURo1KhRZT26Z4FBZWw/FWgTTKOAJypzkUgoV1REpAVwNdAtUMB04HzgXuBhVT0c2ABcHisjHcdJbVxQIqOy90lVpwI/lbHLEOA5NT4D6otI80pdrBwiDX9lALVEZBeQDawCTgIuCLaPA24nRuo3bBisWwdTpsTi7M5+i6pNIURsCm0DuO46+PxzePBBOP74om0isHMn9OwJixfDIYfAjBkwfTr87nfw8sswZw7cdJOd48cf4bHHID8fLr0U/vrXvW0pLITnnoPbb4cNG6wsPR0uuAD69oU77oDjjoPTT7flpUuhYUO480646CLYtQuGDjXbHnoIWrWy6z3wAGzbBkcfDTffDE89BR9/HLt7WhITJtjnTiC1e/Viy6efJtQG2rWDmjUTdfUWwPKw9bygbFW0L1SuqKjqChF5AFgGbAc+AL4ENqpqQTEDY8KOHbBpU6zO7qQUK1dC167w8MNw/vn2ME4LHO7wZYAnn4Q//cke1FdcYdtCQpKWBmecAe+8U7R/585w/fX2MN6yBfr3t+U6daBXLxOKfv1suuUWqFEDvvoKBg2C996DDz+EZ56BmTPh1FMhL8+E4de/tvOfeSbs3g2PPgrnnWcisWGDCcS118K0aSZSZ59t+69bB088AY8/DoceaqIzdiy0aQMjR5qAXXKJCUXz5vDvf0Pt2tC2bdFnGjjQ1idMsOvXrm0iFM+HW+3a0Lhx/K5XEiKJtyE9vSpHZ4jIzLD1Mao6pooWxQZVLXMCGgD/AZoAmcAbwEXA4rB9DgLmlXL8KGAmMLNGjRpaGYYOVW3fvlKHOtWNl14yWahZU/U3v7H5G2+obtig2qSJ6qmnqi5erPrTT6oNGqjm5Nj+xx6r+tBDqgccoHrNNao//GDlgwer3nGH6h/+oNqypZU1a6baurUtn3WW6s8/q3burHrwwapXXGHlTZrYOQ89VDU/X7V+ffuh1qypevzxqunpqi1aqK5cqTpxourkyWb/li12/o4dVdu2DUmcaqNGqmPHqu7evffn/fprK9+5U3X+fNUxY1S3b7dtu3fbeoMGdo5bb7Xr3XOPfaa331YtLLR9N25Uffxx1eXL4/ddBeTm5sb9msXJyclRVdXCwkK98cYb9aijjtIOHTro+PHjVVV15cqV2qdPH+3UqZMeddRROnXqVC0oKNARI0bs2fehhx6Ki60l3S9gq5b/rG5VxnP4n8DwsPVvgOblnbMyUyThr5OB71V1HYCIvAb0wmJyGWreSktgRSmiNQYYA5CTk1OpsYtr1DDv3nGYMQOysqBpU/j732157FhzZdets9DOUUdBly6wcaN5EnPnwo03mhdSuzaMGQP169v5HnjA3vwBfv97ePNNCzHVqAFvvWXLOTm2X//+5hWceKJdZ906uPdes2HIEBg3zs7z5z9DZiYceKB5EEOHFtmfk2Mhq1Gj4LDD7Ly1a8M550CjRvt+3mOOsQmgfXubQqSlwZVXwllnwdSp5uGkpVnIrTj16sFvflP1+19Frr0WZs2K7jmPOQYeeSSyfV977TVmzZrF7Nmz+fHHH+nevTt9+/blxRdf5JRTTuHWW29l9+7dbNu2jVmzZrFixQrmzZsHwMaNG6NreHx5E7hKRMYDxwKbVDXqoS+ILKeyDDhORLKx8Fd/zPP4GDgHGA+MACbFwkCw/+fOnbE6u5M07NxpAtClS1FuI8TcudChg4WWOneGV16BNWvghRdMXDZtgoMOspDQjTfC+PEwYkTRQ/nMM02QGjaEbt3swd+pU5GggD3cL7igaP3cc4uWTzoJTjsNPvkEXnrJno5vvGFhKDBRGDcOmjSBPn3KDnVccYUJyvHHRycM1aTJ3sLllMq0adMYPnw46enpNG3alBNOOIEZM2bQvXt3LrvsMnbt2sVZZ53FMcccw6GHHsqSJUsYPXo0p59+OgMHDky0+aUiIi8B/YDGIpIH/AmLLKGq/wDewaoTL8aqFI+MmTGRuDPAHcBCYB7wPJAFHAp8ERj5CpBV3nmys7Mr5Q5ecYVq8+aVOtRJdrZuVX3lFdWnn1Y98kgL4/Tvr/rdd0X7vPWWlY8fb+Gs0aOLtn36aVEI6brrisrnzlW1nl33pUcP2/+uuypm6/btqsuW2fKWLaoLFxZty89Xbdx4b9scVU2u8Ne1116rTz/99J7yiy66SCdNmqSqqitWrNAxY8Zop06ddNy4caqqunnzZp04caIOGTJER44cGRdbKxv+SpYprherrKj85jf2f3WqIQ8+WCQKrVur3nabar16qkcdpVpQYPuccoptP+QQmwd/eFW1vMKBB1r5p59Gds2XXlLNzFT95pvofpbVq4vyHc4ekklUXn31VR04cKAWFBTo2rVr9eCDD9ZVq1bp0qVLtSD4vT322GN6zTXX6Lp163TTpk2qqjp37lzt1KlTXGxNdVFJiRb1NWp4+KvaMnWq1Wx6+22b16hhYalhwyycdMIJ8P77cPDB8MMPdkz37kXHp6VZFd1XX7XaVJFw/vlwyinQoEF0P0vTptE9nxN1zj77bKZPn06nTp0QEe677z6aNWvGuHHjuP/++8nMzKR27do899xzrFixgpEjR1JYWAjAX/7ylwRbnxqIiWB8yMnJ0cqMp/L731vNzu0pM6LAfk5hoeU5zj+/7GqcqpYPOPNMq4obXt6zp4lIu3YmPDNnwrHHmuhs3Lh3zkLVrlm1KptOjFiwYAHt2rVLtBkpQ0n3S0S2qWpOgkyqECnRS7HX/koxPvsMRo+2tiRl8c03sH499O69d7mIteVIT4cvvrDE9jHHwNVXWyK9uHiIuKA4TpKQMqKye7dNThKxdStcc415Dvn59tBfsQI++MC2v/LK3i3WizNtms2LiwpAjx7WeHDLFvjHP6zs/vvhn/+M7mdwHCeqpEROJdQL9K5d/kKaVHzyiXkUPXpYPuGxx8xrmDHD5t9+C/PmWRchJTFtmoXHjjgivnY7jhMzUsZTAQ+BJQVffWVdkPz0E6wK2k7Nnm0TwLPPWl9ZV15pSfRXXin5PPPmWQK+d+9926Q4jpOypISnEhIVrwGWYFavhsGDLcQ1c+beotK0KWRkwM8/W9nFF8OiRdba/YQTYNIkO27iROsn68wzrZX3tdcm7vM4jhN1UkJUQuEvF5UEogrDh1srdrB8x+rVthwSlZNPhuXLYdkyq6l1991WNfjkk4vO89131gq+USOYPz/xnfw5jhNVUir85aKSQCZNsrEHHnzQ1vPyijyVNWssnNWpk1UN/te/7E2gVy9YuNAS7M8/b/tOm2ZT374uKI5TDUkpUfGcSoLYtcs6KTzySOuUsGnTIlHJyrJ9CgtNVLp3txBZiLp1rS+uCy6wfrfGj7f2JyXV+HKcakpBQUH5O1UTUkJUPPyVQBYtsvFCFi2Ce+6xvEnLlkXhr759i/bt2LH086Slmefy/vu27qLixJmzzjqLrl27ctRRRzFmjA1F8t5779GlSxc6depE//79AdiyZQsjR47k6KOPpmPHjrz66qsA1K5de8+5Jk6cyKWXXgrAW2+9xbHHHkvnzp05+eSTWROEiG+//XYuvvhievXqxcUXX0zfvn2ZFdZFc+/evZkdquBSjUiJnIqHvxLIGWfA2rXWViTkgbRsabmRVausbP58a8QYPjhUSfTubd3J5+SYV+PsfySw7/uxY8fSsGFDtm/fTvfu3RkyZAhXXnklU6dOpXXr1vz0k43Ge9ddd1GvXj3mzp0LwIbQSJyl0Lt3bz777DNEhKeeeor77ruPB4MwcW5uLtOmTaNWrVqMGzeOZ599lkceeYRFixaRn59Pp2r4P0gpUfHwV5xZv97amtx/P/zyl0XlLVuax5Gfb+OF9OpleZWMcn5OIe+kZ8/y93WcKPPoo4/y+uuvA7B8+XLGjBlD3759ad26NQANGzYE4KOPPmL8+PF7jmtQTh9xeXl5nHfeeaxatYqdO3fuOR/A4MGDqVWrFgDDhg3jrrvu4v7772fs2LF7PJ3qRkr8sz38lSDmzLF58bepli2Lxhxv3tyqDUfS3UHXrtbX16mnRtdOJ3WIdDStKDNlyhQ++ugjpk+fTnZ2Nv369eOYY45h4cKFEZ9DwtpT5Yd+/8Do0aO5/vrrGTx4MFOmTOH222/fsy0np6i7ruzsbAYMGMCkSZOYMGECX375ZdU+VJKSEjkVD3/FAFUbbGrt2tL3CcV7i+dKWrYsWm7e3Aa3qlev/GtmZcH333vbFCfubNq0iQYNGpCdnc3ChQv57LPPyM/PZ+rUqXz//fcAe8JfAwYM4PHHH99zbCj81bRpUxYsWEBhYeEejyd07hYtWgAwLjT6ZylcccUVXH311XTv3r1cDyhVSSlR8fBXFHn7bauRVdafYM4cq+lVvEv3cFFp1qxi183JsaS948SRQYMGUVBQQLt27bj55ps57rjjaNKkCWPGjOEXv/gFnTp14rzzzgPgtttuY8OGDXTo0IFOnTrx8ccfA3DPPfdwxhlncPzxx9O8efM957799tsZNmwYXbt2pXE51eS7du1K3bp1GTkydgMvJppyu74XkbbAy2FFhwJ/BJ4LylsBS4FzVbXMjFZlu76fMcO6l3rrLcsbO1WkoMBCWrm5cMMNNk56SXTtao0UQx1Ehli8uGgY3vXrraqw45SCd31fxMqVK+nXrx8LFy4krZSXq2rf9b2qfqOqx6jqMUBXbHzj14Gbgcmq2gaYHKzHBA9/RZlx40xQRGDdupL3KSiwWl0l1U4JXH1q1Ij+QFeOU0157rnnOPbYY/nzn/9cqqBUByqaqO8PfKeqP4jIEKBfUD4OmALcFD3TivDwVxRRtVbxXbrYcmmismgR7NhRsqjUqmUeTE6OdwbpOBFyySWXcMkllyTajJhTUbk8H3gpWG6qqkE/HawGYjaWqtf+iiKffAILFsBvf2u5kpJEZdcuePppWy6tQWPLlpakdxzHCSNiT0VEagCDgVuKb1NVFZESkzMiMgoYBVAj5HJUEA9/VYA//tFqWd16a8nbn3gC6te3oX6nTDGBCfH22/CrX9nAWBs3wtChcNRRJZ/nttu8rYkTMaq6V5Vcp2TiObx7rKiIp3Iq8JWqBt3UskZEmgME8xLrpqrqGFXtpqrdMir5EPLwVwV44QV45519y3fvtlbxr74Kl14K2dnWZiTcU5k0ycTk3HNt+ZVXSh8V7Zxz4KyzYvIRnOpFzZo1Wb9+fbV4YMYSVWX9+vXUrFkz0aZUiYo85YdTFPoCeBMYAdwTzCdF0a69cE+lGLt2FcUEi5f/8IO1GynOrbfCvffCSSfBLYGz2aQJbNtmU3a2tUvp3t2H7HWiSsuWLcnLy2Ndafk7Zw81a9akZXiV/RQkIlERkRxgABDWVwf3ABNE5HLgB+Dc6JtneE4ljHHjYPRo69Cxbl0r27rVPIoVK8wj2bx53+P+8x/o0wc++qgoud6kic3XrbMcydy5e3fH4jhRIDMzc6+uS5zqTUSioqpbgUbFytZjtcFijoe/AlStTcnmzdYnV9euVn7yyTbO+4UX2npxUVG13MnIkXvX1goXlR07YPt27+jRcZwqkRKVpd1TCfj0UxsMC6y7E4ANG+Czz+Djj63nYNhXVPLyLPnevv3e5eGiUlqXLI7jOBUgJUQlLc2iO/u9qDzxhLUNAVi61Ob/+5/Nly+H6dNteceOvd263Fyblycq6en77uM4jlMBUkJUwEJg+3X4q6AAXn8dLr7YqgSHRGXatKJ93nijaHnLlqLlSERlzhwb2THFa544jpNYUkpU9mtP5dtvLefRsye0bl0U/po2DTp0sFxJeNgrfDk31wSkeGd3detabDHkqXjoy3GcKpIyopKZuZ+Iiio89hgsW7Z3eWhsk44doVUr81Ty8+GLL2y431AHdAceaPMtW6xx46uvmqiUFNYSMbGZMcOu1717jD6U4zj7CykjKvuNpzJvHlx9tY0HH87s2daCvV27IlGZOdNuSu/eRYJwzDE237wZ/vxnGDbM9istV9KkiVU3Bm/M6DhOlUkpUdkvciqhbuZfe23v0RRnzzZBycqy8Ne2bfDss5Zc79MHunWz/UJVgjdvtm7pVU14yhIVsOrJ3pbAcZwqkjKiUm3DXxMmwP/7f0XrH3xg1d3WrLEqxCHmzCnKebRqZfPnn4cTT7TxTM4+2/rqGjjQtm3eDD/9ZC3ohwwpfQjfkKicc05UP5bjOPsnKSMq1Tb89cQT8Ne/2vL27TB1Klx2mdXCmjjRyn/6ydqahLyQkKjs3FkkBi1a2P6hLh62bLHjOna0WmGHHVby9UOiMnRo1D+a4zj7HynTzWy1DX/l5lrtq4IC65Y+Px9+8QsLXb3wAgweXNSpY3FRSUvbNw9Sp47Nf/rJvJXyRmUcORIOOqhoJEfHcZwqkDKiUi3DXz/+CGuDzp3XroXJk+2D9u1rnsfQoTBgQNGY7qHwV506Vj24Q4d9x48PicoPP9i8PFE55pii5L7jOE4VSRlRqZbhr/CxTFatsrYobdpYq/mOHa2Dx6eesm2tW0OzZkX7P/+8eRjFqVXLRChUJblRo333cRzHiREuKokk1NIdTDiWLYODDy4qq1kTrrqq5GMHDSq5XMS6vo/UU3Ecx4kiKZOor5bhr3BPZfXqfUWlstSpU9SNi4uK41R7RGSQiHwjIotF5OYSth8sIh+LyNciMkdETouVLSkjKtXWUwnlSZYssYR9NESldm3L14CLiuNUc0QkHXgcG523PTBcRIo3TLsNmKCqnYHzgb/Hyp6UEpVqV/srNxc6d7YH/xdfWFm0PJUQnlNxnOpOD2Cxqi5R1Z3AeGBIsX0UCEb1ox6wMlbGRCQqIlJfRCaKyEIRWSAiPUWkoYh8KCLfBvMGsTISqqGnsmmTjdTYvr0l4GfMsPJoikp6etHokI7jVFdaAMvD1vOCsnBuBy4SkTzgHWB0rIyJ1FP5K/Ceqh4JdAIWADcDk1W1DTA5WI8Z1S6nMmuWzTt0gObN4eefbT2aotKgwd4jPTqOk6pkiMjMsGlUBY8fDjyrqi2B04DnRSQmkapya3+JSD2gL3ApQOBe7RSRIUC/YLdxwBTgplgYCdUw/BUaB6VnTxMVMAFoUfwFoxLUrm1zz6c4TnWhQFW7lbJtBRDevqBlUBbO5cAgAFWdLiI1gcbA2mgbGolStQbWAc8ENQeeEpEcoKmqrgr2WQ00LelgERkVUteCgoJKG1rtwl+hcVAaNCgSlebN7YNWlZCn4vkUx9kfmAG0EZHWIlIDS8S/WWyfZUB/ABFpB9TEnutRJxJRyQC6AE8ENQe2UizUpaqKJYL2QVXHqGo3Ve2WkVH5ZjHVKvy1e7cNA9y7t62HGjVGI/QFRaLinorjVHtUtQC4CngfS01MUNX5InKniAwOdrsBuFJEZgMvAZcGz+2oE8lTPg/IU9XPg/WJmKisEZHmqrpKRJoTAzcqnGoV/po3z3IoIVEJeSouKo7jVAJVfQdLwIeX/TFsORfoFQ9byvVUVHU1sFxE2gZF/YFczL0aEZSNACbFxMKAahX+CuVTYiUqnlNxHCdBRBqPGg28EMTrlgAjMUGaICKXAz8A58bGRCMz06JGu3cXddqbUlx7LWRn29gp06ZZF/UhEYmVp+I5Fcdx4kxEoqKqs4CSah70j645pRPKX+/alYSi0ru39Sh83XUlb9+0ycZNqVfPhvj95BMbrTFU3feII2xMlQsuiI49Hv5yHCdBpFSLekjCvMqPP9oIjffeW3p87t//tm3r1sFHH1mjx1DoC0xcrr46ep6Fh78cx0kQKSMqmZk2j1teZds22LCh/P1CnUKuWWMjLIazaZOdZ+JEC30B3HOPzcNFJdoccIDNQ2E1x3GcOJEyohLyVOImKldeCd27WxKnLELd1zdsaCGucE46yUZpfPddGyK4USP4z3+s65QOHWJiNmB2f/wxnHBC7K7hOI5TAiknKnEJf23dCq+/Dt99B++8U/a+ubkWbrrhBpgypWgck2XL4KuvoLAQduywfEmvoEZfz56xTQyJQL9+3kWL4zhxJ2VEJa7hr3ffhe3bTcmKex/Fyc2Fdu3g7LNt/cMP955//LF1a9+zZ1HIK5ahL8dxnASSMqIS1/DXxInQpAn87nfw3nsmCqWRm2s9DR95pPXb9cEHVv7hh3DggRbmat3aygYNsg9y6qmx/wyO4zgJIOVEJebhr+3brbbW2WfDr38NWVkwYkTJarZxI6xcaaIiAgMHWu2uXbtMVAYO3DsEdfTRsGULdO0a4w/hOI6TGFJGVOIW/vrsM8upDB5snsfYsdZYsaQ2KKGaX+2DQdYGDrQaY/fcAz/9BAMG7HtM6IM4juNUQ1JGVOIW/po50+bHHmvz4cNh5EgTl8LCvfcN1fwKicrJJ5tn8sc/miANGhRjYx3HcZKLlBOVmIe/ZsywasCNGxeVHXcc5OfD8uXWGv6mYNiYmTOtevAhh9h648ZwzTXm1cyb540PHcfZ76h8X/RxJm7hr5kzrZ1HOEccYfNFi+D5520aNcrCYscfv3f14IcfjrGBjuM4yUvKeSoxFZUff4Tvv4duxbo5CxeV2bNtecIE80a8erDjOM4eXFTC+fJLmxf3VJo3twaO8+YVJecffNDmLiqO4zh7SJnwV1aWzXfsiMHJlyyxXoObNLH14lV+RcxbeestS+o0bmxeTWbmvgLkOI6zH5MynkrNmjaPiah88om1N5k921rH16277z5HHGG9CwNcdZXNu3Yt6ijScRzHiUxURGSpiMwVkVkiMjMoaygiH4rIt8G8QSwNDXkq+fkxOHlursXXPv8cXn655H1CeZWsLGsUmZHhHTY6juMUoyLhrxNV9cew9ZuByap6j4jcHKzfFFXrwgh5KjETlbZtoUeP0vcJicpRR1nX8p9/DocfHgNjHMdxUpeqhL+GAOOC5XHAWVU3p3RimlMJ9d9VFm3b2rxTJ5t36VJymMxxHGc/JlJRUeADEflSREYFZU1VdVWwvBpoGnXrwohZ+GvbNqtGHImo1K0LfftG2QDHcZzqQ6Thr96qukJEDgA+FJGF4RtVVUVESzowEKFRADVC9YIrQXq6VbaKuqfyzTegWr6o1KkDeXlFQ/U6juM4+xCRp6KqK4L5WuB1oAewRkSaAwTztaUcO0ZVu6lqt4yMqtVgzsqKgadSvP+usqhTxwe+chzHKYNyRUVEckSkTmgZGAjMA94ERgS7jQAmxcrIEDVrxkhUMjI86e44jhMFInEdmgKvi72hZwAvqup7IjIDmCAilwM/AOfGzkyjZs0YhL/mz4c2bYqa7DuO4ziVplxRUdUlQKcSytcD/WNhVGlEPfxVWAjTp5c87onjOI5TYVKmRT1EIfyVm2vdqnTsCI8+CnPmwNq1NriW4ziOU2VSSlSysqoY/po61bq237wZ/vQneOMNK3dPxXEcZw8icnRlj00pUamyp7Junc2ffNLGl7/3Xhs3vnnzqNjnOI5TTfi7iHwhIr8RkXoVOTDlRKVKnsq6dVCvHvTvb1WI8/PdS3EcxymGqvYBLgQOAr4UkRdFJKKHZUqJSpUT9evWWff2ItYpJMApp0TFNsdxnOqEqn4L3Ib16XgC8KiILBSRX5R1XMqMpwJRCn+Fxkz55S/h4IPdU3EcxymGiHQERgKnAx8CZ6rqVyJyIDAdeK20Y1NOVKoc/mrVypYzM2Hw4GiY5TiOU914DHgK+D9V3R4qVNWVInJbWQfun+Evx3Ecp0REJB1YoarPhwtKCFV9vqzjU0pUqhT+UrUhgF1UHMdxSkVVdwMHiUiluhnZf8JfmzbZ+PIuKo7jOOXxPfCpiLwJbA0VqupD5R2YUqJSpfBXqI2Ki4rjOE55fBdMaUCdihyYUqJSsybs3GlddqVVNHDnouI4jhMRqnpHZY9NqZxKaPTHnTsjPEAVXnsNbrwRVq+2MhcVx3GqGSIySES+EZHFInJzKfucKyK5IjJfRF4s53xNROR+EXlHRP4TmiKxJeU8FbAQWGi5TC6/HJ55xpZ//tnmLiqO41QjgtpajwMDgDxghoi8qaq5Yfu0AW4BeqnqhmAU37J4AXgZOAP4FTZm1rpI7EkpTyUkJBEl6wsL4bnn4PTTbX1SMIaYi4rjONWLHsBiVV2iqjuB8cCQYvtcCTyuqhtgzyi+ZdFIVZ8Gdqnqf1X1MuCkSIxJKVEJhb8iStZv2AC7d1u39ocdZl3c5+RArVoxtdFxHCcGZIjIzLBpVNi2FsDysPW8oCycI4AjRORTEflMRAaVc71dwXyViJwuIp2BhhEZGslOsMfFmok1ijlDRFpjitgI+BK4OFDJmBEe/iqXNWtsfsAB0Ls3fPedeymO46QqBararQrHZwBtgH5AS2CqiBytqhtL2f/uoHfiG7DW9XWB6yK5UEU8lWuABWHr9wIPq+rhwAbg8gqcq1JUKPy1NvDuQqICLj0j5bAAACAASURBVCqO41RHVmC9CYdoGZSFkwe8qaq7VPV7YBEmMiWiqv9W1U2qOk9VT1TVrqr6ZiTGROSpiEhLrGOxPwPXiw1YfxJwQbDLOOB24IlIzldZKhT+CheVAw+0ZRcVx3GqHzOANkH0aAVwPkXP5hBvAMOBZ0SkMRYOW1LaCUXkGUCLlwe5lTKJNPz1CPB7ihrBNAI2qmpBsF5SDC/qVCj8FRKVpk2hcWNo1sx6JXYcx6lGqGqBiFwFvA+kA2NVdb6I3AnMDDyM94GBIpIL7AZ+p6rryzjtv8OWawJnAysjsadcURGRM4C1qvqliPSL5KTFjh8FjAKoUaNSXcnsIeSpRBz+SkuDhg1t/JRPPoEGDap0fcdxnGREVd8B3ilW9sewZQWuD6ZIzvdq+LqIvARMi+TYSDyVXsBgETkNU6y6wF+B+iKSEXgrJcXwQsaNAcYA5OTk7ONOVYQKeyqNG0N6uq0ffnhVLu04jrM/0wYor20LEEGiXlVvUdWWqtoKi9X9R1UvBD4Gzgl2GwFMqpytkVPhRP0BEd0Dx3EcJwwR2SwiP4cm4C1sBMhyqUqL+puA8SJyN/A18HQVzhURFU7Uu6g4juNUGFWtUCeS4VSo8aOqTlHVM4LlJaraQ1UPV9VhqlqVMRkjosLhLxcVx3GcCiMiZwftVELr9UXkrEiOTakW9R7+chzHiQt/UtVNoZWgkeSfIjkwpUQl4vBXfr4NyuWi4jiOUxlK0oaI0iUpJSoRh79CY6e4qDiO41SGmSLykIgcFkwPYd1xlUtKiUpmps3LDX+Ft6Z3HMdxKspoYCfW/f14IB/4bSQHptR4KiLmrZTrqbioOI7jVBpV3QqUONhXeaSUpwIVFJWmTWNuj+M4TnVDRD4Ukfph6w1E5P1Ijk05UcnKiiD8tXSpzd1TcRzHqQyNw7vFDwb3ik6L+mSjXE+lsBCef966u69dO252OY7jVCMKRWRPD7wi0ooSei0uiZTKqYCJSpmeykcf2YBcd94ZN5scx3GqGbcC00Tkv4AAfQg6Bi6PlBOVrKxyPJUnnrBxU4YOjZtNjuM41QlVfU9EumFC8jU2Hsv2SI5NOVEpM/y1fTu89RZcc01RS0nHcRynQojIFdhovy2BWcBxwHRscMYySbmcSpmJ+m++gd274bjj4mqT4zhONeMaoDvwg6qeCHQGShvPfi9STlTK9FRyc23evn3c7HEcx6mG5KtqPoCIZKnqQqBtJAemZPgr1AvLPuTm2qBcbdrE1SbHcZxqRl7QTuUN4EMR2QD8EMmBKScqZSbqc3NNUKo4bLHjOM7+jKqeHSzeLiIfA/WA9yI5ttzwl4jUFJEvRGS2iMwXkTuC8tYi8rmILBaRl0UkLk/ynBzYujWsoLAQRo+G2bNNVDz05TiOEzVU9b+q+qaq7oxk/0hyKjuAk1S1E3AMMEhEjgPuBR5W1cOBDcDllTW6ItSrBz//HFaQlwd/+xvcdBMsXuyi4jiOk0AiGaNeVXVLsJoZTIpVLZsYlI8DIhoVrKrUrWuiUlgYFOTl2fz9963ml4uK4zhOwoio9peIpIvILGAt8CHwHbBRVQuCXfKAFrExcW/q1QPVsBBYSFRCuKg4juMkjIhERVV3q+oxWEOYHsCRkV5AREaJyEwRmVlQUFD+AeVQt67NN4UGugyJSu/ekJYGRxxR5Ws4juM4laNC7VSCXis/BnoC9UUkVHusJbCilGPGqGo3Ve2WkVH1ymb16tl8L1HJzrZOJCdOhFq1qnwNx3Ecp3JEUvurSahffRGpBQwAFmDick6w2whgUqyMDCckKnuS9Xl50LIltGoFZ59d2mGO4zhOHIjEdWgOjBORdEyEJqjqv0UkFxgvIndjHY49HUM791Bi+Ougg+JxacdxHKccyhUVVZ2D9ftSvHwJll+JKyV6KieV28eZ4ziOEwdSru+vvTyV3bth5UoLfzmO4zgJJ+VEZS9PZc0aExYXFcdxnKQg5USldm0QCTyVUHViFxXHcZykIOVEJS0N6tQJPBUXFcdxnKQi5UQFLATmnorjOE7ykZKiUrdumKhkZUGjRok2yXEcxyFFRWVPT8XLllkbFZFEm+Q4juOQgoN0gXkqP/4IbP4eWrdOtDmO4zhOQGp7KkuXWvcsjuM4TlKQkqJSty7s3LgN1q51UXEcx0kiUlJU6tWDBpuW2oqHvxzH2c8RkUEi8k0wvPvNZew3VERURLrFypaUFZVmO5bainsqjuPsxwSd/T4OnAq0B4aLyD6jFYpIHeAa4PNY2pOSolK3LrTme1txUXEcZ/+mB7BYVZeo6k5gPDCkhP3uAu4F8mNpTEqKSr160IqlFGbVhGbNEm2O4zhOImkBLA9b32d4dxHpAhykqm/H2piUrVKczVJ2Nj+Emt5GxXGc6k+GiMwMWx+jqmMiOVBE0oCHgEtjYVhxUlJU6tWDunzPtiatqJloYxzHcWJPgaqWllxfAYSPVFh8ePc6QAdgithLeDPgTREZrKrhQhUVIhlO+CAR+VhEckVkvohcE5Q3FJEPReTbYN4g2saVRt26Fv76uZHX/HIcZ79nBtBGRFqLSA3gfODN0EZV3aSqjVW1laq2Aj4DYiIoEFlOpQC4QVXbA8cBvw1qFtwMTFbVNsDkYD0u1E/fTGPW81PdVvG6pOM4TlKiqgXAVcD7wAJsyPf5InKniAyOtz2RDCe8ClgVLG8WkQVYEmgI0C/YbRwwBbgpJlYWo/msdwH4rm5nusTjgo7jOEmMqr4DvFOs7I+l7NsvlrZUqPaXiLTCxqv/HGgaCA7AaqBpVC0rg+znnmCptGJ6dv94XdJxHMeJgIhFRURqA68C16rqz+HbVFUBLeW4USIyU0RmFhQUVMlYABYsQKZMYWLDX5K3Kr3q53Mcx3GiRkSiIiKZmKC8oKqvBcVrRKR5sL05sLakY1V1jKp2U9VuGRlRqGz21FOQmcmnbS9j5cqqn85xHMeJHpHU/hLgaWCBqj4UtulNYESwPAKYFH3zSuDTT6FXL3JaH8CKFeXv7jiO48SPSDyVXsDFwEkiMiuYTgPuAQaIyLfAycF6bCkshLlzoVMnWrSAlStBSwy6OY7jOIkgktpf04DSmq3HN1P+3XewbRt07EiLLbBzpw3W1aRJXK1wHMdxSiG1+v6aPdvmgacCeAjMcRwniUgtUZkzB9LSoH17FxXHcZwkJLVEZfZsaNsWatVyUXEcx0lCUktU5syBTp0A6/FexEXFcRwnmUgdUdm0CZYuhY4dAcjMhKZNXVQcx3GSidQRle+DkR6POGJPUYsWLiqO4zjJROqIytatNq9TZ0+Ri4rjOE5ykTqism2bzXNy9hQddBAsW+YNIB3HcZKF1BOV7Ow9RYcdZqmW9esTZJPjOI6zFyktKm3a2PzbbxNgj+M4jrMPqSMqoZxKmKiEcvYuKo7jOMlB6ohKCTmV1q0hPd1FxXEcJ1lIPVEJ81QyM6FVKxcVx3GcZCG1REUEsrL2Km7TBhYtSpBNjuM4zl6kjqhs3WpeiuzdC3+bNuapeLVix3GcxJM6orJt2175lBBt2sCWLbBmTQJschzHcfYikuGEx4rIWhGZF1bWUEQ+FJFvg3mD2JqJiUpYPiWEVyt2HMdJHiLxVJ4FBhUruxmYrKptgMnBemwpRVRC1Yq/+SbmFjiO4zjlUK6oqOpU4KdixUOAccHyOOCsKNu1L6GcSjEOOcR6K/7gg5hb4DiO45RDZXMqTVV1VbC8Gmha2o4iMkpEZorIzIKCgkpejlJzKunp8ItfwNtvF9U6dhzHcRJDlRP1qqpAqXWvVHWMqnZT1W4ZGRmVv1Ap4S+AYcNs8zvvVP70juM4TtWprKisEZHmAMF8bfRMKoUyRKVvXzjgAHjllZhb4TiO45RBZUXlTWBEsDwCmBQdc8qgDFEJhcD+/W/YvDnmljiO4zilEEmV4peA6UBbEckTkcuBe4ABIvItcHKwHlu2bi0xpxLi4otNdyZOjLkljuM4TimUm+RQ1eGlbOofZVvKpgxPBaBnT6te/MwzMHJkHO1yHMdx9pAaLepVyxUVEbj0UvjkE1i8OH6mOY7jOEWkhqjs3AmFhWWKCsAll0BaGvz973Gyy3Ecx9mL1BCV0ABdZeRUAFq0MGH5+98hLy8OdjmO4zh7kRqiUsJYKqVx++0WLbvjjtia5DiO4+xLtROVQw6BX/8axo6F6dNjbJfjOI6zF9VOVMC8lIMOsmrG3m7FcRwnfqSGqESYUwlRrx48/zx8/z2MGAFV6XLMcRzHiZzUEJUKeioAffrAww/D669bu5Vdu2Jkm+M4jrOHaisqAFdfDXffDf/6l/UPtnRp9E1zHMdJNCIySES+EZHFIrLP+FYicr2I5IrIHBGZLCKHxMqWai0qALfeChMmQG4udO5snovjOE51QUTSgceBU4H2wHARaV9st6+BbqraEZgI3Bcre1JDVCqYUynOsGHw9ddw+OHW8eTgwfDppx4ScxynWtADWKyqS1R1JzAeG0hxD6r6saqGRpz6DGgZK2NSQ1Sq4KmEOPRQE5J77oEpU6B3b2jUCC66CJ58Et59F3bsiI65juM4caQFsDxsPS8oK43LgXdjZUwVRs2KI1EQFYAaNeCmm2DUKJg8Gd57z8JhL7xg21u2hOHDrTryWWfZvCwKC60HmZo1q2SW4zhOeWSIyMyw9TGqOqaiJxGRi4BuwAlRs6wYqSMqIpCVFZXTNWgA55xj0z/+AStXwpw58MADVmOsoACuuw66d4fatU00GjSwXpCPPBK2b4fx483z2bYNTj8dTj0V2rSB9ett/wMOgC1b4LDDrEGm4zhOFShQ1W6lbFsBhL8CtwzK9kJETgZuBU5Q1ZjFZcRGA44POTk5ujWUH6kIN94I//xnXFoyqsKSJRYSmzED8vNNRNat27s/sVat4JRTTOcmTIDVq0s/Z79+0KMH1K9vIbZ27UywWrc2rdywAZ57zrrv79Gj6p+hsBA+/xy6djXvzHGc1EZEtqlqiUllEckAFmHDkawAZgAXqOr8sH06Ywn6Qar6bUxtrYqoiMgg4K9AOvCUqpY5WFelReXXv4bXXoM1ayplZ7TYuhUWLbIEf7du1iMyFAnR0qXQpEmRCOXkWFf8r74KCxbsWzGgUSMTl6+/LvpoXbpAw4bmPeXl2bkaNIC2beHcc80bWrUKpk614wcONGErKLARMNu3h2uvhRdfNNH6zW9MXDp1svOGs3s33HmnjZjZp49VYujdu+hzOY6THJQlKsH204BHsGfxWFX9s4jcCcxU1TdF5CPgaGBVcMgyVR0cE1srKypBNbZFwAAsMTQDGK6quaUdU2lRGTHCnqLff18pW5OBggITlfR0mDcPvvjCPKEZM6wHgL/8BaZNg48+MvFq3hwOPthCaT/9ZJ7HnDlF52vY0By30mqwjR4N//sffPllUVlOjtlRuzY0bWpezKxZJmS5ueaV1a9vItW4sfX6nJ1tthxyiInQjh2waRPMnm12Dx1qntG2bZCZaTZlZFjYLz/fPktWln2OlSvh/fehVi0LJbZtayJ5wAEmqmlpdt3Gjc3erVvtuJwcE+5Fi8yza9vWhc/ZvyhPVJKJqohKT+B2VT0lWL8FQFX/UtoxlRaVYcPsqTd/fvn7VmMWLjSBqV/fcjubNxeJRkaGeTUzZ8LRR1u1abCH9ezZJh6rV9t+W7faA37ZMrjiCvjVr0wU3njDhK2wENauhRUr7Jx5ebBxY5Ed2dnQoYOVr1xZsc/QqpXNf/jBhCJS0tLMLoC6dU1sdu60adcu+1wHHGDl6emlT2lp9lk2bTLRrl3bPvv27TbftQsOPNCE7OefbXt2tl07fAITuIpMaWkVPyYe54q3bSVtDxFaLqkskuXKHgf2ewz9JkPLpa2L2EtZZqb99iI5//HHV7pVxH4jKudg8bkrgvWLgWNV9arSjqm0qJx+usWTvviiUrY6VUPVhCgz0/5IoT/K7t3mddWpYw/enTtteccOCwdmZ9ufaOdO81rq1DHPRMTWFy+2ae1aaNbMrvPjjzaJ2PH5+faw37nTPJSCAhPS3buL/tQ1atj2tWvt2rt3lzwVFtq8Xj2zZdkyE5PsbPOesrNNeEJiWreufe5t2+xBGBKl0OcPf9BEOhUWVu64aJ/LiT8LFtjLYGVIJVGJee0vERkFjAKoUdms8QMP2NPFSQgi9sZenPR0y9WURLNmZZ+zZk3zdjp0qLg9I0dW/BhnXxIpeOHbw+0Jn1d0uSrHhXs6xZdLWi8sNK925869O6wt7VpgnvH+QFVEJaJqbEFd6jFgnkqlrtSuXaUOcxyndIqHnhwnGlQl3TkDaCMirUWkBnA+8GZ0zHIcx3FSkUp7KqpaICJXAe9TVI1t/86kO47j7OekRuNHx3Gc/ZhUStR7bX/HcRwnarioOI7jOFHDRcVxHMeJGi4qjuM4TtSIa6JeRAqB7ZU8PAMoKHev+JOsdkHy2uZ2VQy3q+Ikq22VtauWqqaEExBXUakKIjKzjPEEEkay2gXJa5vbVTHcroqTrLYlq13RJCWUz3Ecx0kNXFQcx3GcqJFKolLh8ZjjRLLaBclrm9tVMdyuipOstiWrXVEjZXIqjuM4TvKTSp6K4ziOk+S4qDiO4zhRw0UlBoj4KBUVJVnvWbLa5VRfUv03l9SiIiJtRaSniGSKSHqi7SkPEWkkIjmahIkqEckWkaxE21GcZL5nAMlqVzI/eJLVtiS2q6OI9BeRZiKSqaqarLZGQsyHE64sIvIL4P9ho0muAGaKyLOq+nNiLSuZwN5fATVE5AVgjqp+nmCzgD22XQTUF5GHgFxVXZJgs5L6ngGISD/gFGxAuiWqOiuxFu1FBrArtCIikkQCKMAeW5LItqS7ZyJyFvAX4FtgHfCjiNylqluSwb7KkJS1v0QkE/gX8KiqfioiQ4HjgJ3AvckmLCJyIPAxMBxoDHQDDgZeVdUPE2xba2wgtQuBtkBPYC3wpqp+nUC7kvaeAYjIScALwINAG6AW8F9VfTqhhgEicirwS0zsVodsSoaHkIgMwL7Tz4DlqvpuMtiWjPdMRNKAccALqvqeiPQEzgEaAVelqrAkc/irLvZnBngd+DeQCVyQhK5hJrBMVb9S1Q+A8cBs4Bci0jWxplEXyFPVGar6L+AZ7I3tTBE5JIF2JfM9A2gO3K+qDwB/Al4EhojIZYk0SkS6A49i9+sb4LeB90miwyYi0ht4GhOUBsA1IvJ/IdsSaFey3rM0zKNrEax/AfwdWA/cLCIZqSYokKSioqq7gIewB0wfVS0EpgGzgN4JNa4EVPUHYIOIPBisLwE+ANYAR0Pi4rmqOhvYKCKjg/WZwJvAQcCRibApsCNp71lAFnB+8MdeDUwFngD6iEi7BNqVhnlM41V1InAycEbYfUzkQ6g28JKqjgH+CtwIDBaRWxJoEyTZPRORHBGpqaoF2EveNSJysqruBn7A/p8HAnXiaVe0SEpRCfgEe8hcLCJ9VXW3qr6I3exOiTXN3HwR+a2IXBcU3QNkiMjvAFT1O8zVPj/4AcXthysi/UTkXBG5OCh6DjhERM4PbJsBTAd+E4Qa42VX0t6zwL5DRKRDYMtY7I37GRHJUtVtmCeVAbSKp13F2A4cKCJNAVT1Jyw03FdELkqgXWBv3SeISA1VzVfVecAVQG8R6Z9Au5LmngV5xOeBd0VkCJZLuQO4TkQGqGqBqv4X814S+fJSaZJWVFQ1H4tpzwZuEZFRIjICaAqsSqRtgZv/IpAPnCMiDwMNgclASxF5NNi1NpYYjFvNNRE5EXgJy09cG7j53wDfA91F5IZg1+3AFiypGg+7kvaeBfYNxV5i/iYiE0TkTMwzWQmMDR6UK7HfXuc429ZFRIaIyEGqOgf4HPhIRGrAnofk37DcVFwRkSNFpE/g0b2PCfFkEakZ7LIEmImFE+NpV9LdsyC/eQ+WmH8aGAiMBH4M1h8RkV+JyG8xUfkhXrZFk6RM1IcT/Ah6YUm2fOCviUwwBzZdD9RR1TuCP88fsDfY97Ek+J+AHCzEdEm87A3CRfcCq1T14cC2Z4ClwLPAodh9rIP9yS+Mo21Jec8C23IwwbtLVWeKyLWBHYuB/wBXAcdiFQsuBE5S1UVxsm0w8AAW+t2BhQdvwh5Mg4DTVXV54O0dA1wCFMbDywveuu/BXlg2YiHCfwH/h4WpB6jqNhG5HXuBuAZiH25K1nsmIp2Ah1X1pGC9OzA4sPGfmGcyFKsU8ngQuk49VDUlJuzNNS3BNoREuD/wLnBEsJ6F/WAfCdv3AKBeAmw8H/gH0DRYzwYmYD/m0D5tgEZ+z/ZctxbwX+C8sLILsZpfpwTr52APgLZxtu0JYHCw3CW4Z88F/4fbgEnAy8ACoEMc7crAIgl9gvWzgPuBuzFv82EshD0G81ba7e/3LLDnNWB02HoP4EngtGBd4mlPLKak91SSBRE5Fvsj/Q9ohiUh5wPvquqq4O37E+CfqvpUnG07CHvbT8MezP8P80ymqep2EckObLtbVV+Po11Je88C+wR7UdktIucAJwJPq+pXwbbfAceq6tB42xbYl449IL9T1XuDsoOwtj1pqnqLiISqO29Sq/wQL9sysVqZ76jq34Oy3pjwfquqT4pIL+w3uVItXxYPu5Lqnom1dToAyFLV58XapfQGZqrq+GCfyzBRPkdVd8bSnniQtDmVZEJETsHqk+ersQqrjdYLOE1EjlTLAb2JtaWJp22nYx7AY8DY4PovYaGGPiLSXC3JPDmetiXzPQvsG4Ldr6eCh9/nmDAPFpGugc33YQ1GD4uzbZ1FpIVabaC/AcNFZFiwOQ94C6t4cYCqfquqc+IlKCJSI6hEsQu4DzhFRE4ONn8KfBmUiap+qqqfxENQkvGeVTC/uTmWtsSVRLtKyT5hbxUrgBOD9bph23phIZL/YlUo1xAnNx9LsB8EzAX6YRUYfg8sx5J8p2Mu/3OY+59HEHqKg23HYQntpLpnYTZ0AhYCpwG/Du7hWZin8kcsHHEx1ohvPtAwjradgiW7jworOwt4Bzg3rOwtLGcRz/s2FJiIVWg4E6sFdzkW4hoQtt9HQLf9+Z4F/8/7gOuC9ZqYwPwFa4R8KvAG9rKXC3SO53cZyylpu2lJIjpib2DrxRoL/kVEtmLJ7htU9QYR6YP9UB5T1cXxMErtl7pcRKYDi4C1qnqfiBRg4abjgK+B7thDtL/GIbksIq0wIZ6CdTmRNPcsjGbAQlV9J7D5BywZ/89gOhYYhb09XqRWUyjmiMgZWOjyUlWdLyJpqlqoqm+IiAJ3icgRWK29wzFhjAvBde8GLsPE5ErMQ14EFGJVYtsDP2MvNXlxsisp75mqqoh8BfQTkaaqukZELsfC0r9S1euwasVtgJ9UdX087IoLiVa1ZJ2wH2B77AF0LRanzQOuxpJrt2E1l+omwLYzgeuwVunjgf8rtv0WLLSTFWe7TsHevLpgMeyxyXLPitnZFPPgjiWo/IF5LfOBnsF6FlAjjjalYYK2KFivjdXkexo4IyjrgNW2egDoFOd7dhwwJWz9eMxD+RVWk7AXlrh/kji+dQf3J2nuGRY9yMJyNocE92QAUCvYno2FCM+O5/cXz8k9lRIIe/vZiMVAnwV2A1+rtRZGRFZiVXR3xNm2gcBdwE2quktEbgamishuDRKTmJv9f8Q3hzIQ+0M3BIaq6q1BdfDpqvpksE9C7llw7WOxEMRWtWrDS4HzgDUislxV3xGRw4FhIvKZqsbVRlUtFJGrgIdF5DMsfDIRE+UbxXpyfhm4OZ52iUi2Wk7uC+B7ETkXeE1V/xdUZrgN+EFV3w3sVrUeMGJtVxtV/RbzmB5JhnsW5DfvxSIFdYDrKcpviojMVaugEtf8ZtxJtKol24S9gS0geNvCquc+Fixnhe13IRbiqR9n29YAPYL1xljtqi5YDuN64AjgUqzBWYM42XUy1qbjKKAGFlPvinlSNRN5z4Lrnoq1XB6DVQx4KOy7fYiiarFXYe0D4mlbFyxceGywLlgFhz+G7XNBYHdmnG07BasBVwvzpK7C3vj7hWwBRmBV1uNmG/bm/yNwebAeao+VkHtGEuc3EzG5p1Iy92pR47s/AE+LtajeARDERq8GLlDVjXG0az3W2ry5iDQCXgEKsLDNU9iDvA3W4+9IVd0QJ7vSsQaL80WkPibKx6rql0GOBxG5Evgt1uAybvcsqGI6ArhTrUpnXeADEXlSVa8UkT8AvxSRW7EHwwVxtO0MzOucC9QSkY9U9Z8icqXuXbW0Bvbdx9wDCLPtVCxsdI2qbg/KngVuAIZg9+p5rGuW/HjZJiKDArvewR7eqGq+iPxK9/Yu43bPVFUDL3w69vKSFPnNhJFoVUu2CXtA1g1bbon9IJoEZYdib7dHJsi+TlhjsjzM9U/DksqPAwcF+8TFQynBtlB+YhCwGjg6WM/BasLEtZZXmF03ARcXK/sf1gsxWI+6fYCWcbSpMzCHIM4PDCNoCEpYAzisG48ZxLdhY3us2uuoYL1RUNYqWL8Iazk/BRPEuORQMC/ga+zlqUnwGxtYwn5xu2dY7rV7cI9eBn5fbHtC8puJnBJuQDJPWGipNjA5WL8Ii5kmOtHcHhtvIbzsfaBLsJzwVrnAncEfKj1Yj2tvCISFF4LvbR5wcFhZY6x1c/sE3Z/jsVpAofXDsbzFQaHvD/M6x4bEOY62dcW6YL8ieEH4KHhg/gfz4kP7HQ0cEEe7TiMIEwbrVwX3p15Y2ZFYZYGY3zPgjODF4L9Y25jBWJdIt4Tt0woLuyb8Pxm37ynRBqTChCXq/4LV2uiYaHtKsG9oYFvTRNtSzKZpQEYCrn0GsA0YH1Z2FxbjDheW8QT5qTjaFi52Ie83HasV9BZFXnLrYF4rQbb1wrpa+Q6r4RXKG0wG+sb5nrUtth7yiHtgeZNDwrcB2XGwqXjudQxW5fpAYBlWJwA2zgAABDhJREFUgeFw4pzfTIbJW9SXgRg1sNDIhcD5aj2eJgWBfZdhD8xLVHVNom0KoaqvYj38tozndcU6h7wKqwaeLyIvBfb8AXs5eEtEQjmUjtgQrvGy7QxgloiMD2xaF7Sr2I3lJTKC/S4GHhOR+hrkMxJg26dYzaUbVfUfaizHhDmetQrPAL4OfY8BaYGNX2Df399CG9TaqGyLk3nhuddbMYFZiYXpDsUqzowmvvnNhON9f0WAiFwKzFDV+Ym2JZygSucJ2PCocWsIVx5BFx0J+2GJDVX8M1Yr6B/ALlUdHmw7G2t71BXLYcyLk005wKtYyO14zIO7KNiWjj0oXwQ2EfScq6q5CbKthqpeEGyrpUWJ+qFY9dxzNA7dwpRzz7JUdYeINMa8hIdUdVqsbQqzLR3IUdWfg+XmmKd5mlq14UOwnjhyVHVTvOxKBlxUIiDRD0mn8gS15MYAO1V1uIgcBWyJx0OxBFuKi11+6CEZbH8DqxJ+tqp+k2DbdqjqhWHbR2Ae4Mh4CXEpdhW/Z9nYsAkPq43QGXdEJCOwb5Kq9hcb+KsPcG28PM1kwkXFqfYEb7P3Y2+76UA/VY1LNyJl2BQSu+2qelHQXcdI4F/x8lAqYFs7rF+099SGfU4Wu7phtSDXahwaXJZHUOV6FTb41qWqOjexFiUGFxVnv0BsCOObsA4Fk+LPHiZ2vYKiPsmSFysmxAKcoNbTdEIJs6snloNKhhcEwRr6Lgjm/dVa+++XeKLeqfaISAOsOurAZBEUAFX9EauSWhfr2iYpBAX2sq0eZlvCBQX2sqs+FiZMqKCANX5Ua6x6F3Dq/iwo4J6Ks58gNgZIfqLtCCcQuwlYz81JU6sQkte2ZLULPPcawkXFcRJIMopdiGS1LVntcgwXFcdxHCdqeE7FcRzHiRouKo7jOE7UcFFxHMdxooaLirPfISLXBi2xHceJMp6od/Y7gqGEuwVtHhzHiSLuqTjVGhHJEZG3RWS2iMwTkT9h3ZN/LCIfB/sMFJHpIvKViLwiIrWD8qUicp+IzBWRL8TGsHccpwxcVJzqziBgpap2UtUOwCNYl/wnquqJQbcftwEnq2oXbOyL68OO36SqR2Pdqz8SZ9sdJ+VwUXGqO3OBASJyr4j0KaEb8uOwkTQ/FZFZ2Hj2h4Rtfyls3jPm1jpOipORaAMcJ5ao6iIR6YL1/XW3iEwutosAH4bGWynpFKUsO45TAu6pONWaYDyObar6L6x32y7AZqBOsMtnQK9QviTIwRwRdorzwubT42O146Qu7qk41Z2jgftFpBDYBfwaC2O9JyIrg7zKpcBLIpIVHHMbsChYbiAic4AdQGnejOM4AV6l2HFKwaseO07F8fCX4ziOEzXcU3Ecx3GihnsqjuM4TtRwUXEcx3GihouK4ziOEzVcVBzHcZyo4aLiOI7jRA0XFcdxHCdq/H/eAvilv8/9bAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IaoqttaHiDaz",
        "outputId": "1dbcd8fb-311a-4620-a7e1-aab9b42e3e90"
      },
      "source": [
        "#  scale=False 36min\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.layers as layers\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "#随机旋转，裁切等数据增强\n",
        "def data_enhace(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_hue(x, max_delta=0.2)\n",
        "    x = tf.image.random_brightness(x, max_delta=0.7)\n",
        "    result = tf.image.random_contrast(x, lower=0.2, upper=1.8)\n",
        "    return result\n",
        "\n",
        "#mixup数据增强\n",
        "def criterion(batch_x, batch_y, batch_size,alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    index = tf.random_shuffle(batch_size)\n",
        "    mixed_batchx = lam * batch_x + (1 - lam) * batch_x[index, :]\n",
        "\n",
        "    batch_y_= lam*batch_y+(1-lam)*batch_y[index]\n",
        "    return mixed_batchx,batch_y_\n",
        "\n",
        "#本地载入cifar10数据集\n",
        "def load_cifar10_batch(cifar10_dataset_folder_path,batch_id):\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id),mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding = 'latin1')\n",
        "\n",
        "    # features and labels\n",
        "    features = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return  features, labels\n",
        "\n",
        "#在线载入cifar10数据集\n",
        "def load_cifar10_batch_online():\n",
        "    (x_Train, y_Train), (x_Test, y_Test)=cifar10.load_data()\n",
        "    return x_Train,y_Train,x_Test,y_Test\n",
        "\n",
        "#cifar10数据集读取器\n",
        "class Cifar10DataReader():\n",
        "    def __init__(self, cifar_folder=None, onehot=True):\n",
        "        self.cifar_folder = cifar_folder\n",
        "        self.onehot = onehot\n",
        "        self.data_label_train = None\n",
        "        self.data_label_test = None\n",
        "        self.batch_index = 0\n",
        "\n",
        "        if cifar_folder==None:#在线\n",
        "            x_train, y_train,x_test,y_test=load_cifar10_batch_online()\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            self.data_label_test = list(zip(x_test, y_test))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "        else:#本地\n",
        "            x_train, y_train = load_cifar10_batch(self.cifar_folder, 1)\n",
        "            for i in range(2,6):\n",
        "                features,labels = load_cifar10_batch(self.cifar_folder, i)\n",
        "                x_train, y_train = np.concatenate([x_train, features]),np.concatenate([y_train, labels])\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "    #批次读取训练集数据\n",
        "    def next_train_data(self, batch_size=100):\n",
        "        if self.batch_index < len(self.data_label_train) // batch_size:\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有训练集数据\n",
        "    def all_train_data(self):\n",
        "        np.random.shuffle(self.data_label_train)\n",
        "        datum = self.data_label_train\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #独热编码\n",
        "    def _decode(self, datum, onehot):\n",
        "        rdata = list()\n",
        "        rlabel = list()\n",
        "        if onehot:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                hot = np.zeros(10)\n",
        "                hot[int(l)] = 1\n",
        "                rlabel.append(hot)\n",
        "        else:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                rlabel.append(int(l))\n",
        "        return rdata, rlabel\n",
        "\n",
        "    #批次读取测试集数据\n",
        "    def next_test_data(self, batch_size=100):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test[0:batch_size]\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有测试集数据\n",
        "    def all_test_data(self):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "\n",
        "#网络结构\n",
        "def weight_variable(shape):\n",
        "    weights = tf.get_variable(\"weights\", shape, initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    #加入l2正则化\n",
        "    regular=tf.multiply(tf.nn.l2_loss(weights),0.005,name='regular')\n",
        "    tf.add_to_collection('losses',regular)\n",
        "    return weights\n",
        "def bias_variable(shape):\n",
        "    biases = tf.get_variable(\"biases\", shape, initializer=tf.constant_initializer(0.0))\n",
        "    return biases\n",
        "def conv_layer(x, w, b, name, strides, padding = 'SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = weight_variable(w)\n",
        "        b = bias_variable(b)\n",
        "        conv_and_biased = tf.nn.conv2d(x, w, strides = strides, padding = padding, name = name) + b\n",
        "    return conv_and_biased\n",
        "def batch_normalization(inputs, scope, is_training=True, need_relu=True):\n",
        "    bn = tf.contrib.layers.batch_norm(inputs,\n",
        "        decay=0.999,\n",
        "        center=True,\n",
        "        scale=False,  # 可以让学生实验True和False的区别。\n",
        "        epsilon=0.001,\n",
        "        activation_fn=None,\n",
        "        param_initializers=None,\n",
        "        param_regularizers=None,\n",
        "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
        "        is_training=is_training,  # 可以让学生实验True和False的区别。\n",
        "        reuse=None,\n",
        "        variables_collections=None,\n",
        "        outputs_collections=None,\n",
        "        trainable=True,\n",
        "        batch_weights=None,\n",
        "        fused=False,\n",
        "        data_format='NHWC',\n",
        "        zero_debias_moving_mean=False,\n",
        "        scope=scope,\n",
        "        renorm=False,\n",
        "        renorm_clipping=None,\n",
        "        renorm_decay=0.99)\n",
        "    if need_relu:\n",
        "        bn = tf.nn.relu(bn, name='relu')\n",
        "    return bn\n",
        "def maxpooling(x,kernal_size, strides, name):  # 最大池化，前面的是核大小，一般为[1, 2, 2, 1]，后面的strides指的是步长，如[1, 2, 2, 1]。\n",
        "    return tf.nn.max_pool(x, ksize=kernal_size, strides=strides, padding='SAME',name = name)\n",
        "def avg_pool(input_feats, k):\n",
        "    ksize = [1, k, k, 1]\n",
        "    strides = [1, k, k, 1]\n",
        "    padding = 'VALID'\n",
        "    output = tf.nn.avg_pool(input_feats, ksize, strides, padding)  # 平均池化\n",
        "    return output\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, train_flag,stride2=False):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters  # nb_filter1/2/3分别是64/64/256，三个整数。\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'  # 'res2a_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'  # 'bn2a_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    if stride2==False:\n",
        "        stride_for_first_conv = [1, 1, 1, 1]  # 。\n",
        "    else:\n",
        "        stride_for_first_conv = [1, 2, 2, 1]\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    shortcut = conv_layer(input_tensor, [1, 1, input_layers, nb_filter3], [nb_filter3], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '1')\n",
        "    shortcut = batch_normalization(shortcut, scope=bn_name_base + '1', is_training=train_flag)\n",
        "    x = tf.add(x, shortcut)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block, train_flag):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x =  batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    x = tf.add(x, input_tensor)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def resnet_graph(input_image, architecture,train_flag=True, stage5=False,ttf=True):  # 残差网络函数\n",
        "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
        "    # Stage 1\n",
        "    paddings = tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])  # 上下左右各补3个0。\n",
        "    x = tf.pad(input_image, paddings, \"CONSTANT\")\n",
        "    w = weight_variable([7, 7, 3, 64])\n",
        "    b = bias_variable([64])  #\n",
        "    x = tf.nn.conv2d(x, w, strides = [1, 2, 2, 1], padding = 'VALID', name = 'conv_1') + b\n",
        "    x = batch_normalization(x, scope='bn_conv1', is_training=train_flag, need_relu=True)\n",
        "    C1 = x = maxpooling(x, [1, 3, 3, 1], [1, 2, 2, 1], name='stage1')\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', train_flag=train_flag)  # 结构块。\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_flag=train_flag)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_flag=train_flag, stride2=True)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_flag=train_flag)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_flag=train_flag)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_flag=train_flag, stride2=True)\n",
        "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  # block_count=22，即，下句for循环的range是0~22，正好是加23个层。\n",
        "    for i in range(block_count):  # 加22个层，都是用identity_block函数加。\n",
        "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_flag=train_flag)\n",
        "    C4 = x  #\n",
        "    # Stage 5\n",
        "    if stage5:\n",
        "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_flag=train_flag, stride2=True)\n",
        "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_flag=train_flag)\n",
        "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_flag=train_flag)\n",
        "        output=avg_pool(C5,1)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)#加入dropout\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "    else:\n",
        "        C5 = None\n",
        "        output=avg_pool(C4,2)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "\n",
        "    return [C1, C2, C3, C4, C5,output]\n",
        "\n",
        "\n",
        "\n",
        "filepath='C:/Users/zy109/Desktop/1/data/cifar-10-batches-py'\n",
        "model_save_path='C:/Users/zy109/Desktop/1/cifar10'\n",
        "data=Cifar10DataReader()\n",
        "sess=tf.InteractiveSession()\n",
        "\n",
        "#指数衰减学习率\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(learning_rate=0.001, global_step=global_step, decay_steps=50, decay_rate=0.99, staircase=False)\n",
        "\n",
        "#占位符\n",
        "input_image=tf.placeholder(tf.float32,[None,32,32,3])\n",
        "y_=tf.placeholder(tf.float32,[None,10])\n",
        "tf_is_train=tf.placeholder(tf.bool,None)\n",
        "input_image=data_enhace(input_image)#数据增强\n",
        "input_image,y_=criterion(input_image,y_,batch_size)#mixup\n",
        "\n",
        "[C1,C2,C3,C4,C5,output]=resnet_graph(input_image,'resnet50',ttf=tf_is_train)\n",
        "\n",
        "#交叉熵损失\n",
        "cross_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_,logits=output))\n",
        "\n",
        "tf.add_to_collection('losses',cross_loss)\n",
        "loss=tf.add_n(tf.get_collection('losses'))\n",
        "train_step=tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "\n",
        "#正确率\n",
        "correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y_,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "sess.run(tf.initialize_all_variables())\n",
        "saver = tf.train.Saver()\n",
        "#读取已经训练好的权重\n",
        "ckpt = tf.train.get_checkpoint_state(model_save_path)\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "#加载测试集数据\n",
        "test_data=data.all_test_data()\n",
        "x_test=np.array(test_data[0])/255\n",
        "y_test=np.array(test_data[1])\n",
        "\n",
        "total_loss=[]\n",
        "total_acc=[]\n",
        "total_step=[]\n",
        "\n",
        "\n",
        "# 训练\n",
        "for i in range(20000):\n",
        "\n",
        "    #逐批加载训练集数据\n",
        "    train_data=data.next_train_data(batch_size)\n",
        "    x_train=np.array(train_data[0])/255\n",
        "    y_train=np.array(train_data[1])\n",
        "\n",
        "    train_step.run(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        # 测试准确率\n",
        "        train_accuracy = accuracy.eval(feed_dict={input_image: x_train,y_: y_train,tf_is_train:False})\n",
        "        test_accuracy = accuracy.eval(feed_dict={input_image: x_test,y_: y_test,tf_is_train:False})\n",
        "        # 该次训练的损失\n",
        "        loss_value = loss.eval(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "        global_var=sess.run(global_step)\n",
        "        lr_val=sess.run(lr)\n",
        "        print(\"step %d, trainning accuracy， %g , testing accuracy， %g , loss %g  ,lr %g\" % (global_var, train_accuracy,test_accuracy, loss_value,lr_val))\n",
        "        total_step.append(i)\n",
        "        total_loss.append(loss_value)\n",
        "        total_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "save_path = saver.save(sess,\"./cifar10/model.ckpt\")\n",
        "print(\"save model:{0} Finished\".format(save_path))\n",
        "np.save(\"cifar10_step\",total_step)\n",
        "np.save(\"cifar10_loss\",total_loss)\n",
        "np.save(\"cifar10_acc\",total_acc)\n",
        "\n",
        "#测试\n",
        "y_log_predict=output.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "test_accuracy = accuracy.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "print(\"test accuracy %g\" % test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test.argmax(axis=1), y_log_predict.argmax(axis=1))\n",
        "\n",
        "\n",
        "for i in range(len(y_log_predict)):\n",
        "        max_value=max(y_log_predict[i])\n",
        "        for j in range(len(y_log_predict[i])):\n",
        "            if max_value==y_log_predict[i][j]:\n",
        "                y_log_predict[i][j]=1\n",
        "            else:\n",
        "                y_log_predict[i][j]=0\n",
        "\n",
        "# 精确率\n",
        "from sklearn.metrics import precision_score\n",
        "ps=precision_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_precision_score\",ps)\n",
        "\n",
        "#召回率\n",
        "from sklearn.metrics import recall_score\n",
        "rs=recall_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_recall_score\",rs)\n",
        "\n",
        "print(\"precision score:\",ps)\n",
        "print(\"recall score:\",rs)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "ax1.plot(total_step, total_loss, color=\"blue\", label=\"loss\")\n",
        "ax1.set_xlabel(\"step\")\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_step, total_acc, color=\"red\", label=\"accuary\")\n",
        "ax2.set_ylabel(\"accuary\")\n",
        "\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:650: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-e2beb5a4e8b0>:249: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-e2beb5a4e8b0>:280: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "step 1, trainning accuracy， 0.15625 , testing accuracy， 0.117 , loss 80.8099  ,lr 0.000999799\n",
            "step 101, trainning accuracy， 0.289062 , testing accuracy， 0.3015 , loss 51.1716  ,lr 0.000979903\n",
            "step 201, trainning accuracy， 0.449219 , testing accuracy， 0.3854 , loss 30.1415  ,lr 0.000960403\n",
            "step 301, trainning accuracy， 0.441406 , testing accuracy， 0.4279 , loss 19.1061  ,lr 0.000941291\n",
            "step 401, trainning accuracy， 0.511719 , testing accuracy， 0.467 , loss 13.511  ,lr 0.000922559\n",
            "step 501, trainning accuracy， 0.433594 , testing accuracy， 0.4217 , loss 10.7712  ,lr 0.0009042\n",
            "step 601, trainning accuracy， 0.480469 , testing accuracy， 0.4752 , loss 8.98087  ,lr 0.000886207\n",
            "step 701, trainning accuracy， 0.523438 , testing accuracy， 0.5172 , loss 7.53429  ,lr 0.000868571\n",
            "step 801, trainning accuracy， 0.535156 , testing accuracy， 0.5471 , loss 6.027  ,lr 0.000851287\n",
            "step 901, trainning accuracy， 0.585938 , testing accuracy， 0.5605 , loss 5.2431  ,lr 0.000834346\n",
            "step 1001, trainning accuracy， 0.640625 , testing accuracy， 0.5761 , loss 4.59372  ,lr 0.000817743\n",
            "step 1101, trainning accuracy， 0.585938 , testing accuracy， 0.5853 , loss 4.25538  ,lr 0.00080147\n",
            "step 1201, trainning accuracy， 0.5625 , testing accuracy， 0.6035 , loss 3.95346  ,lr 0.00078552\n",
            "step 1301, trainning accuracy， 0.625 , testing accuracy， 0.6091 , loss 3.47103  ,lr 0.000769889\n",
            "step 1401, trainning accuracy， 0.707031 , testing accuracy， 0.6293 , loss 2.97928  ,lr 0.000754568\n",
            "step 1501, trainning accuracy， 0.695312 , testing accuracy， 0.6283 , loss 2.95297  ,lr 0.000739552\n",
            "step 1601, trainning accuracy， 0.738281 , testing accuracy， 0.6302 , loss 2.75757  ,lr 0.000724835\n",
            "step 1701, trainning accuracy， 0.703125 , testing accuracy， 0.6231 , loss 2.59768  ,lr 0.000710411\n",
            "step 1801, trainning accuracy， 0.671875 , testing accuracy， 0.6517 , loss 2.69872  ,lr 0.000696274\n",
            "step 1901, trainning accuracy， 0.6875 , testing accuracy， 0.6647 , loss 2.33299  ,lr 0.000682418\n",
            "step 2001, trainning accuracy， 0.667969 , testing accuracy， 0.6208 , loss 2.63423  ,lr 0.000668838\n",
            "step 2101, trainning accuracy， 0.675781 , testing accuracy， 0.6473 , loss 2.38589  ,lr 0.000655528\n",
            "step 2201, trainning accuracy， 0.660156 , testing accuracy， 0.6081 , loss 2.37045  ,lr 0.000642483\n",
            "step 2301, trainning accuracy， 0.675781 , testing accuracy， 0.655 , loss 2.24023  ,lr 0.000629697\n",
            "step 2401, trainning accuracy， 0.746094 , testing accuracy， 0.6812 , loss 2.04576  ,lr 0.000617166\n",
            "step 2501, trainning accuracy， 0.652344 , testing accuracy， 0.6333 , loss 2.31759  ,lr 0.000604885\n",
            "step 2601, trainning accuracy， 0.753906 , testing accuracy， 0.7087 , loss 1.88332  ,lr 0.000592848\n",
            "step 2701, trainning accuracy， 0.761719 , testing accuracy， 0.7013 , loss 1.73595  ,lr 0.00058105\n",
            "step 2801, trainning accuracy， 0.761719 , testing accuracy， 0.6992 , loss 1.77786  ,lr 0.000569487\n",
            "step 2901, trainning accuracy， 0.726562 , testing accuracy， 0.7175 , loss 1.71638  ,lr 0.000558154\n",
            "step 3001, trainning accuracy， 0.777344 , testing accuracy， 0.7251 , loss 1.5909  ,lr 0.000547047\n",
            "step 3101, trainning accuracy， 0.746094 , testing accuracy， 0.7281 , loss 1.54754  ,lr 0.000536161\n",
            "step 3201, trainning accuracy， 0.773438 , testing accuracy， 0.728 , loss 1.53162  ,lr 0.000525491\n",
            "step 3301, trainning accuracy， 0.710938 , testing accuracy， 0.6544 , loss 1.87922  ,lr 0.000515034\n",
            "step 3401, trainning accuracy， 0.640625 , testing accuracy， 0.647 , loss 2.25427  ,lr 0.000504785\n",
            "step 3501, trainning accuracy， 0.769531 , testing accuracy， 0.7093 , loss 1.71237  ,lr 0.00049474\n",
            "step 3601, trainning accuracy， 0.785156 , testing accuracy， 0.7348 , loss 1.56028  ,lr 0.000484894\n",
            "step 3701, trainning accuracy， 0.773438 , testing accuracy， 0.737 , loss 1.49934  ,lr 0.000475245\n",
            "step 3801, trainning accuracy， 0.84375 , testing accuracy， 0.7446 , loss 1.24429  ,lr 0.000465788\n",
            "step 3901, trainning accuracy， 0.839844 , testing accuracy， 0.7485 , loss 1.30613  ,lr 0.000456518\n",
            "step 4001, trainning accuracy， 0.851562 , testing accuracy， 0.7457 , loss 1.20699  ,lr 0.000447434\n",
            "step 4101, trainning accuracy， 0.863281 , testing accuracy， 0.7521 , loss 1.18686  ,lr 0.00043853\n",
            "step 4201, trainning accuracy， 0.84375 , testing accuracy， 0.7537 , loss 1.16078  ,lr 0.000429803\n",
            "step 4301, trainning accuracy， 0.875 , testing accuracy， 0.7652 , loss 1.07378  ,lr 0.00042125\n",
            "step 4401, trainning accuracy， 0.855469 , testing accuracy， 0.7579 , loss 1.07153  ,lr 0.000412867\n",
            "step 4501, trainning accuracy， 0.894531 , testing accuracy， 0.7588 , loss 0.999675  ,lr 0.000404651\n",
            "step 4601, trainning accuracy， 0.738281 , testing accuracy， 0.6738 , loss 1.57743  ,lr 0.000396598\n",
            "step 4701, trainning accuracy， 0.855469 , testing accuracy， 0.7495 , loss 1.29019  ,lr 0.000388706\n",
            "step 4801, trainning accuracy， 0.847656 , testing accuracy， 0.7546 , loss 1.20027  ,lr 0.000380971\n",
            "step 4901, trainning accuracy， 0.894531 , testing accuracy， 0.7664 , loss 1.02755  ,lr 0.00037339\n",
            "step 5001, trainning accuracy， 0.875 , testing accuracy， 0.7637 , loss 0.989856  ,lr 0.000365959\n",
            "step 5101, trainning accuracy， 0.9375 , testing accuracy， 0.7677 , loss 0.850372  ,lr 0.000358677\n",
            "step 5201, trainning accuracy， 0.875 , testing accuracy， 0.772 , loss 1.029  ,lr 0.000351539\n",
            "step 5301, trainning accuracy， 0.902344 , testing accuracy， 0.7628 , loss 0.975694  ,lr 0.000344543\n",
            "step 5401, trainning accuracy， 0.917969 , testing accuracy， 0.7713 , loss 0.843959  ,lr 0.000337687\n",
            "step 5501, trainning accuracy， 0.90625 , testing accuracy， 0.769 , loss 0.829712  ,lr 0.000330967\n",
            "step 5601, trainning accuracy， 0.871094 , testing accuracy， 0.7729 , loss 0.959608  ,lr 0.000324381\n",
            "step 5701, trainning accuracy， 0.898438 , testing accuracy， 0.7649 , loss 0.860947  ,lr 0.000317926\n",
            "step 5801, trainning accuracy， 0.890625 , testing accuracy， 0.7724 , loss 0.835585  ,lr 0.000311599\n",
            "step 5901, trainning accuracy， 0.949219 , testing accuracy， 0.771 , loss 0.80793  ,lr 0.000305398\n",
            "step 6001, trainning accuracy， 0.929688 , testing accuracy， 0.7751 , loss 0.791241  ,lr 0.000299321\n",
            "step 6101, trainning accuracy， 0.9375 , testing accuracy， 0.7707 , loss 0.738876  ,lr 0.000293364\n",
            "step 6201, trainning accuracy， 0.914062 , testing accuracy， 0.7739 , loss 0.811062  ,lr 0.000287526\n",
            "step 6301, trainning accuracy， 0.820312 , testing accuracy， 0.726 , loss 1.2886  ,lr 0.000281804\n",
            "step 6401, trainning accuracy， 0.910156 , testing accuracy， 0.7774 , loss 0.937665  ,lr 0.000276196\n",
            "step 6501, trainning accuracy， 0.925781 , testing accuracy， 0.7748 , loss 0.884556  ,lr 0.0002707\n",
            "step 6601, trainning accuracy， 0.894531 , testing accuracy， 0.7729 , loss 0.810496  ,lr 0.000265313\n",
            "step 6701, trainning accuracy， 0.945312 , testing accuracy， 0.7727 , loss 0.747405  ,lr 0.000260034\n",
            "step 6801, trainning accuracy， 0.917969 , testing accuracy， 0.7745 , loss 0.76852  ,lr 0.000254859\n",
            "step 6901, trainning accuracy， 0.945312 , testing accuracy， 0.7714 , loss 0.711157  ,lr 0.000249787\n",
            "step 7001, trainning accuracy， 0.964844 , testing accuracy， 0.7738 , loss 0.637612  ,lr 0.000244816\n",
            "step 7101, trainning accuracy， 0.972656 , testing accuracy， 0.7671 , loss 0.655091  ,lr 0.000239945\n",
            "step 7201, trainning accuracy， 0.949219 , testing accuracy， 0.7811 , loss 0.640648  ,lr 0.00023517\n",
            "step 7301, trainning accuracy， 0.960938 , testing accuracy， 0.776 , loss 0.683425  ,lr 0.00023049\n",
            "step 7401, trainning accuracy， 0.972656 , testing accuracy， 0.7774 , loss 0.626336  ,lr 0.000225903\n",
            "step 7501, trainning accuracy， 0.972656 , testing accuracy， 0.7825 , loss 0.602436  ,lr 0.000221408\n",
            "step 7601, trainning accuracy， 0.953125 , testing accuracy， 0.7789 , loss 0.687596  ,lr 0.000217002\n",
            "step 7701, trainning accuracy， 0.992188 , testing accuracy， 0.7763 , loss 0.572301  ,lr 0.000212683\n",
            "step 7801, trainning accuracy， 0.996094 , testing accuracy， 0.7756 , loss 0.517678  ,lr 0.000208451\n",
            "step 7901, trainning accuracy， 0.984375 , testing accuracy， 0.7824 , loss 0.524227  ,lr 0.000204303\n",
            "step 8001, trainning accuracy， 0.988281 , testing accuracy， 0.7768 , loss 0.524079  ,lr 0.000200237\n",
            "step 8101, trainning accuracy， 0.964844 , testing accuracy， 0.7759 , loss 0.597536  ,lr 0.000196252\n",
            "step 8201, trainning accuracy， 0.984375 , testing accuracy， 0.7789 , loss 0.519617  ,lr 0.000192347\n",
            "step 8301, trainning accuracy， 0.992188 , testing accuracy， 0.7799 , loss 0.484458  ,lr 0.000188519\n",
            "step 8401, trainning accuracy， 0.996094 , testing accuracy， 0.7782 , loss 0.483189  ,lr 0.000184768\n",
            "step 8501, trainning accuracy， 0.957031 , testing accuracy， 0.7813 , loss 0.609272  ,lr 0.000181091\n",
            "step 8601, trainning accuracy， 0.984375 , testing accuracy， 0.7838 , loss 0.485869  ,lr 0.000177487\n",
            "step 8701, trainning accuracy， 0.992188 , testing accuracy， 0.7758 , loss 0.475645  ,lr 0.000173955\n",
            "step 8801, trainning accuracy， 1 , testing accuracy， 0.7764 , loss 0.5115  ,lr 0.000170493\n",
            "step 8901, trainning accuracy， 0.984375 , testing accuracy， 0.7729 , loss 0.515161  ,lr 0.000167101\n",
            "step 9001, trainning accuracy， 0.996094 , testing accuracy， 0.7794 , loss 0.477512  ,lr 0.000163775\n",
            "step 9101, trainning accuracy， 0.992188 , testing accuracy， 0.7836 , loss 0.513626  ,lr 0.000160516\n",
            "step 9201, trainning accuracy， 0.996094 , testing accuracy， 0.7852 , loss 0.468752  ,lr 0.000157322\n",
            "step 9301, trainning accuracy， 0.988281 , testing accuracy， 0.7742 , loss 0.491215  ,lr 0.000154191\n",
            "step 9401, trainning accuracy， 0.992188 , testing accuracy， 0.7853 , loss 0.455673  ,lr 0.000151123\n",
            "step 9501, trainning accuracy， 0.992188 , testing accuracy， 0.7831 , loss 0.476855  ,lr 0.000148115\n",
            "step 9601, trainning accuracy， 0.988281 , testing accuracy， 0.7819 , loss 0.500159  ,lr 0.000145168\n",
            "step 9701, trainning accuracy， 1 , testing accuracy， 0.7835 , loss 0.416808  ,lr 0.000142279\n",
            "step 9801, trainning accuracy， 0.992188 , testing accuracy， 0.7831 , loss 0.483246  ,lr 0.000139448\n",
            "step 9901, trainning accuracy， 0.988281 , testing accuracy， 0.7838 , loss 0.485575  ,lr 0.000136673\n",
            "step 10001, trainning accuracy， 1 , testing accuracy， 0.7819 , loss 0.417516  ,lr 0.000133953\n",
            "step 10101, trainning accuracy， 0.992188 , testing accuracy， 0.7734 , loss 0.457958  ,lr 0.000131287\n",
            "step 10201, trainning accuracy， 1 , testing accuracy， 0.7781 , loss 0.414698  ,lr 0.000128675\n",
            "step 10301, trainning accuracy， 0.992188 , testing accuracy， 0.7824 , loss 0.425325  ,lr 0.000126114\n",
            "step 10401, trainning accuracy， 0.992188 , testing accuracy， 0.7815 , loss 0.434577  ,lr 0.000123604\n",
            "step 10501, trainning accuracy， 0.996094 , testing accuracy， 0.7831 , loss 0.428821  ,lr 0.000121145\n",
            "step 10601, trainning accuracy， 1 , testing accuracy， 0.7808 , loss 0.394336  ,lr 0.000118734\n",
            "step 10701, trainning accuracy， 0.988281 , testing accuracy， 0.7799 , loss 0.43837  ,lr 0.000116371\n",
            "step 10801, trainning accuracy， 0.996094 , testing accuracy， 0.7773 , loss 0.38276  ,lr 0.000114055\n",
            "step 10901, trainning accuracy， 0.996094 , testing accuracy， 0.7809 , loss 0.402299  ,lr 0.000111786\n",
            "step 11001, trainning accuracy， 1 , testing accuracy， 0.785 , loss 0.383962  ,lr 0.000109561\n",
            "step 11101, trainning accuracy， 0.996094 , testing accuracy， 0.7814 , loss 0.377112  ,lr 0.000107381\n",
            "step 11201, trainning accuracy， 1 , testing accuracy， 0.7803 , loss 0.401029  ,lr 0.000105244\n",
            "step 11301, trainning accuracy， 0.992188 , testing accuracy， 0.7792 , loss 0.398779  ,lr 0.00010315\n",
            "step 11401, trainning accuracy， 1 , testing accuracy， 0.7833 , loss 0.375929  ,lr 0.000101097\n",
            "step 11501, trainning accuracy， 1 , testing accuracy， 0.7824 , loss 0.403044  ,lr 9.90851e-05\n",
            "step 11601, trainning accuracy， 1 , testing accuracy， 0.7838 , loss 0.374686  ,lr 9.71133e-05\n",
            "step 11701, trainning accuracy， 1 , testing accuracy， 0.7814 , loss 0.377857  ,lr 9.51808e-05\n",
            "step 11801, trainning accuracy， 0.996094 , testing accuracy， 0.7854 , loss 0.371988  ,lr 9.32867e-05\n",
            "step 11901, trainning accuracy， 1 , testing accuracy， 0.7768 , loss 0.355153  ,lr 9.14303e-05\n",
            "step 12001, trainning accuracy， 1 , testing accuracy， 0.7842 , loss 0.347563  ,lr 8.96108e-05\n",
            "step 12101, trainning accuracy， 1 , testing accuracy， 0.785 , loss 0.355343  ,lr 8.78276e-05\n",
            "step 12201, trainning accuracy， 1 , testing accuracy， 0.787 , loss 0.337767  ,lr 8.60798e-05\n",
            "step 12301, trainning accuracy， 0.996094 , testing accuracy， 0.7868 , loss 0.357489  ,lr 8.43668e-05\n",
            "step 12401, trainning accuracy， 1 , testing accuracy， 0.7823 , loss 0.334074  ,lr 8.26879e-05\n",
            "step 12501, trainning accuracy， 1 , testing accuracy， 0.7824 , loss 0.337843  ,lr 8.10424e-05\n",
            "step 12601, trainning accuracy， 0.992188 , testing accuracy， 0.7853 , loss 0.365789  ,lr 7.94297e-05\n",
            "step 12701, trainning accuracy， 1 , testing accuracy， 0.7837 , loss 0.346731  ,lr 7.7849e-05\n",
            "step 12801, trainning accuracy， 1 , testing accuracy， 0.7812 , loss 0.317549  ,lr 7.62998e-05\n",
            "step 12901, trainning accuracy， 1 , testing accuracy， 0.7852 , loss 0.332904  ,lr 7.47815e-05\n",
            "step 13001, trainning accuracy， 0.996094 , testing accuracy， 0.7867 , loss 0.336912  ,lr 7.32933e-05\n",
            "step 13101, trainning accuracy， 1 , testing accuracy， 0.7794 , loss 0.337882  ,lr 7.18348e-05\n",
            "step 13201, trainning accuracy， 1 , testing accuracy， 0.7823 , loss 0.342646  ,lr 7.04053e-05\n",
            "step 13301, trainning accuracy， 0.996094 , testing accuracy， 0.7795 , loss 0.333717  ,lr 6.90042e-05\n",
            "step 13401, trainning accuracy， 1 , testing accuracy， 0.7828 , loss 0.319499  ,lr 6.7631e-05\n",
            "step 13501, trainning accuracy， 1 , testing accuracy， 0.781 , loss 0.339961  ,lr 6.62852e-05\n",
            "step 13601, trainning accuracy， 1 , testing accuracy， 0.7838 , loss 0.318281  ,lr 6.49661e-05\n",
            "step 13701, trainning accuracy， 0.996094 , testing accuracy， 0.7892 , loss 0.340948  ,lr 6.36733e-05\n",
            "step 13801, trainning accuracy， 1 , testing accuracy， 0.7813 , loss 0.314454  ,lr 6.24062e-05\n",
            "step 13901, trainning accuracy， 1 , testing accuracy， 0.7844 , loss 0.30431  ,lr 6.11643e-05\n",
            "step 14001, trainning accuracy， 1 , testing accuracy， 0.7774 , loss 0.312959  ,lr 5.99471e-05\n",
            "step 14101, trainning accuracy， 1 , testing accuracy， 0.7871 , loss 0.334823  ,lr 5.87542e-05\n",
            "step 14201, trainning accuracy， 1 , testing accuracy， 0.7861 , loss 0.315791  ,lr 5.7585e-05\n",
            "step 14301, trainning accuracy， 1 , testing accuracy， 0.7855 , loss 0.322021  ,lr 5.6439e-05\n",
            "step 14401, trainning accuracy， 1 , testing accuracy， 0.7839 , loss 0.313303  ,lr 5.53159e-05\n",
            "step 14501, trainning accuracy， 1 , testing accuracy， 0.7823 , loss 0.321383  ,lr 5.42151e-05\n",
            "step 14601, trainning accuracy， 1 , testing accuracy， 0.7806 , loss 0.302494  ,lr 5.31362e-05\n",
            "step 14701, trainning accuracy， 1 , testing accuracy， 0.7879 , loss 0.302542  ,lr 5.20788e-05\n",
            "step 14801, trainning accuracy， 1 , testing accuracy， 0.7858 , loss 0.289124  ,lr 5.10425e-05\n",
            "step 14901, trainning accuracy， 1 , testing accuracy， 0.7868 , loss 0.307055  ,lr 5.00267e-05\n",
            "step 15001, trainning accuracy， 1 , testing accuracy， 0.7893 , loss 0.314404  ,lr 4.90312e-05\n",
            "step 15101, trainning accuracy， 1 , testing accuracy， 0.7855 , loss 0.291408  ,lr 4.80555e-05\n",
            "step 15201, trainning accuracy， 1 , testing accuracy， 0.7832 , loss 0.282615  ,lr 4.70992e-05\n",
            "step 15301, trainning accuracy， 1 , testing accuracy， 0.7871 , loss 0.284009  ,lr 4.61619e-05\n",
            "step 15401, trainning accuracy， 1 , testing accuracy， 0.7824 , loss 0.306574  ,lr 4.52433e-05\n",
            "step 15501, trainning accuracy， 1 , testing accuracy， 0.7836 , loss 0.278269  ,lr 4.43429e-05\n",
            "step 15601, trainning accuracy， 1 , testing accuracy， 0.788 , loss 0.284554  ,lr 4.34605e-05\n",
            "step 15701, trainning accuracy， 1 , testing accuracy， 0.7873 , loss 0.277904  ,lr 4.25956e-05\n",
            "step 15801, trainning accuracy， 1 , testing accuracy， 0.7882 , loss 0.283565  ,lr 4.1748e-05\n",
            "step 15901, trainning accuracy， 1 , testing accuracy， 0.7867 , loss 0.276754  ,lr 4.09172e-05\n",
            "step 16001, trainning accuracy， 1 , testing accuracy， 0.7887 , loss 0.277491  ,lr 4.0103e-05\n",
            "step 16101, trainning accuracy， 1 , testing accuracy， 0.7841 , loss 0.291551  ,lr 3.93049e-05\n",
            "step 16201, trainning accuracy， 1 , testing accuracy， 0.7899 , loss 0.269642  ,lr 3.85227e-05\n",
            "step 16301, trainning accuracy， 1 , testing accuracy， 0.7835 , loss 0.288609  ,lr 3.77561e-05\n",
            "step 16401, trainning accuracy， 1 , testing accuracy， 0.7883 , loss 0.267155  ,lr 3.70048e-05\n",
            "step 16501, trainning accuracy， 1 , testing accuracy， 0.7888 , loss 0.267323  ,lr 3.62684e-05\n",
            "step 16601, trainning accuracy， 1 , testing accuracy， 0.7891 , loss 0.27779  ,lr 3.55467e-05\n",
            "step 16701, trainning accuracy， 1 , testing accuracy， 0.787 , loss 0.266768  ,lr 3.48393e-05\n",
            "step 16801, trainning accuracy， 1 , testing accuracy， 0.7881 , loss 0.272146  ,lr 3.4146e-05\n",
            "step 16901, trainning accuracy， 1 , testing accuracy， 0.7887 , loss 0.260461  ,lr 3.34665e-05\n",
            "step 17001, trainning accuracy， 1 , testing accuracy， 0.7873 , loss 0.273147  ,lr 3.28005e-05\n",
            "step 17101, trainning accuracy， 1 , testing accuracy， 0.7897 , loss 0.271969  ,lr 3.21478e-05\n",
            "step 17201, trainning accuracy， 1 , testing accuracy， 0.7891 , loss 0.263548  ,lr 3.1508e-05\n",
            "step 17301, trainning accuracy， 1 , testing accuracy， 0.789 , loss 0.265079  ,lr 3.0881e-05\n",
            "step 17401, trainning accuracy， 1 , testing accuracy， 0.7892 , loss 0.25996  ,lr 3.02665e-05\n",
            "step 17501, trainning accuracy， 1 , testing accuracy， 0.7886 , loss 0.254771  ,lr 2.96642e-05\n",
            "step 17601, trainning accuracy， 1 , testing accuracy， 0.7849 , loss 0.253946  ,lr 2.90739e-05\n",
            "step 17701, trainning accuracy， 1 , testing accuracy， 0.7907 , loss 0.271796  ,lr 2.84953e-05\n",
            "step 17801, trainning accuracy， 1 , testing accuracy， 0.7856 , loss 0.260984  ,lr 2.79282e-05\n",
            "step 17901, trainning accuracy， 1 , testing accuracy， 0.7881 , loss 0.255364  ,lr 2.73725e-05\n",
            "step 18001, trainning accuracy， 1 , testing accuracy， 0.7861 , loss 0.253098  ,lr 2.68278e-05\n",
            "step 18101, trainning accuracy， 1 , testing accuracy， 0.7909 , loss 0.251039  ,lr 2.62939e-05\n",
            "step 18201, trainning accuracy， 1 , testing accuracy， 0.7931 , loss 0.251727  ,lr 2.57706e-05\n",
            "step 18301, trainning accuracy， 1 , testing accuracy， 0.7919 , loss 0.247256  ,lr 2.52578e-05\n",
            "step 18401, trainning accuracy， 1 , testing accuracy， 0.7908 , loss 0.245575  ,lr 2.47552e-05\n",
            "step 18501, trainning accuracy， 1 , testing accuracy， 0.7943 , loss 0.265426  ,lr 2.42625e-05\n",
            "step 18601, trainning accuracy， 1 , testing accuracy， 0.7905 , loss 0.25868  ,lr 2.37797e-05\n",
            "step 18701, trainning accuracy， 1 , testing accuracy， 0.791 , loss 0.253933  ,lr 2.33065e-05\n",
            "step 18801, trainning accuracy， 1 , testing accuracy， 0.7901 , loss 0.246371  ,lr 2.28427e-05\n",
            "step 18901, trainning accuracy， 1 , testing accuracy， 0.7901 , loss 0.242727  ,lr 2.23881e-05\n",
            "step 19001, trainning accuracy， 0.996094 , testing accuracy， 0.7891 , loss 0.241701  ,lr 2.19426e-05\n",
            "step 19101, trainning accuracy， 1 , testing accuracy， 0.7885 , loss 0.248921  ,lr 2.1506e-05\n",
            "step 19201, trainning accuracy， 1 , testing accuracy， 0.7877 , loss 0.251814  ,lr 2.1078e-05\n",
            "step 19301, trainning accuracy， 1 , testing accuracy， 0.7865 , loss 0.262603  ,lr 2.06585e-05\n",
            "step 19401, trainning accuracy， 1 , testing accuracy， 0.7897 , loss 0.247013  ,lr 2.02474e-05\n",
            "step 19501, trainning accuracy， 1 , testing accuracy， 0.7892 , loss 0.242985  ,lr 1.98445e-05\n",
            "step 19601, trainning accuracy， 1 , testing accuracy， 0.7905 , loss 0.239293  ,lr 1.94496e-05\n",
            "step 19701, trainning accuracy， 1 , testing accuracy， 0.791 , loss 0.246485  ,lr 1.90626e-05\n",
            "step 19801, trainning accuracy， 1 , testing accuracy， 0.7948 , loss 0.245325  ,lr 1.86832e-05\n",
            "step 19901, trainning accuracy， 1 , testing accuracy， 0.7909 , loss 0.252268  ,lr 1.83114e-05\n",
            "save model:./cifar10/model.ckpt Finished\n",
            "test accuracy 0.7917\n",
            "precision score: [0.82233503 0.90349076 0.73881374 0.63820704 0.77525773 0.67988107\n",
            " 0.83333333 0.80074836 0.86164122 0.8493558 ]\n",
            "recall score: [0.81  0.88  0.71  0.598 0.752 0.686 0.865 0.856 0.903 0.857]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B3ENbqG55kkd",
        "outputId": "f5d7a63a-eb70-4983-d182-e4f1ecac9b9d"
      },
      "source": [
        "# 无BN 22min\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.layers as layers\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "#随机旋转，裁切等数据增强\n",
        "def data_enhace(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_hue(x, max_delta=0.2)\n",
        "    x = tf.image.random_brightness(x, max_delta=0.7)\n",
        "    result = tf.image.random_contrast(x, lower=0.2, upper=1.8)\n",
        "    return result\n",
        "\n",
        "#mixup数据增强\n",
        "def criterion(batch_x, batch_y, batch_size,alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    index = tf.random_shuffle(batch_size)\n",
        "    mixed_batchx = lam * batch_x + (1 - lam) * batch_x[index, :]\n",
        "\n",
        "    batch_y_= lam*batch_y+(1-lam)*batch_y[index]\n",
        "    return mixed_batchx,batch_y_\n",
        "\n",
        "#本地载入cifar10数据集\n",
        "def load_cifar10_batch(cifar10_dataset_folder_path,batch_id):\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id),mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding = 'latin1')\n",
        "\n",
        "    # features and labels\n",
        "    features = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return  features, labels\n",
        "\n",
        "#在线载入cifar10数据集\n",
        "def load_cifar10_batch_online():\n",
        "    (x_Train, y_Train), (x_Test, y_Test)=cifar10.load_data()\n",
        "    return x_Train,y_Train,x_Test,y_Test\n",
        "\n",
        "#cifar10数据集读取器\n",
        "class Cifar10DataReader():\n",
        "    def __init__(self, cifar_folder=None, onehot=True):\n",
        "        self.cifar_folder = cifar_folder\n",
        "        self.onehot = onehot\n",
        "        self.data_label_train = None\n",
        "        self.data_label_test = None\n",
        "        self.batch_index = 0\n",
        "\n",
        "        if cifar_folder==None:#在线\n",
        "            x_train, y_train,x_test,y_test=load_cifar10_batch_online()\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            self.data_label_test = list(zip(x_test, y_test))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "        else:#本地\n",
        "            x_train, y_train = load_cifar10_batch(self.cifar_folder, 1)\n",
        "            for i in range(2,6):\n",
        "                features,labels = load_cifar10_batch(self.cifar_folder, i)\n",
        "                x_train, y_train = np.concatenate([x_train, features]),np.concatenate([y_train, labels])\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "    #批次读取训练集数据\n",
        "    def next_train_data(self, batch_size=100):\n",
        "        if self.batch_index < len(self.data_label_train) // batch_size:\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有训练集数据\n",
        "    def all_train_data(self):\n",
        "        np.random.shuffle(self.data_label_train)\n",
        "        datum = self.data_label_train\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #独热编码\n",
        "    def _decode(self, datum, onehot):\n",
        "        rdata = list()\n",
        "        rlabel = list()\n",
        "        if onehot:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                hot = np.zeros(10)\n",
        "                hot[int(l)] = 1\n",
        "                rlabel.append(hot)\n",
        "        else:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                rlabel.append(int(l))\n",
        "        return rdata, rlabel\n",
        "\n",
        "    #批次读取测试集数据\n",
        "    def next_test_data(self, batch_size=100):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test[0:batch_size]\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有测试集数据\n",
        "    def all_test_data(self):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "\n",
        "#网络结构\n",
        "def weight_variable(shape):\n",
        "    weights = tf.get_variable(\"weights\", shape, initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    #加入l2正则化\n",
        "    regular=tf.multiply(tf.nn.l2_loss(weights),0.005,name='regular')\n",
        "    tf.add_to_collection('losses',regular)\n",
        "    return weights\n",
        "def bias_variable(shape):\n",
        "    biases = tf.get_variable(\"biases\", shape, initializer=tf.constant_initializer(0.0))\n",
        "    return biases\n",
        "def conv_layer(x, w, b, name, strides, padding = 'SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = weight_variable(w)\n",
        "        b = bias_variable(b)\n",
        "        conv_and_biased = tf.nn.conv2d(x, w, strides = strides, padding = padding, name = name) + b\n",
        "    return conv_and_biased\n",
        "def batch_normalization(inputs, scope, is_training=True, need_relu=True):\n",
        "    bn = tf.contrib.layers.batch_norm(inputs,\n",
        "        decay=0.999,\n",
        "        center=True, # 是否有beta\n",
        "        scale=True,  # 是否有gamma\n",
        "        epsilon=0.001,\n",
        "        activation_fn=None,\n",
        "        param_initializers=None, # beta, gamma, moving mean and moving variance的优化初始化\n",
        "        param_regularizers=None, # beta, gamma的正则化优化\n",
        "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
        "        is_training=is_training,  # 可以让学生实验True和False的区别。\n",
        "        reuse=None,\n",
        "        variables_collections=None,\n",
        "        outputs_collections=None,\n",
        "        trainable=True,\n",
        "        batch_weights=None,\n",
        "        fused=False,\n",
        "        data_format='NHWC',\n",
        "        zero_debias_moving_mean=False,\n",
        "        scope=scope,\n",
        "        renorm=False,\n",
        "        renorm_clipping=None,\n",
        "        renorm_decay=0.99)\n",
        "    if need_relu:\n",
        "        bn = tf.nn.relu(bn, name='relu')\n",
        "    return bn\n",
        "def maxpooling(x,kernal_size, strides, name):  # 最大池化，前面的是核大小，一般为[1, 2, 2, 1]，后面的strides指的是步长，如[1, 2, 2, 1]。\n",
        "    return tf.nn.max_pool(x, ksize=kernal_size, strides=strides, padding='SAME',name = name)\n",
        "def avg_pool(input_feats, k):\n",
        "    ksize = [1, k, k, 1]\n",
        "    strides = [1, k, k, 1]\n",
        "    padding = 'VALID'\n",
        "    output = tf.nn.avg_pool(input_feats, ksize, strides, padding)  # 平均池化\n",
        "    return output\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, train_flag,stride2=False):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters  # nb_filter1/2/3分别是64/64/256，三个整数。\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'  # 'res2a_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'  # 'bn2a_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    if stride2==False:\n",
        "        stride_for_first_conv = [1, 1, 1, 1]  # 。\n",
        "    else:\n",
        "        stride_for_first_conv = [1, 2, 2, 1]\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '2a')\n",
        "    # x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    # x = batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    # x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    shortcut = conv_layer(input_tensor, [1, 1, input_layers, nb_filter3], [nb_filter3], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '1')\n",
        "    # shortcut = batch_normalization(shortcut, scope=bn_name_base + '1', is_training=train_flag)\n",
        "    x = tf.add(x, shortcut)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block, train_flag):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2a')\n",
        "    # x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    # x =  batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    # x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    x = tf.add(x, input_tensor)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def resnet_graph(input_image, architecture,train_flag=True, stage5=False,ttf=True):  # 残差网络函数\n",
        "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
        "    # Stage 1\n",
        "    paddings = tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])  # 上下左右各补3个0。\n",
        "    x = tf.pad(input_image, paddings, \"CONSTANT\")\n",
        "    w = weight_variable([7, 7, 3, 64])\n",
        "    b = bias_variable([64])  #\n",
        "    x = tf.nn.conv2d(x, w, strides = [1, 2, 2, 1], padding = 'VALID', name = 'conv_1') + b\n",
        "    # x = batch_normalization(x, scope='bn_conv1', is_training=train_flag, need_relu=True)\n",
        "    C1 = x = maxpooling(x, [1, 3, 3, 1], [1, 2, 2, 1], name='stage1')\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', train_flag=train_flag)  # 结构块。\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_flag=train_flag)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_flag=train_flag, stride2=True)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_flag=train_flag)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_flag=train_flag)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_flag=train_flag, stride2=True)\n",
        "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  # block_count=22，即，下句for循环的range是0~22，正好是加23个层。\n",
        "    for i in range(block_count):  # 加22个层，都是用identity_block函数加。\n",
        "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_flag=train_flag)\n",
        "    C4 = x  #\n",
        "    # Stage 5\n",
        "    if stage5:\n",
        "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_flag=train_flag, stride2=True)\n",
        "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_flag=train_flag)\n",
        "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_flag=train_flag)\n",
        "        output=avg_pool(C5,1)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)#加入dropout\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "    else:\n",
        "        C5 = None\n",
        "        output=avg_pool(C4,2)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "\n",
        "    return [C1, C2, C3, C4, C5,output]\n",
        "\n",
        "\n",
        "\n",
        "filepath='C:/Users/zy109/Desktop/1/data/cifar-10-batches-py'\n",
        "model_save_path='C:/Users/zy109/Desktop/1/cifar10'\n",
        "data=Cifar10DataReader()\n",
        "sess=tf.InteractiveSession()\n",
        "\n",
        "#指数衰减学习率\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(learning_rate=0.001, global_step=global_step, decay_steps=50, decay_rate=0.99, staircase=False)\n",
        "\n",
        "#占位符\n",
        "input_image=tf.placeholder(tf.float32,[None,32,32,3])\n",
        "y_=tf.placeholder(tf.float32,[None,10])\n",
        "tf_is_train=tf.placeholder(tf.bool,None)\n",
        "input_image=data_enhace(input_image)#数据增强\n",
        "input_image,y_=criterion(input_image,y_,batch_size)#mixup\n",
        "\n",
        "[C1,C2,C3,C4,C5,output]=resnet_graph(input_image,'resnet50',ttf=tf_is_train)\n",
        "\n",
        "#交叉熵损失\n",
        "cross_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_,logits=output))\n",
        "\n",
        "tf.add_to_collection('losses',cross_loss)\n",
        "loss=tf.add_n(tf.get_collection('losses'))\n",
        "train_step=tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "\n",
        "#正确率\n",
        "correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y_,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "sess.run(tf.initialize_all_variables())\n",
        "saver = tf.train.Saver()\n",
        "#读取已经训练好的权重\n",
        "ckpt = tf.train.get_checkpoint_state(model_save_path)\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "#加载测试集数据\n",
        "test_data=data.all_test_data()\n",
        "x_test=np.array(test_data[0])/255\n",
        "y_test=np.array(test_data[1])\n",
        "\n",
        "total_loss=[]\n",
        "total_acc=[]\n",
        "total_step=[]\n",
        "\n",
        "\n",
        "# 训练\n",
        "for i in range(20000):\n",
        "\n",
        "    #逐批加载训练集数据\n",
        "    train_data=data.next_train_data(batch_size)\n",
        "    x_train=np.array(train_data[0])/255\n",
        "    y_train=np.array(train_data[1])\n",
        "\n",
        "    train_step.run(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        # 测试准确率\n",
        "        train_accuracy = accuracy.eval(feed_dict={input_image: x_train,y_: y_train,tf_is_train:False})\n",
        "        test_accuracy = accuracy.eval(feed_dict={input_image: x_test,y_: y_test,tf_is_train:False})\n",
        "        # 该次训练的损失\n",
        "        loss_value = loss.eval(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "        global_var=sess.run(global_step)\n",
        "        lr_val=sess.run(lr)\n",
        "        print(\"step %d, trainning accuracy， %g , testing accuracy， %g , loss %g  ,lr %g\" % (global_var, train_accuracy,test_accuracy, loss_value,lr_val))\n",
        "        total_step.append(i)\n",
        "        total_loss.append(loss_value)\n",
        "        total_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "save_path = saver.save(sess,\"./cifar10/model.ckpt\")\n",
        "print(\"save model:{0} Finished\".format(save_path))\n",
        "np.save(\"cifar10_step\",total_step)\n",
        "np.save(\"cifar10_loss\",total_loss)\n",
        "np.save(\"cifar10_acc\",total_acc)\n",
        "\n",
        "#测试\n",
        "y_log_predict=output.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "test_accuracy = accuracy.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "print(\"test accuracy %g\" % test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test.argmax(axis=1), y_log_predict.argmax(axis=1))\n",
        "\n",
        "\n",
        "for i in range(len(y_log_predict)):\n",
        "        max_value=max(y_log_predict[i])\n",
        "        for j in range(len(y_log_predict[i])):\n",
        "            if max_value==y_log_predict[i][j]:\n",
        "                y_log_predict[i][j]=1\n",
        "            else:\n",
        "                y_log_predict[i][j]=0\n",
        "\n",
        "# 精确率\n",
        "from sklearn.metrics import precision_score\n",
        "ps=precision_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_precision_score\",ps)\n",
        "\n",
        "#召回率\n",
        "from sklearn.metrics import recall_score\n",
        "rs=recall_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_recall_score\",rs)\n",
        "\n",
        "print(\"precision score:\",ps)\n",
        "print(\"recall score:\",rs)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "ax1.plot(total_step, total_loss, color=\"blue\", label=\"loss\")\n",
        "ax1.set_xlabel(\"step\")\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_step, total_acc, color=\"red\", label=\"accuary\")\n",
        "ax2.set_ylabel(\"accuary\")\n",
        "\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
        "plt.show()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-fbeb70415f96>:249: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-fbeb70415f96>:280: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "step 1, trainning accuracy， 0.078125 , testing accuracy， 0.1 , loss 36092.1  ,lr 0.000999799\n",
            "step 101, trainning accuracy， 0.117188 , testing accuracy， 0.129 , loss 88.357  ,lr 0.000979903\n",
            "step 201, trainning accuracy， 0.128906 , testing accuracy， 0.111 , loss 81.9189  ,lr 0.000960403\n",
            "step 301, trainning accuracy， 0.109375 , testing accuracy， 0.1007 , loss 79.3128  ,lr 0.000941291\n",
            "step 401, trainning accuracy， 0.0976562 , testing accuracy， 0.1 , loss 78.5787  ,lr 0.000922559\n",
            "step 501, trainning accuracy， 0.121094 , testing accuracy， 0.1 , loss 78.4849  ,lr 0.0009042\n",
            "step 601, trainning accuracy， 0.113281 , testing accuracy， 0.1 , loss 78.3954  ,lr 0.000886207\n",
            "step 701, trainning accuracy， 0.109375 , testing accuracy， 0.1 , loss 78.3273  ,lr 0.000868571\n",
            "step 801, trainning accuracy， 0.09375 , testing accuracy， 0.1 , loss 78.2556  ,lr 0.000851287\n",
            "step 901, trainning accuracy， 0.140625 , testing accuracy， 0.1 , loss 78.1635  ,lr 0.000834346\n",
            "step 1001, trainning accuracy， 0.125 , testing accuracy， 0.1 , loss 78.0651  ,lr 0.000817743\n",
            "step 1101, trainning accuracy， 0.152344 , testing accuracy， 0.1 , loss 77.9797  ,lr 0.00080147\n",
            "step 1201, trainning accuracy， 0.117188 , testing accuracy， 0.1 , loss 77.868  ,lr 0.00078552\n",
            "step 1301, trainning accuracy， 0.109375 , testing accuracy， 0.1 , loss 77.7463  ,lr 0.000769889\n",
            "step 1401, trainning accuracy， 0.101562 , testing accuracy， 0.1 , loss 77.6466  ,lr 0.000754568\n",
            "step 1501, trainning accuracy， 0.101562 , testing accuracy， 0.1 , loss 77.5384  ,lr 0.000739552\n",
            "step 1601, trainning accuracy， 0.109375 , testing accuracy， 0.1 , loss 77.3897  ,lr 0.000724835\n",
            "step 1701, trainning accuracy， 0.0859375 , testing accuracy， 0.1 , loss 77.2674  ,lr 0.000710411\n",
            "step 1801, trainning accuracy， 0.0898438 , testing accuracy， 0.1 , loss 77.1374  ,lr 0.000696274\n",
            "step 1901, trainning accuracy， 0.105469 , testing accuracy， 0.1 , loss 77.0031  ,lr 0.000682418\n",
            "step 2001, trainning accuracy， 0.101562 , testing accuracy， 0.1 , loss 76.8507  ,lr 0.000668838\n",
            "step 2101, trainning accuracy， 0.0976562 , testing accuracy， 0.1 , loss 76.7131  ,lr 0.000655528\n",
            "step 2201, trainning accuracy， 0.09375 , testing accuracy， 0.1 , loss 76.5604  ,lr 0.000642483\n",
            "step 2301, trainning accuracy， 0.078125 , testing accuracy， 0.1 , loss 76.406  ,lr 0.000629697\n",
            "step 2401, trainning accuracy， 0.113281 , testing accuracy， 0.1 , loss 76.2431  ,lr 0.000617166\n",
            "step 2501, trainning accuracy， 0.136719 , testing accuracy， 0.1 , loss 76.0664  ,lr 0.000604885\n",
            "step 2601, trainning accuracy， 0.128906 , testing accuracy， 0.1008 , loss 75.9113  ,lr 0.000592848\n",
            "step 2701, trainning accuracy， 0.128906 , testing accuracy， 0.1009 , loss 75.7487  ,lr 0.00058105\n",
            "step 2801, trainning accuracy， 0.0820312 , testing accuracy， 0.128 , loss 75.5694  ,lr 0.000569487\n",
            "step 2901, trainning accuracy， 0.128906 , testing accuracy， 0.1086 , loss 75.3537  ,lr 0.000558154\n",
            "step 3001, trainning accuracy， 0.144531 , testing accuracy， 0.1511 , loss 75.1537  ,lr 0.000547047\n",
            "step 3101, trainning accuracy， 0.148438 , testing accuracy， 0.1585 , loss 74.9259  ,lr 0.000536161\n",
            "step 3201, trainning accuracy， 0.128906 , testing accuracy， 0.1648 , loss 74.7085  ,lr 0.000525491\n",
            "step 3301, trainning accuracy， 0.160156 , testing accuracy， 0.1781 , loss 74.4767  ,lr 0.000515034\n",
            "step 3401, trainning accuracy， 0.203125 , testing accuracy， 0.1845 , loss 74.1909  ,lr 0.000504785\n",
            "step 3501, trainning accuracy， 0.21875 , testing accuracy， 0.1865 , loss 74.0934  ,lr 0.00049474\n",
            "step 3601, trainning accuracy， 0.21875 , testing accuracy， 0.1871 , loss 73.7454  ,lr 0.000484894\n",
            "step 3701, trainning accuracy， 0.222656 , testing accuracy， 0.1969 , loss 73.4849  ,lr 0.000475245\n",
            "step 3801, trainning accuracy， 0.238281 , testing accuracy， 0.2168 , loss 73.2459  ,lr 0.000465788\n",
            "step 3901, trainning accuracy， 0.175781 , testing accuracy， 0.2 , loss 73.0262  ,lr 0.000456518\n",
            "step 4001, trainning accuracy， 0.25 , testing accuracy， 0.2148 , loss 72.7322  ,lr 0.000447434\n",
            "step 4101, trainning accuracy， 0.210938 , testing accuracy， 0.1936 , loss 72.5426  ,lr 0.00043853\n",
            "step 4201, trainning accuracy， 0.179688 , testing accuracy， 0.2279 , loss 72.1747  ,lr 0.000429803\n",
            "step 4301, trainning accuracy， 0.226562 , testing accuracy， 0.2241 , loss 71.9061  ,lr 0.00042125\n",
            "step 4401, trainning accuracy， 0.257812 , testing accuracy， 0.2141 , loss 71.6847  ,lr 0.000412867\n",
            "step 4501, trainning accuracy， 0.207031 , testing accuracy， 0.2225 , loss 71.4243  ,lr 0.000404651\n",
            "step 4601, trainning accuracy， 0.226562 , testing accuracy， 0.2196 , loss 71.0653  ,lr 0.000396598\n",
            "step 4701, trainning accuracy， 0.25 , testing accuracy， 0.2091 , loss 70.7389  ,lr 0.000388706\n",
            "step 4801, trainning accuracy， 0.25 , testing accuracy， 0.2244 , loss 70.4659  ,lr 0.000380971\n",
            "step 4901, trainning accuracy， 0.253906 , testing accuracy， 0.2137 , loss 70.1334  ,lr 0.00037339\n",
            "step 5001, trainning accuracy， 0.261719 , testing accuracy， 0.2144 , loss 69.8921  ,lr 0.000365959\n",
            "step 5101, trainning accuracy， 0.222656 , testing accuracy， 0.2177 , loss 69.5928  ,lr 0.000358677\n",
            "step 5201, trainning accuracy， 0.25 , testing accuracy， 0.2269 , loss 69.2388  ,lr 0.000351539\n",
            "step 5301, trainning accuracy， 0.226562 , testing accuracy， 0.2123 , loss 68.9137  ,lr 0.000344543\n",
            "step 5401, trainning accuracy， 0.230469 , testing accuracy， 0.2232 , loss 68.5836  ,lr 0.000337687\n",
            "step 5501, trainning accuracy， 0.207031 , testing accuracy， 0.2281 , loss 68.2196  ,lr 0.000330967\n",
            "step 5601, trainning accuracy， 0.222656 , testing accuracy， 0.2233 , loss 67.8587  ,lr 0.000324381\n",
            "step 5701, trainning accuracy， 0.214844 , testing accuracy， 0.2258 , loss 67.6144  ,lr 0.000317926\n",
            "step 5801, trainning accuracy， 0.207031 , testing accuracy， 0.2251 , loss 67.23  ,lr 0.000311599\n",
            "step 5901, trainning accuracy， 0.257812 , testing accuracy， 0.2248 , loss 66.8151  ,lr 0.000305398\n",
            "step 6001, trainning accuracy， 0.246094 , testing accuracy， 0.2303 , loss 66.4337  ,lr 0.000299321\n",
            "step 6101, trainning accuracy， 0.257812 , testing accuracy， 0.2269 , loss 66.043  ,lr 0.000293364\n",
            "step 6201, trainning accuracy， 0.21875 , testing accuracy， 0.2346 , loss 65.6837  ,lr 0.000287526\n",
            "step 6301, trainning accuracy， 0.222656 , testing accuracy， 0.2179 , loss 65.3005  ,lr 0.000281804\n",
            "step 6401, trainning accuracy， 0.246094 , testing accuracy， 0.2311 , loss 64.8352  ,lr 0.000276196\n",
            "step 6501, trainning accuracy， 0.230469 , testing accuracy， 0.2233 , loss 64.4765  ,lr 0.0002707\n",
            "step 6601, trainning accuracy， 0.246094 , testing accuracy， 0.2227 , loss 64.0194  ,lr 0.000265313\n",
            "step 6701, trainning accuracy， 0.210938 , testing accuracy， 0.2405 , loss 63.6343  ,lr 0.000260034\n",
            "step 6801, trainning accuracy， 0.253906 , testing accuracy， 0.2519 , loss 63.1812  ,lr 0.000254859\n",
            "step 6901, trainning accuracy， 0.265625 , testing accuracy， 0.2604 , loss 62.7435  ,lr 0.000249787\n",
            "step 7001, trainning accuracy， 0.265625 , testing accuracy， 0.2788 , loss 62.3611  ,lr 0.000244816\n",
            "step 7101, trainning accuracy， 0.261719 , testing accuracy， 0.2659 , loss 61.8448  ,lr 0.000239945\n",
            "step 7201, trainning accuracy， 0.273438 , testing accuracy， 0.2861 , loss 61.3834  ,lr 0.00023517\n",
            "step 7301, trainning accuracy， 0.324219 , testing accuracy， 0.306 , loss 60.9314  ,lr 0.00023049\n",
            "step 7401, trainning accuracy， 0.226562 , testing accuracy， 0.2965 , loss 60.4892  ,lr 0.000225903\n",
            "step 7501, trainning accuracy， 0.28125 , testing accuracy， 0.2961 , loss 59.9915  ,lr 0.000221408\n",
            "step 7601, trainning accuracy， 0.285156 , testing accuracy， 0.3121 , loss 59.5299  ,lr 0.000217002\n",
            "step 7701, trainning accuracy， 0.324219 , testing accuracy， 0.338 , loss 59.0058  ,lr 0.000212683\n",
            "step 7801, trainning accuracy， 0.355469 , testing accuracy， 0.3263 , loss 58.4447  ,lr 0.000208451\n",
            "step 7901, trainning accuracy， 0.359375 , testing accuracy， 0.3406 , loss 57.9513  ,lr 0.000204303\n",
            "step 8001, trainning accuracy， 0.363281 , testing accuracy， 0.3539 , loss 57.3564  ,lr 0.000200237\n",
            "step 8101, trainning accuracy， 0.328125 , testing accuracy， 0.3677 , loss 56.8923  ,lr 0.000196252\n",
            "step 8201, trainning accuracy， 0.339844 , testing accuracy， 0.36 , loss 56.3712  ,lr 0.000192347\n",
            "step 8301, trainning accuracy， 0.378906 , testing accuracy， 0.3768 , loss 55.7482  ,lr 0.000188519\n",
            "step 8401, trainning accuracy， 0.324219 , testing accuracy， 0.3837 , loss 55.3416  ,lr 0.000184768\n",
            "step 8501, trainning accuracy， 0.371094 , testing accuracy， 0.3915 , loss 54.6953  ,lr 0.000181091\n",
            "step 8601, trainning accuracy， 0.402344 , testing accuracy， 0.4063 , loss 54.1774  ,lr 0.000177487\n",
            "step 8701, trainning accuracy， 0.382812 , testing accuracy， 0.4067 , loss 53.6668  ,lr 0.000173955\n",
            "step 8801, trainning accuracy， 0.46875 , testing accuracy， 0.4051 , loss 53.0094  ,lr 0.000170493\n",
            "step 8901, trainning accuracy， 0.375 , testing accuracy， 0.4017 , loss 52.5089  ,lr 0.000167101\n",
            "step 9001, trainning accuracy， 0.464844 , testing accuracy， 0.411 , loss 51.8158  ,lr 0.000163775\n",
            "step 9101, trainning accuracy， 0.433594 , testing accuracy， 0.4213 , loss 51.305  ,lr 0.000160516\n",
            "step 9201, trainning accuracy， 0.421875 , testing accuracy， 0.4164 , loss 50.8136  ,lr 0.000157322\n",
            "step 9301, trainning accuracy， 0.449219 , testing accuracy， 0.4348 , loss 50.0941  ,lr 0.000154191\n",
            "step 9401, trainning accuracy， 0.382812 , testing accuracy， 0.4341 , loss 49.6206  ,lr 0.000151123\n",
            "step 9501, trainning accuracy， 0.464844 , testing accuracy， 0.4341 , loss 48.9381  ,lr 0.000148115\n",
            "step 9601, trainning accuracy， 0.414062 , testing accuracy， 0.4313 , loss 48.411  ,lr 0.000145168\n",
            "step 9701, trainning accuracy， 0.414062 , testing accuracy， 0.4389 , loss 47.7815  ,lr 0.000142279\n",
            "step 9801, trainning accuracy， 0.445312 , testing accuracy， 0.427 , loss 47.2023  ,lr 0.000139448\n",
            "step 9901, trainning accuracy， 0.417969 , testing accuracy， 0.4505 , loss 46.6055  ,lr 0.000136673\n",
            "step 10001, trainning accuracy， 0.472656 , testing accuracy， 0.4575 , loss 45.9183  ,lr 0.000133953\n",
            "step 10101, trainning accuracy， 0.496094 , testing accuracy， 0.465 , loss 45.3253  ,lr 0.000131287\n",
            "step 10201, trainning accuracy， 0.441406 , testing accuracy， 0.4627 , loss 44.7733  ,lr 0.000128675\n",
            "step 10301, trainning accuracy， 0.457031 , testing accuracy， 0.48 , loss 44.0845  ,lr 0.000126114\n",
            "step 10401, trainning accuracy， 0.503906 , testing accuracy， 0.4854 , loss 43.5037  ,lr 0.000123604\n",
            "step 10501, trainning accuracy， 0.488281 , testing accuracy， 0.4946 , loss 42.8744  ,lr 0.000121145\n",
            "step 10601, trainning accuracy， 0.484375 , testing accuracy， 0.4796 , loss 42.2098  ,lr 0.000118734\n",
            "step 10701, trainning accuracy， 0.449219 , testing accuracy， 0.4979 , loss 41.6084  ,lr 0.000116371\n",
            "step 10801, trainning accuracy， 0.519531 , testing accuracy， 0.494 , loss 40.8916  ,lr 0.000114055\n",
            "step 10901, trainning accuracy， 0.5625 , testing accuracy， 0.5023 , loss 40.1788  ,lr 0.000111786\n",
            "step 11001, trainning accuracy， 0.492188 , testing accuracy， 0.4965 , loss 39.7483  ,lr 0.000109561\n",
            "step 11101, trainning accuracy， 0.566406 , testing accuracy， 0.513 , loss 39.0576  ,lr 0.000107381\n",
            "step 11201, trainning accuracy， 0.480469 , testing accuracy， 0.5048 , loss 38.4774  ,lr 0.000105244\n",
            "step 11301, trainning accuracy， 0.527344 , testing accuracy， 0.5205 , loss 37.8776  ,lr 0.00010315\n",
            "step 11401, trainning accuracy， 0.53125 , testing accuracy， 0.5207 , loss 37.1787  ,lr 0.000101097\n",
            "step 11501, trainning accuracy， 0.542969 , testing accuracy， 0.5301 , loss 36.5681  ,lr 9.90851e-05\n",
            "step 11601, trainning accuracy， 0.539062 , testing accuracy， 0.5162 , loss 35.9439  ,lr 9.71133e-05\n",
            "step 11701, trainning accuracy， 0.5625 , testing accuracy， 0.5333 , loss 35.2031  ,lr 9.51808e-05\n",
            "step 11801, trainning accuracy， 0.628906 , testing accuracy， 0.5291 , loss 34.5756  ,lr 9.32867e-05\n",
            "step 11901, trainning accuracy， 0.480469 , testing accuracy， 0.5366 , loss 34.1726  ,lr 9.14303e-05\n",
            "step 12001, trainning accuracy， 0.546875 , testing accuracy， 0.5411 , loss 33.5202  ,lr 8.96108e-05\n",
            "step 12101, trainning accuracy， 0.550781 , testing accuracy， 0.5492 , loss 32.9052  ,lr 8.78276e-05\n",
            "step 12201, trainning accuracy， 0.585938 , testing accuracy， 0.5403 , loss 32.2195  ,lr 8.60798e-05\n",
            "step 12301, trainning accuracy， 0.601562 , testing accuracy， 0.5406 , loss 31.6167  ,lr 8.43668e-05\n",
            "step 12401, trainning accuracy， 0.542969 , testing accuracy， 0.5581 , loss 31.0806  ,lr 8.26879e-05\n",
            "step 12501, trainning accuracy， 0.507812 , testing accuracy， 0.5562 , loss 30.599  ,lr 8.10424e-05\n",
            "step 12601, trainning accuracy， 0.492188 , testing accuracy， 0.5505 , loss 30.1115  ,lr 7.94297e-05\n",
            "step 12701, trainning accuracy， 0.636719 , testing accuracy， 0.5466 , loss 29.1899  ,lr 7.7849e-05\n",
            "step 12801, trainning accuracy， 0.601562 , testing accuracy， 0.5716 , loss 28.6995  ,lr 7.62998e-05\n",
            "step 12901, trainning accuracy， 0.582031 , testing accuracy， 0.5616 , loss 28.141  ,lr 7.47815e-05\n",
            "step 13001, trainning accuracy， 0.59375 , testing accuracy， 0.5637 , loss 27.636  ,lr 7.32933e-05\n",
            "step 13101, trainning accuracy， 0.550781 , testing accuracy， 0.5532 , loss 27.0916  ,lr 7.18348e-05\n",
            "step 13201, trainning accuracy， 0.597656 , testing accuracy， 0.5823 , loss 26.471  ,lr 7.04053e-05\n",
            "step 13301, trainning accuracy， 0.585938 , testing accuracy， 0.5766 , loss 25.8772  ,lr 6.90042e-05\n",
            "step 13401, trainning accuracy， 0.578125 , testing accuracy， 0.5695 , loss 25.4723  ,lr 6.7631e-05\n",
            "step 13501, trainning accuracy， 0.566406 , testing accuracy， 0.5718 , loss 24.938  ,lr 6.62852e-05\n",
            "step 13601, trainning accuracy， 0.613281 , testing accuracy， 0.5773 , loss 24.4296  ,lr 6.49661e-05\n",
            "step 13701, trainning accuracy， 0.601562 , testing accuracy， 0.573 , loss 23.8534  ,lr 6.36733e-05\n",
            "step 13801, trainning accuracy， 0.632812 , testing accuracy， 0.5671 , loss 23.349  ,lr 6.24062e-05\n",
            "step 13901, trainning accuracy， 0.578125 , testing accuracy， 0.5847 , loss 22.8876  ,lr 6.11643e-05\n",
            "step 14001, trainning accuracy， 0.566406 , testing accuracy， 0.5803 , loss 22.456  ,lr 5.99471e-05\n",
            "step 14101, trainning accuracy， 0.566406 , testing accuracy， 0.5765 , loss 21.8781  ,lr 5.87542e-05\n",
            "step 14201, trainning accuracy， 0.597656 , testing accuracy， 0.5916 , loss 21.4103  ,lr 5.7585e-05\n",
            "step 14301, trainning accuracy， 0.601562 , testing accuracy， 0.5875 , loss 21.0061  ,lr 5.6439e-05\n",
            "step 14401, trainning accuracy， 0.574219 , testing accuracy， 0.5926 , loss 20.4349  ,lr 5.53159e-05\n",
            "step 14501, trainning accuracy， 0.617188 , testing accuracy， 0.5978 , loss 20.0194  ,lr 5.42151e-05\n",
            "step 14601, trainning accuracy， 0.589844 , testing accuracy， 0.5971 , loss 19.6975  ,lr 5.31362e-05\n",
            "step 14701, trainning accuracy， 0.683594 , testing accuracy， 0.5961 , loss 18.9776  ,lr 5.20788e-05\n",
            "step 14801, trainning accuracy， 0.59375 , testing accuracy， 0.602 , loss 18.7366  ,lr 5.10425e-05\n",
            "step 14901, trainning accuracy， 0.648438 , testing accuracy， 0.5952 , loss 18.2461  ,lr 5.00267e-05\n",
            "step 15001, trainning accuracy， 0.597656 , testing accuracy， 0.5997 , loss 17.8669  ,lr 4.90312e-05\n",
            "step 15101, trainning accuracy， 0.609375 , testing accuracy， 0.5876 , loss 17.5619  ,lr 4.80555e-05\n",
            "step 15201, trainning accuracy， 0.617188 , testing accuracy， 0.5862 , loss 17.0452  ,lr 4.70992e-05\n",
            "step 15301, trainning accuracy， 0.589844 , testing accuracy， 0.6091 , loss 16.7499  ,lr 4.61619e-05\n",
            "step 15401, trainning accuracy， 0.664062 , testing accuracy， 0.6037 , loss 16.2749  ,lr 4.52433e-05\n",
            "step 15501, trainning accuracy， 0.660156 , testing accuracy， 0.6044 , loss 15.9605  ,lr 4.43429e-05\n",
            "step 15601, trainning accuracy， 0.597656 , testing accuracy， 0.5921 , loss 15.6107  ,lr 4.34605e-05\n",
            "step 15701, trainning accuracy， 0.613281 , testing accuracy， 0.6075 , loss 15.344  ,lr 4.25956e-05\n",
            "step 15801, trainning accuracy， 0.609375 , testing accuracy， 0.6103 , loss 14.91  ,lr 4.1748e-05\n",
            "step 15901, trainning accuracy， 0.59375 , testing accuracy， 0.6087 , loss 14.6913  ,lr 4.09172e-05\n",
            "step 16001, trainning accuracy， 0.628906 , testing accuracy， 0.6164 , loss 14.2423  ,lr 4.0103e-05\n",
            "step 16101, trainning accuracy， 0.585938 , testing accuracy， 0.6041 , loss 14.0198  ,lr 3.93049e-05\n",
            "step 16201, trainning accuracy， 0.625 , testing accuracy， 0.6134 , loss 13.6719  ,lr 3.85227e-05\n",
            "step 16301, trainning accuracy， 0.664062 , testing accuracy， 0.6197 , loss 13.297  ,lr 3.77561e-05\n",
            "step 16401, trainning accuracy， 0.589844 , testing accuracy， 0.6125 , loss 13.1559  ,lr 3.70048e-05\n",
            "step 16501, trainning accuracy， 0.652344 , testing accuracy， 0.6207 , loss 12.8939  ,lr 3.62684e-05\n",
            "step 16601, trainning accuracy， 0.625 , testing accuracy， 0.6181 , loss 12.5891  ,lr 3.55467e-05\n",
            "step 16701, trainning accuracy， 0.628906 , testing accuracy， 0.6189 , loss 12.1989  ,lr 3.48393e-05\n",
            "step 16801, trainning accuracy， 0.65625 , testing accuracy， 0.6261 , loss 11.9528  ,lr 3.4146e-05\n",
            "step 16901, trainning accuracy， 0.613281 , testing accuracy， 0.6236 , loss 11.761  ,lr 3.34665e-05\n",
            "step 17001, trainning accuracy， 0.644531 , testing accuracy， 0.6186 , loss 11.5242  ,lr 3.28005e-05\n",
            "step 17101, trainning accuracy， 0.636719 , testing accuracy， 0.6284 , loss 11.3195  ,lr 3.21478e-05\n",
            "step 17201, trainning accuracy， 0.632812 , testing accuracy， 0.6262 , loss 11.1133  ,lr 3.1508e-05\n",
            "step 17301, trainning accuracy， 0.613281 , testing accuracy， 0.6176 , loss 10.9941  ,lr 3.0881e-05\n",
            "step 17401, trainning accuracy， 0.648438 , testing accuracy， 0.6223 , loss 10.5633  ,lr 3.02665e-05\n",
            "step 17501, trainning accuracy， 0.664062 , testing accuracy， 0.6301 , loss 10.4422  ,lr 2.96642e-05\n",
            "step 17601, trainning accuracy， 0.605469 , testing accuracy， 0.624 , loss 10.3044  ,lr 2.90739e-05\n",
            "step 17701, trainning accuracy， 0.648438 , testing accuracy， 0.6243 , loss 9.99199  ,lr 2.84953e-05\n",
            "step 17801, trainning accuracy， 0.652344 , testing accuracy， 0.6217 , loss 9.87508  ,lr 2.79282e-05\n",
            "step 17901, trainning accuracy， 0.628906 , testing accuracy， 0.6292 , loss 9.82534  ,lr 2.73725e-05\n",
            "step 18001, trainning accuracy， 0.617188 , testing accuracy， 0.628 , loss 9.57432  ,lr 2.68278e-05\n",
            "step 18101, trainning accuracy， 0.625 , testing accuracy， 0.6293 , loss 9.42168  ,lr 2.62939e-05\n",
            "step 18201, trainning accuracy， 0.636719 , testing accuracy， 0.6358 , loss 9.24848  ,lr 2.57706e-05\n",
            "step 18301, trainning accuracy， 0.683594 , testing accuracy， 0.6281 , loss 9.0315  ,lr 2.52578e-05\n",
            "step 18401, trainning accuracy， 0.667969 , testing accuracy， 0.6317 , loss 8.87556  ,lr 2.47552e-05\n",
            "step 18501, trainning accuracy， 0.640625 , testing accuracy， 0.6313 , loss 8.79689  ,lr 2.42625e-05\n",
            "step 18601, trainning accuracy， 0.640625 , testing accuracy， 0.6319 , loss 8.6609  ,lr 2.37797e-05\n",
            "step 18701, trainning accuracy， 0.703125 , testing accuracy， 0.6381 , loss 8.44129  ,lr 2.33065e-05\n",
            "step 18801, trainning accuracy， 0.726562 , testing accuracy， 0.6275 , loss 8.2526  ,lr 2.28427e-05\n",
            "step 18901, trainning accuracy， 0.648438 , testing accuracy， 0.6381 , loss 8.2239  ,lr 2.23881e-05\n",
            "step 19001, trainning accuracy， 0.648438 , testing accuracy， 0.6304 , loss 8.21932  ,lr 2.19426e-05\n",
            "step 19101, trainning accuracy， 0.648438 , testing accuracy， 0.636 , loss 8.07709  ,lr 2.1506e-05\n",
            "step 19201, trainning accuracy， 0.695312 , testing accuracy， 0.6356 , loss 7.86777  ,lr 2.1078e-05\n",
            "step 19301, trainning accuracy， 0.660156 , testing accuracy， 0.6385 , loss 7.8287  ,lr 2.06585e-05\n",
            "step 19401, trainning accuracy， 0.671875 , testing accuracy， 0.6397 , loss 7.70806  ,lr 2.02474e-05\n",
            "step 19501, trainning accuracy， 0.667969 , testing accuracy， 0.6406 , loss 7.66198  ,lr 1.98445e-05\n",
            "step 19601, trainning accuracy， 0.65625 , testing accuracy， 0.6407 , loss 7.52545  ,lr 1.94496e-05\n",
            "step 19701, trainning accuracy， 0.675781 , testing accuracy， 0.6352 , loss 7.41871  ,lr 1.90626e-05\n",
            "step 19801, trainning accuracy， 0.585938 , testing accuracy， 0.6389 , loss 7.50976  ,lr 1.86832e-05\n",
            "step 19901, trainning accuracy， 0.625 , testing accuracy， 0.6415 , loss 7.33738  ,lr 1.83114e-05\n",
            "save model:./cifar10/model.ckpt Finished\n",
            "test accuracy 0.6396\n",
            "precision score: [0.77631579 0.77947154 0.55476529 0.42167256 0.51531815 0.49652778\n",
            " 0.67716535 0.6741784  0.79453441 0.72194638]\n",
            "recall score: [0.649 0.767 0.39  0.358 0.656 0.572 0.774 0.718 0.785 0.727]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hvqElCnsd8BW",
        "outputId": "a6604edb-4e94-4b4c-e411-bb636a1b2500"
      },
      "source": [
        "# 无center scale 36min\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.layers as layers\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "#随机旋转，裁切等数据增强\n",
        "def data_enhace(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_hue(x, max_delta=0.2)\n",
        "    x = tf.image.random_brightness(x, max_delta=0.7)\n",
        "    result = tf.image.random_contrast(x, lower=0.2, upper=1.8)\n",
        "    return result\n",
        "\n",
        "#mixup数据增强\n",
        "def criterion(batch_x, batch_y, batch_size,alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    index = tf.random_shuffle(batch_size)\n",
        "    mixed_batchx = lam * batch_x + (1 - lam) * batch_x[index, :]\n",
        "\n",
        "    batch_y_= lam*batch_y+(1-lam)*batch_y[index]\n",
        "    return mixed_batchx,batch_y_\n",
        "\n",
        "#本地载入cifar10数据集\n",
        "def load_cifar10_batch(cifar10_dataset_folder_path,batch_id):\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id),mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding = 'latin1')\n",
        "\n",
        "    # features and labels\n",
        "    features = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return  features, labels\n",
        "\n",
        "#在线载入cifar10数据集\n",
        "def load_cifar10_batch_online():\n",
        "    (x_Train, y_Train), (x_Test, y_Test)=cifar10.load_data()\n",
        "    return x_Train,y_Train,x_Test,y_Test\n",
        "\n",
        "#cifar10数据集读取器\n",
        "class Cifar10DataReader():\n",
        "    def __init__(self, cifar_folder=None, onehot=True):\n",
        "        self.cifar_folder = cifar_folder\n",
        "        self.onehot = onehot\n",
        "        self.data_label_train = None\n",
        "        self.data_label_test = None\n",
        "        self.batch_index = 0\n",
        "\n",
        "        if cifar_folder==None:#在线\n",
        "            x_train, y_train,x_test,y_test=load_cifar10_batch_online()\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            self.data_label_test = list(zip(x_test, y_test))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "        else:#本地\n",
        "            x_train, y_train = load_cifar10_batch(self.cifar_folder, 1)\n",
        "            for i in range(2,6):\n",
        "                features,labels = load_cifar10_batch(self.cifar_folder, i)\n",
        "                x_train, y_train = np.concatenate([x_train, features]),np.concatenate([y_train, labels])\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "    #批次读取训练集数据\n",
        "    def next_train_data(self, batch_size=100):\n",
        "        if self.batch_index < len(self.data_label_train) // batch_size:\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有训练集数据\n",
        "    def all_train_data(self):\n",
        "        np.random.shuffle(self.data_label_train)\n",
        "        datum = self.data_label_train\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #独热编码\n",
        "    def _decode(self, datum, onehot):\n",
        "        rdata = list()\n",
        "        rlabel = list()\n",
        "        if onehot:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                hot = np.zeros(10)\n",
        "                hot[int(l)] = 1\n",
        "                rlabel.append(hot)\n",
        "        else:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                rlabel.append(int(l))\n",
        "        return rdata, rlabel\n",
        "\n",
        "    #批次读取测试集数据\n",
        "    def next_test_data(self, batch_size=100):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test[0:batch_size]\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有测试集数据\n",
        "    def all_test_data(self):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "\n",
        "#网络结构\n",
        "def weight_variable(shape):\n",
        "    weights = tf.get_variable(\"weights\", shape, initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    #加入l2正则化\n",
        "    regular=tf.multiply(tf.nn.l2_loss(weights),0.005,name='regular')\n",
        "    tf.add_to_collection('losses',regular)\n",
        "    return weights\n",
        "def bias_variable(shape):\n",
        "    biases = tf.get_variable(\"biases\", shape, initializer=tf.constant_initializer(0.0))\n",
        "    return biases\n",
        "def conv_layer(x, w, b, name, strides, padding = 'SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = weight_variable(w)\n",
        "        b = bias_variable(b)\n",
        "        conv_and_biased = tf.nn.conv2d(x, w, strides = strides, padding = padding, name = name) + b\n",
        "    return conv_and_biased\n",
        "def batch_normalization(inputs, scope, is_training=True, need_relu=True):\n",
        "    bn = tf.contrib.layers.batch_norm(inputs,\n",
        "        decay=0.999,\n",
        "        center=False, # 是否有beta\n",
        "        scale=False,  # 是否有gamma\n",
        "        epsilon=0.001,\n",
        "        activation_fn=None,\n",
        "        param_initializers=None, # beta, gamma, moving mean and moving variance的优化初始化\n",
        "        param_regularizers=None, # beta, gamma的正则化优化\n",
        "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
        "        is_training=is_training,  # 可以让学生实验True和False的区别。\n",
        "        reuse=None,\n",
        "        variables_collections=None,\n",
        "        outputs_collections=None,\n",
        "        trainable=True,\n",
        "        batch_weights=None,\n",
        "        fused=False,\n",
        "        data_format='NHWC',\n",
        "        zero_debias_moving_mean=False,\n",
        "        scope=scope,\n",
        "        renorm=False,\n",
        "        renorm_clipping=None,\n",
        "        renorm_decay=0.99)\n",
        "    if need_relu:\n",
        "        bn = tf.nn.relu(bn, name='relu')\n",
        "    return bn\n",
        "def maxpooling(x,kernal_size, strides, name):  # 最大池化，前面的是核大小，一般为[1, 2, 2, 1]，后面的strides指的是步长，如[1, 2, 2, 1]。\n",
        "    return tf.nn.max_pool(x, ksize=kernal_size, strides=strides, padding='SAME',name = name)\n",
        "def avg_pool(input_feats, k):\n",
        "    ksize = [1, k, k, 1]\n",
        "    strides = [1, k, k, 1]\n",
        "    padding = 'VALID'\n",
        "    output = tf.nn.avg_pool(input_feats, ksize, strides, padding)  # 平均池化\n",
        "    return output\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, train_flag,stride2=False):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters  # nb_filter1/2/3分别是64/64/256，三个整数。\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'  # 'res2a_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'  # 'bn2a_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    if stride2==False:\n",
        "        stride_for_first_conv = [1, 1, 1, 1]  # 。\n",
        "    else:\n",
        "        stride_for_first_conv = [1, 2, 2, 1]\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    shortcut = conv_layer(input_tensor, [1, 1, input_layers, nb_filter3], [nb_filter3], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '1')\n",
        "    shortcut = batch_normalization(shortcut, scope=bn_name_base + '1', is_training=train_flag)\n",
        "    x = tf.add(x, shortcut)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block, train_flag):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x =  batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    x = tf.add(x, input_tensor)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def resnet_graph(input_image, architecture,train_flag=True, stage5=False,ttf=True):  # 残差网络函数\n",
        "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
        "    # Stage 1\n",
        "    paddings = tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])  # 上下左右各补3个0。\n",
        "    x = tf.pad(input_image, paddings, \"CONSTANT\")\n",
        "    w = weight_variable([7, 7, 3, 64])\n",
        "    b = bias_variable([64])  #\n",
        "    x = tf.nn.conv2d(x, w, strides = [1, 2, 2, 1], padding = 'VALID', name = 'conv_1') + b\n",
        "    x = batch_normalization(x, scope='bn_conv1', is_training=train_flag, need_relu=True)\n",
        "    C1 = x = maxpooling(x, [1, 3, 3, 1], [1, 2, 2, 1], name='stage1')\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', train_flag=train_flag)  # 结构块。\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_flag=train_flag)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_flag=train_flag, stride2=True)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_flag=train_flag)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_flag=train_flag)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_flag=train_flag, stride2=True)\n",
        "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  # block_count=22，即，下句for循环的range是0~22，正好是加23个层。\n",
        "    for i in range(block_count):  # 加22个层，都是用identity_block函数加。\n",
        "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_flag=train_flag)\n",
        "    C4 = x  #\n",
        "    # Stage 5\n",
        "    if stage5:\n",
        "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_flag=train_flag, stride2=True)\n",
        "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_flag=train_flag)\n",
        "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_flag=train_flag)\n",
        "        output=avg_pool(C5,1)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)#加入dropout\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "    else:\n",
        "        C5 = None\n",
        "        output=avg_pool(C4,2)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "\n",
        "    return [C1, C2, C3, C4, C5,output]\n",
        "\n",
        "\n",
        "\n",
        "filepath='C:/Users/zy109/Desktop/1/data/cifar-10-batches-py'\n",
        "model_save_path='C:/Users/zy109/Desktop/1/cifar10'\n",
        "data=Cifar10DataReader()\n",
        "sess=tf.InteractiveSession()\n",
        "\n",
        "#指数衰减学习率\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(learning_rate=0.001, global_step=global_step, decay_steps=50, decay_rate=0.99, staircase=False)\n",
        "\n",
        "#占位符\n",
        "input_image=tf.placeholder(tf.float32,[None,32,32,3])\n",
        "y_=tf.placeholder(tf.float32,[None,10])\n",
        "tf_is_train=tf.placeholder(tf.bool,None)\n",
        "input_image=data_enhace(input_image)#数据增强\n",
        "input_image,y_=criterion(input_image,y_,batch_size)#mixup\n",
        "\n",
        "[C1,C2,C3,C4,C5,output]=resnet_graph(input_image,'resnet50',ttf=tf_is_train)\n",
        "\n",
        "#交叉熵损失\n",
        "cross_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_,logits=output))\n",
        "\n",
        "tf.add_to_collection('losses',cross_loss)\n",
        "loss=tf.add_n(tf.get_collection('losses'))\n",
        "train_step=tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "\n",
        "#正确率\n",
        "correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y_,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "sess.run(tf.initialize_all_variables())\n",
        "saver = tf.train.Saver()\n",
        "#读取已经训练好的权重\n",
        "ckpt = tf.train.get_checkpoint_state(model_save_path)\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "#加载测试集数据\n",
        "test_data=data.all_test_data()\n",
        "x_test=np.array(test_data[0])/255\n",
        "y_test=np.array(test_data[1])\n",
        "\n",
        "total_loss=[]\n",
        "total_acc=[]\n",
        "total_step=[]\n",
        "\n",
        "\n",
        "# 训练\n",
        "for i in range(20000):\n",
        "\n",
        "    #逐批加载训练集数据\n",
        "    train_data=data.next_train_data(batch_size)\n",
        "    x_train=np.array(train_data[0])/255\n",
        "    y_train=np.array(train_data[1])\n",
        "\n",
        "    train_step.run(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        # 测试准确率\n",
        "        train_accuracy = accuracy.eval(feed_dict={input_image: x_train,y_: y_train,tf_is_train:False})\n",
        "        test_accuracy = accuracy.eval(feed_dict={input_image: x_test,y_: y_test,tf_is_train:False})\n",
        "        # 该次训练的损失\n",
        "        loss_value = loss.eval(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "        global_var=sess.run(global_step)\n",
        "        lr_val=sess.run(lr)\n",
        "        print(\"step %d, trainning accuracy， %g , testing accuracy， %g , loss %g  ,lr %g\" % (global_var, train_accuracy,test_accuracy, loss_value,lr_val))\n",
        "        total_step.append(i)\n",
        "        total_loss.append(loss_value)\n",
        "        total_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "save_path = saver.save(sess,\"./cifar10/model.ckpt\")\n",
        "print(\"save model:{0} Finished\".format(save_path))\n",
        "np.save(\"cifar10_step\",total_step)\n",
        "np.save(\"cifar10_loss\",total_loss)\n",
        "np.save(\"cifar10_acc\",total_acc)\n",
        "\n",
        "#测试\n",
        "y_log_predict=output.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "test_accuracy = accuracy.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "print(\"test accuracy %g\" % test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test.argmax(axis=1), y_log_predict.argmax(axis=1))\n",
        "\n",
        "\n",
        "for i in range(len(y_log_predict)):\n",
        "        max_value=max(y_log_predict[i])\n",
        "        for j in range(len(y_log_predict[i])):\n",
        "            if max_value==y_log_predict[i][j]:\n",
        "                y_log_predict[i][j]=1\n",
        "            else:\n",
        "                y_log_predict[i][j]=0\n",
        "\n",
        "# 精确率\n",
        "from sklearn.metrics import precision_score\n",
        "ps=precision_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_precision_score\",ps)\n",
        "\n",
        "#召回率\n",
        "from sklearn.metrics import recall_score\n",
        "rs=recall_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_recall_score\",rs)\n",
        "\n",
        "print(\"precision score:\",ps)\n",
        "print(\"recall score:\",rs)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "ax1.plot(total_step, total_loss, color=\"blue\", label=\"loss\")\n",
        "ax1.set_xlabel(\"step\")\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_step, total_acc, color=\"red\", label=\"accuary\")\n",
        "ax2.set_ylabel(\"accuary\")\n",
        "\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:650: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-1caaa8e6913e>:249: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-1caaa8e6913e>:280: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "step 1, trainning accuracy， 0.148438 , testing accuracy， 0.1213 , loss 81.5084  ,lr 0.000999799\n",
            "step 101, trainning accuracy， 0.320312 , testing accuracy， 0.3145 , loss 52.5669  ,lr 0.000979903\n",
            "step 201, trainning accuracy， 0.386719 , testing accuracy， 0.3973 , loss 31.415  ,lr 0.000960403\n",
            "step 301, trainning accuracy， 0.460938 , testing accuracy， 0.4325 , loss 20.3426  ,lr 0.000941291\n",
            "step 401, trainning accuracy， 0.410156 , testing accuracy， 0.4502 , loss 14.3321  ,lr 0.000922559\n",
            "step 501, trainning accuracy， 0.4375 , testing accuracy， 0.4764 , loss 10.5815  ,lr 0.0009042\n",
            "step 601, trainning accuracy， 0.53125 , testing accuracy， 0.4983 , loss 8.18099  ,lr 0.000886207\n",
            "step 701, trainning accuracy， 0.578125 , testing accuracy， 0.5217 , loss 6.94147  ,lr 0.000868571\n",
            "step 801, trainning accuracy， 0.558594 , testing accuracy， 0.5568 , loss 5.65793  ,lr 0.000851287\n",
            "step 901, trainning accuracy， 0.53125 , testing accuracy， 0.5319 , loss 5.16497  ,lr 0.000834346\n",
            "step 1001, trainning accuracy， 0.566406 , testing accuracy， 0.5722 , loss 4.77618  ,lr 0.000817743\n",
            "step 1101, trainning accuracy， 0.578125 , testing accuracy， 0.5873 , loss 4.0572  ,lr 0.00080147\n",
            "step 1201, trainning accuracy， 0.652344 , testing accuracy， 0.601 , loss 3.35901  ,lr 0.00078552\n",
            "step 1301, trainning accuracy， 0.613281 , testing accuracy， 0.6108 , loss 3.19034  ,lr 0.000769889\n",
            "step 1401, trainning accuracy， 0.59375 , testing accuracy， 0.5918 , loss 3.48043  ,lr 0.000754568\n",
            "step 1501, trainning accuracy， 0.671875 , testing accuracy， 0.6303 , loss 2.85421  ,lr 0.000739552\n",
            "step 1601, trainning accuracy， 0.648438 , testing accuracy， 0.6344 , loss 2.70654  ,lr 0.000724835\n",
            "step 1701, trainning accuracy， 0.648438 , testing accuracy， 0.6443 , loss 2.48458  ,lr 0.000710411\n",
            "step 1801, trainning accuracy， 0.730469 , testing accuracy， 0.6525 , loss 2.30644  ,lr 0.000696274\n",
            "step 1901, trainning accuracy， 0.703125 , testing accuracy， 0.6709 , loss 2.23888  ,lr 0.000682418\n",
            "step 2001, trainning accuracy， 0.738281 , testing accuracy， 0.6413 , loss 2.41719  ,lr 0.000668838\n",
            "step 2101, trainning accuracy， 0.746094 , testing accuracy， 0.671 , loss 2.06127  ,lr 0.000655528\n",
            "step 2201, trainning accuracy， 0.742188 , testing accuracy， 0.6887 , loss 1.95266  ,lr 0.000642483\n",
            "step 2301, trainning accuracy， 0.765625 , testing accuracy， 0.6933 , loss 2.00754  ,lr 0.000629697\n",
            "step 2401, trainning accuracy， 0.738281 , testing accuracy， 0.6787 , loss 1.80309  ,lr 0.000617166\n",
            "step 2501, trainning accuracy， 0.757812 , testing accuracy， 0.7128 , loss 1.73943  ,lr 0.000604885\n",
            "step 2601, trainning accuracy， 0.726562 , testing accuracy， 0.6826 , loss 1.85819  ,lr 0.000592848\n",
            "step 2701, trainning accuracy， 0.742188 , testing accuracy， 0.7025 , loss 1.81396  ,lr 0.00058105\n",
            "step 2801, trainning accuracy， 0.707031 , testing accuracy， 0.6825 , loss 1.93818  ,lr 0.000569487\n",
            "step 2901, trainning accuracy， 0.722656 , testing accuracy， 0.6676 , loss 2.00966  ,lr 0.000558154\n",
            "step 3001, trainning accuracy， 0.738281 , testing accuracy， 0.713 , loss 1.89364  ,lr 0.000547047\n",
            "step 3101, trainning accuracy， 0.730469 , testing accuracy， 0.7193 , loss 1.75837  ,lr 0.000536161\n",
            "step 3201, trainning accuracy， 0.742188 , testing accuracy， 0.7273 , loss 1.61576  ,lr 0.000525491\n",
            "step 3301, trainning accuracy， 0.804688 , testing accuracy， 0.7239 , loss 1.49463  ,lr 0.000515034\n",
            "step 3401, trainning accuracy， 0.851562 , testing accuracy， 0.7182 , loss 1.3655  ,lr 0.000504785\n",
            "step 3501, trainning accuracy， 0.785156 , testing accuracy， 0.723 , loss 1.50987  ,lr 0.00049474\n",
            "step 3601, trainning accuracy， 0.78125 , testing accuracy， 0.7397 , loss 1.489  ,lr 0.000484894\n",
            "step 3701, trainning accuracy， 0.796875 , testing accuracy， 0.744 , loss 1.31253  ,lr 0.000475245\n",
            "step 3801, trainning accuracy， 0.804688 , testing accuracy， 0.7343 , loss 1.65669  ,lr 0.000465788\n",
            "step 3901, trainning accuracy， 0.832031 , testing accuracy， 0.7433 , loss 1.26492  ,lr 0.000456518\n",
            "step 4001, trainning accuracy， 0.84375 , testing accuracy， 0.7424 , loss 1.19193  ,lr 0.000447434\n",
            "step 4101, trainning accuracy， 0.84375 , testing accuracy， 0.7424 , loss 1.23298  ,lr 0.00043853\n",
            "step 4201, trainning accuracy， 0.792969 , testing accuracy， 0.7288 , loss 1.44293  ,lr 0.000429803\n",
            "step 4301, trainning accuracy， 0.878906 , testing accuracy， 0.7551 , loss 1.11845  ,lr 0.00042125\n",
            "step 4401, trainning accuracy， 0.8125 , testing accuracy， 0.7427 , loss 1.23479  ,lr 0.000412867\n",
            "step 4501, trainning accuracy， 0.773438 , testing accuracy， 0.6816 , loss 1.47601  ,lr 0.000404651\n",
            "step 4601, trainning accuracy， 0.796875 , testing accuracy， 0.7456 , loss 1.27635  ,lr 0.000396598\n",
            "step 4701, trainning accuracy， 0.804688 , testing accuracy， 0.7306 , loss 1.36525  ,lr 0.000388706\n",
            "step 4801, trainning accuracy， 0.863281 , testing accuracy， 0.759 , loss 1.23414  ,lr 0.000380971\n",
            "step 4901, trainning accuracy， 0.859375 , testing accuracy， 0.7674 , loss 1.12775  ,lr 0.00037339\n",
            "step 5001, trainning accuracy， 0.851562 , testing accuracy， 0.7602 , loss 1.10937  ,lr 0.000365959\n",
            "step 5101, trainning accuracy， 0.847656 , testing accuracy， 0.7468 , loss 1.34089  ,lr 0.000358677\n",
            "step 5201, trainning accuracy， 0.867188 , testing accuracy， 0.7665 , loss 1.14901  ,lr 0.000351539\n",
            "step 5301, trainning accuracy， 0.882812 , testing accuracy， 0.7634 , loss 1.04322  ,lr 0.000344543\n",
            "step 5401, trainning accuracy， 0.875 , testing accuracy， 0.7657 , loss 1.06379  ,lr 0.000337687\n",
            "step 5501, trainning accuracy， 0.867188 , testing accuracy， 0.7673 , loss 1.20412  ,lr 0.000330967\n",
            "step 5601, trainning accuracy， 0.878906 , testing accuracy， 0.773 , loss 1.17777  ,lr 0.000324381\n",
            "step 5701, trainning accuracy， 0.882812 , testing accuracy， 0.7726 , loss 0.943234  ,lr 0.000317926\n",
            "step 5801, trainning accuracy， 0.882812 , testing accuracy， 0.7727 , loss 1.00191  ,lr 0.000311599\n",
            "step 5901, trainning accuracy， 0.914062 , testing accuracy， 0.7691 , loss 0.881819  ,lr 0.000305398\n",
            "step 6001, trainning accuracy， 0.921875 , testing accuracy， 0.7741 , loss 0.885102  ,lr 0.000299321\n",
            "step 6101, trainning accuracy， 0.921875 , testing accuracy， 0.7693 , loss 0.846919  ,lr 0.000293364\n",
            "step 6201, trainning accuracy， 0.851562 , testing accuracy， 0.7708 , loss 1.06445  ,lr 0.000287526\n",
            "step 6301, trainning accuracy， 0.902344 , testing accuracy， 0.7685 , loss 1.03371  ,lr 0.000281804\n",
            "step 6401, trainning accuracy， 0.882812 , testing accuracy， 0.7757 , loss 0.932706  ,lr 0.000276196\n",
            "step 6501, trainning accuracy， 0.914062 , testing accuracy， 0.7774 , loss 0.853486  ,lr 0.0002707\n",
            "step 6601, trainning accuracy， 0.914062 , testing accuracy， 0.7788 , loss 0.852331  ,lr 0.000265313\n",
            "step 6701, trainning accuracy， 0.917969 , testing accuracy， 0.7781 , loss 0.857215  ,lr 0.000260034\n",
            "step 6801, trainning accuracy， 0.902344 , testing accuracy， 0.7637 , loss 0.983753  ,lr 0.000254859\n",
            "step 6901, trainning accuracy， 0.824219 , testing accuracy， 0.7047 , loss 1.2121  ,lr 0.000249787\n",
            "step 7001, trainning accuracy， 0.863281 , testing accuracy， 0.7828 , loss 1.0379  ,lr 0.000244816\n",
            "step 7101, trainning accuracy， 0.914062 , testing accuracy， 0.7771 , loss 1.11032  ,lr 0.000239945\n",
            "step 7201, trainning accuracy， 0.921875 , testing accuracy， 0.7855 , loss 0.788775  ,lr 0.00023517\n",
            "step 7301, trainning accuracy， 0.933594 , testing accuracy， 0.7781 , loss 0.8369  ,lr 0.00023049\n",
            "step 7401, trainning accuracy， 0.902344 , testing accuracy， 0.7891 , loss 0.911116  ,lr 0.000225903\n",
            "step 7501, trainning accuracy， 0.902344 , testing accuracy， 0.776 , loss 0.895519  ,lr 0.000221408\n",
            "step 7601, trainning accuracy， 0.886719 , testing accuracy， 0.7816 , loss 0.894534  ,lr 0.000217002\n",
            "step 7701, trainning accuracy， 0.984375 , testing accuracy， 0.785 , loss 0.659823  ,lr 0.000212683\n",
            "step 7801, trainning accuracy， 0.976562 , testing accuracy， 0.7852 , loss 0.683494  ,lr 0.000208451\n",
            "step 7901, trainning accuracy， 0.972656 , testing accuracy， 0.7838 , loss 0.637636  ,lr 0.000204303\n",
            "step 8001, trainning accuracy， 0.96875 , testing accuracy， 0.787 , loss 0.647113  ,lr 0.000200237\n",
            "step 8101, trainning accuracy， 0.945312 , testing accuracy， 0.7797 , loss 0.664609  ,lr 0.000196252\n",
            "step 8201, trainning accuracy， 0.953125 , testing accuracy， 0.7799 , loss 0.692155  ,lr 0.000192347\n",
            "step 8301, trainning accuracy， 0.972656 , testing accuracy， 0.7836 , loss 0.66627  ,lr 0.000188519\n",
            "step 8401, trainning accuracy， 0.976562 , testing accuracy， 0.7858 , loss 0.619796  ,lr 0.000184768\n",
            "step 8501, trainning accuracy， 0.957031 , testing accuracy， 0.7862 , loss 0.686526  ,lr 0.000181091\n",
            "step 8601, trainning accuracy， 0.972656 , testing accuracy， 0.785 , loss 0.589901  ,lr 0.000177487\n",
            "step 8701, trainning accuracy， 0.964844 , testing accuracy， 0.7821 , loss 0.595202  ,lr 0.000173955\n",
            "step 8801, trainning accuracy， 0.976562 , testing accuracy， 0.785 , loss 0.562216  ,lr 0.000170493\n",
            "step 8901, trainning accuracy， 0.96875 , testing accuracy， 0.7802 , loss 0.60632  ,lr 0.000167101\n",
            "step 9001, trainning accuracy， 0.972656 , testing accuracy， 0.7779 , loss 0.667105  ,lr 0.000163775\n",
            "step 9101, trainning accuracy， 0.945312 , testing accuracy， 0.7763 , loss 0.704555  ,lr 0.000160516\n",
            "step 9201, trainning accuracy， 0.976562 , testing accuracy， 0.7815 , loss 0.691964  ,lr 0.000157322\n",
            "step 9301, trainning accuracy， 0.972656 , testing accuracy， 0.7883 , loss 0.584242  ,lr 0.000154191\n",
            "step 9401, trainning accuracy， 0.988281 , testing accuracy， 0.7847 , loss 0.526358  ,lr 0.000151123\n",
            "step 9501, trainning accuracy， 0.984375 , testing accuracy， 0.7883 , loss 0.604475  ,lr 0.000148115\n",
            "step 9601, trainning accuracy， 0.871094 , testing accuracy， 0.7303 , loss 1.09526  ,lr 0.000145168\n",
            "step 9701, trainning accuracy， 0.9375 , testing accuracy， 0.779 , loss 0.686103  ,lr 0.000142279\n",
            "step 9801, trainning accuracy， 0.945312 , testing accuracy， 0.7867 , loss 0.747175  ,lr 0.000139448\n",
            "step 9901, trainning accuracy， 0.972656 , testing accuracy， 0.7831 , loss 0.627028  ,lr 0.000136673\n",
            "step 10001, trainning accuracy， 0.996094 , testing accuracy， 0.7923 , loss 0.544611  ,lr 0.000133953\n",
            "step 10101, trainning accuracy， 0.980469 , testing accuracy， 0.7881 , loss 0.58485  ,lr 0.000131287\n",
            "step 10201, trainning accuracy， 0.976562 , testing accuracy， 0.7884 , loss 0.536347  ,lr 0.000128675\n",
            "step 10301, trainning accuracy， 0.964844 , testing accuracy， 0.7828 , loss 0.588628  ,lr 0.000126114\n",
            "step 10401, trainning accuracy， 0.984375 , testing accuracy， 0.7842 , loss 0.549407  ,lr 0.000123604\n",
            "step 10501, trainning accuracy， 0.984375 , testing accuracy， 0.7896 , loss 0.546582  ,lr 0.000121145\n",
            "step 10601, trainning accuracy， 0.992188 , testing accuracy， 0.7833 , loss 0.50584  ,lr 0.000118734\n",
            "step 10701, trainning accuracy， 0.988281 , testing accuracy， 0.7884 , loss 0.50031  ,lr 0.000116371\n",
            "step 10801, trainning accuracy， 0.992188 , testing accuracy， 0.79 , loss 0.509335  ,lr 0.000114055\n",
            "step 10901, trainning accuracy， 0.992188 , testing accuracy， 0.7814 , loss 0.50809  ,lr 0.000111786\n",
            "step 11001, trainning accuracy， 0.996094 , testing accuracy， 0.7874 , loss 0.504429  ,lr 0.000109561\n",
            "step 11101, trainning accuracy， 0.996094 , testing accuracy， 0.7884 , loss 0.479124  ,lr 0.000107381\n",
            "step 11201, trainning accuracy， 0.996094 , testing accuracy， 0.7839 , loss 0.518002  ,lr 0.000105244\n",
            "step 11301, trainning accuracy， 0.980469 , testing accuracy， 0.7862 , loss 0.47505  ,lr 0.00010315\n",
            "step 11401, trainning accuracy， 0.996094 , testing accuracy， 0.7867 , loss 0.486448  ,lr 0.000101097\n",
            "step 11501, trainning accuracy， 0.996094 , testing accuracy， 0.7895 , loss 0.460719  ,lr 9.90851e-05\n",
            "step 11601, trainning accuracy， 0.988281 , testing accuracy， 0.7846 , loss 0.473907  ,lr 9.71133e-05\n",
            "step 11701, trainning accuracy， 0.996094 , testing accuracy， 0.788 , loss 0.429078  ,lr 9.51808e-05\n",
            "step 11801, trainning accuracy， 1 , testing accuracy， 0.788 , loss 0.4248  ,lr 9.32867e-05\n",
            "step 11901, trainning accuracy， 0.996094 , testing accuracy， 0.7871 , loss 0.438153  ,lr 9.14303e-05\n",
            "step 12001, trainning accuracy， 0.996094 , testing accuracy， 0.7868 , loss 0.434369  ,lr 8.96108e-05\n",
            "step 12101, trainning accuracy， 1 , testing accuracy， 0.7847 , loss 0.410574  ,lr 8.78276e-05\n",
            "step 12201, trainning accuracy， 0.996094 , testing accuracy， 0.7896 , loss 0.431277  ,lr 8.60798e-05\n",
            "step 12301, trainning accuracy， 1 , testing accuracy， 0.7834 , loss 0.422838  ,lr 8.43668e-05\n",
            "step 12401, trainning accuracy， 0.992188 , testing accuracy， 0.7869 , loss 0.403676  ,lr 8.26879e-05\n",
            "step 12501, trainning accuracy， 0.992188 , testing accuracy， 0.7884 , loss 0.39918  ,lr 8.10424e-05\n",
            "step 12601, trainning accuracy， 1 , testing accuracy， 0.7888 , loss 0.398704  ,lr 7.94297e-05\n",
            "step 12701, trainning accuracy， 1 , testing accuracy， 0.787 , loss 0.394594  ,lr 7.7849e-05\n",
            "step 12801, trainning accuracy， 0.996094 , testing accuracy， 0.7874 , loss 0.397156  ,lr 7.62998e-05\n",
            "step 12901, trainning accuracy， 0.996094 , testing accuracy， 0.7871 , loss 0.402707  ,lr 7.47815e-05\n",
            "step 13001, trainning accuracy， 0.992188 , testing accuracy， 0.7882 , loss 0.433849  ,lr 7.32933e-05\n",
            "step 13101, trainning accuracy， 0.996094 , testing accuracy， 0.7895 , loss 0.399802  ,lr 7.18348e-05\n",
            "step 13201, trainning accuracy， 0.996094 , testing accuracy， 0.7866 , loss 0.383345  ,lr 7.04053e-05\n",
            "step 13301, trainning accuracy， 1 , testing accuracy， 0.7865 , loss 0.388467  ,lr 6.90042e-05\n",
            "step 13401, trainning accuracy， 0.996094 , testing accuracy， 0.7889 , loss 0.428122  ,lr 6.7631e-05\n",
            "step 13501, trainning accuracy， 1 , testing accuracy， 0.7883 , loss 0.353016  ,lr 6.62852e-05\n",
            "step 13601, trainning accuracy， 0.992188 , testing accuracy， 0.7867 , loss 0.392766  ,lr 6.49661e-05\n",
            "step 13701, trainning accuracy， 1 , testing accuracy， 0.7855 , loss 0.350002  ,lr 6.36733e-05\n",
            "step 13801, trainning accuracy， 0.996094 , testing accuracy， 0.7884 , loss 0.398683  ,lr 6.24062e-05\n",
            "step 13901, trainning accuracy， 1 , testing accuracy， 0.7896 , loss 0.366252  ,lr 6.11643e-05\n",
            "step 14001, trainning accuracy， 1 , testing accuracy， 0.7873 , loss 0.353613  ,lr 5.99471e-05\n",
            "step 14101, trainning accuracy， 1 , testing accuracy， 0.7859 , loss 0.342446  ,lr 5.87542e-05\n",
            "step 14201, trainning accuracy， 0.996094 , testing accuracy， 0.7864 , loss 0.352469  ,lr 5.7585e-05\n",
            "step 14301, trainning accuracy， 1 , testing accuracy， 0.785 , loss 0.366518  ,lr 5.6439e-05\n",
            "step 14401, trainning accuracy， 1 , testing accuracy， 0.789 , loss 0.34552  ,lr 5.53159e-05\n",
            "step 14501, trainning accuracy， 1 , testing accuracy， 0.7858 , loss 0.377703  ,lr 5.42151e-05\n",
            "step 14601, trainning accuracy， 0.996094 , testing accuracy， 0.7872 , loss 0.351099  ,lr 5.31362e-05\n",
            "step 14701, trainning accuracy， 1 , testing accuracy， 0.7853 , loss 0.346756  ,lr 5.20788e-05\n",
            "step 14801, trainning accuracy， 0.996094 , testing accuracy， 0.7854 , loss 0.413202  ,lr 5.10425e-05\n",
            "step 14901, trainning accuracy， 1 , testing accuracy， 0.7885 , loss 0.338532  ,lr 5.00267e-05\n",
            "step 15001, trainning accuracy， 1 , testing accuracy， 0.7922 , loss 0.353182  ,lr 4.90312e-05\n",
            "step 15101, trainning accuracy， 0.996094 , testing accuracy， 0.7957 , loss 0.36143  ,lr 4.80555e-05\n",
            "step 15201, trainning accuracy， 1 , testing accuracy， 0.7916 , loss 0.359139  ,lr 4.70992e-05\n",
            "step 15301, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.349256  ,lr 4.61619e-05\n",
            "step 15401, trainning accuracy， 1 , testing accuracy， 0.7874 , loss 0.343677  ,lr 4.52433e-05\n",
            "step 15501, trainning accuracy， 0.996094 , testing accuracy， 0.7911 , loss 0.346533  ,lr 4.43429e-05\n",
            "step 15601, trainning accuracy， 1 , testing accuracy， 0.7898 , loss 0.333896  ,lr 4.34605e-05\n",
            "step 15701, trainning accuracy， 1 , testing accuracy， 0.7928 , loss 0.330734  ,lr 4.25956e-05\n",
            "step 15801, trainning accuracy， 1 , testing accuracy， 0.7933 , loss 0.336271  ,lr 4.1748e-05\n",
            "step 15901, trainning accuracy， 1 , testing accuracy， 0.7897 , loss 0.325456  ,lr 4.09172e-05\n",
            "step 16001, trainning accuracy， 1 , testing accuracy， 0.7909 , loss 0.33552  ,lr 4.0103e-05\n",
            "step 16101, trainning accuracy， 1 , testing accuracy， 0.7929 , loss 0.324846  ,lr 3.93049e-05\n",
            "step 16201, trainning accuracy， 1 , testing accuracy， 0.7892 , loss 0.315304  ,lr 3.85227e-05\n",
            "step 16301, trainning accuracy， 1 , testing accuracy， 0.7901 , loss 0.3235  ,lr 3.77561e-05\n",
            "step 16401, trainning accuracy， 1 , testing accuracy， 0.7905 , loss 0.362555  ,lr 3.70048e-05\n",
            "step 16501, trainning accuracy， 1 , testing accuracy， 0.7909 , loss 0.341618  ,lr 3.62684e-05\n",
            "step 16601, trainning accuracy， 1 , testing accuracy， 0.7944 , loss 0.316581  ,lr 3.55467e-05\n",
            "step 16701, trainning accuracy， 1 , testing accuracy， 0.796 , loss 0.308296  ,lr 3.48393e-05\n",
            "step 16801, trainning accuracy， 0.992188 , testing accuracy， 0.7851 , loss 0.403913  ,lr 3.4146e-05\n",
            "step 16901, trainning accuracy， 1 , testing accuracy， 0.7894 , loss 0.331084  ,lr 3.34665e-05\n",
            "step 17001, trainning accuracy， 1 , testing accuracy， 0.7926 , loss 0.335094  ,lr 3.28005e-05\n",
            "step 17101, trainning accuracy， 1 , testing accuracy， 0.791 , loss 0.337065  ,lr 3.21478e-05\n",
            "step 17201, trainning accuracy， 1 , testing accuracy， 0.7926 , loss 0.324512  ,lr 3.1508e-05\n",
            "step 17301, trainning accuracy， 1 , testing accuracy， 0.7914 , loss 0.318384  ,lr 3.0881e-05\n",
            "step 17401, trainning accuracy， 1 , testing accuracy， 0.794 , loss 0.326394  ,lr 3.02665e-05\n",
            "step 17501, trainning accuracy， 1 , testing accuracy， 0.791 , loss 0.313154  ,lr 2.96642e-05\n",
            "step 17601, trainning accuracy， 1 , testing accuracy， 0.7947 , loss 0.315698  ,lr 2.90739e-05\n",
            "step 17701, trainning accuracy， 0.996094 , testing accuracy， 0.79 , loss 0.312679  ,lr 2.84953e-05\n",
            "step 17801, trainning accuracy， 1 , testing accuracy， 0.793 , loss 0.319777  ,lr 2.79282e-05\n",
            "step 17901, trainning accuracy， 1 , testing accuracy， 0.7904 , loss 0.312799  ,lr 2.73725e-05\n",
            "step 18001, trainning accuracy， 1 , testing accuracy， 0.7916 , loss 0.304654  ,lr 2.68278e-05\n",
            "step 18101, trainning accuracy， 1 , testing accuracy， 0.79 , loss 0.305155  ,lr 2.62939e-05\n",
            "step 18201, trainning accuracy， 1 , testing accuracy， 0.7939 , loss 0.325889  ,lr 2.57706e-05\n",
            "step 18301, trainning accuracy， 1 , testing accuracy， 0.7955 , loss 0.322133  ,lr 2.52578e-05\n",
            "step 18401, trainning accuracy， 0.996094 , testing accuracy， 0.7931 , loss 0.304535  ,lr 2.47552e-05\n",
            "step 18501, trainning accuracy， 1 , testing accuracy， 0.7897 , loss 0.303509  ,lr 2.42625e-05\n",
            "step 18601, trainning accuracy， 1 , testing accuracy， 0.7952 , loss 0.316844  ,lr 2.37797e-05\n",
            "step 18701, trainning accuracy， 1 , testing accuracy， 0.7954 , loss 0.297327  ,lr 2.33065e-05\n",
            "step 18801, trainning accuracy， 1 , testing accuracy， 0.7925 , loss 0.316458  ,lr 2.28427e-05\n",
            "step 18901, trainning accuracy， 1 , testing accuracy， 0.7929 , loss 0.297396  ,lr 2.23881e-05\n",
            "step 19001, trainning accuracy， 1 , testing accuracy， 0.7935 , loss 0.303131  ,lr 2.19426e-05\n",
            "step 19101, trainning accuracy， 1 , testing accuracy， 0.7942 , loss 0.300411  ,lr 2.1506e-05\n",
            "step 19201, trainning accuracy， 1 , testing accuracy， 0.7928 , loss 0.296462  ,lr 2.1078e-05\n",
            "step 19301, trainning accuracy， 1 , testing accuracy， 0.7934 , loss 0.295759  ,lr 2.06585e-05\n",
            "step 19401, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.325644  ,lr 2.02474e-05\n",
            "step 19501, trainning accuracy， 1 , testing accuracy， 0.793 , loss 0.301775  ,lr 1.98445e-05\n",
            "step 19601, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.289978  ,lr 1.94496e-05\n",
            "step 19701, trainning accuracy， 1 , testing accuracy， 0.7931 , loss 0.298815  ,lr 1.90626e-05\n",
            "step 19801, trainning accuracy， 1 , testing accuracy， 0.7929 , loss 0.294335  ,lr 1.86832e-05\n",
            "step 19901, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.294739  ,lr 1.83114e-05\n",
            "save model:./cifar10/model.ckpt Finished\n",
            "test accuracy 0.7891\n",
            "precision score: [0.82533197 0.89370485 0.73360242 0.63241525 0.76447106 0.6851312\n",
            " 0.81268293 0.83134921 0.8471575  0.85918367]\n",
            "recall score: [0.808 0.866 0.727 0.597 0.766 0.705 0.833 0.838 0.909 0.842]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LyTyOT0-tokE",
        "outputId": "14dcd8cb-b6dd-4baa-9c2d-b9799dd5a1e6"
      },
      "source": [
        "# zero_debias_moving_mean=True, 17min\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.layers as layers\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "#随机旋转，裁切等数据增强\n",
        "def data_enhace(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_hue(x, max_delta=0.2)\n",
        "    x = tf.image.random_brightness(x, max_delta=0.7)\n",
        "    result = tf.image.random_contrast(x, lower=0.2, upper=1.8)\n",
        "    return result\n",
        "\n",
        "#mixup数据增强\n",
        "def criterion(batch_x, batch_y, batch_size,alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    index = tf.random_shuffle(batch_size)\n",
        "    mixed_batchx = lam * batch_x + (1 - lam) * batch_x[index, :]\n",
        "\n",
        "    batch_y_= lam*batch_y+(1-lam)*batch_y[index]\n",
        "    return mixed_batchx,batch_y_\n",
        "\n",
        "#本地载入cifar10数据集\n",
        "def load_cifar10_batch(cifar10_dataset_folder_path,batch_id):\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id),mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding = 'latin1')\n",
        "\n",
        "    # features and labels\n",
        "    features = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return  features, labels\n",
        "\n",
        "#在线载入cifar10数据集\n",
        "def load_cifar10_batch_online():\n",
        "    (x_Train, y_Train), (x_Test, y_Test)=cifar10.load_data()\n",
        "    return x_Train,y_Train,x_Test,y_Test\n",
        "\n",
        "#cifar10数据集读取器\n",
        "class Cifar10DataReader():\n",
        "    def __init__(self, cifar_folder=None, onehot=True):\n",
        "        self.cifar_folder = cifar_folder\n",
        "        self.onehot = onehot\n",
        "        self.data_label_train = None\n",
        "        self.data_label_test = None\n",
        "        self.batch_index = 0\n",
        "\n",
        "        if cifar_folder==None:#在线\n",
        "            x_train, y_train,x_test,y_test=load_cifar10_batch_online()\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            self.data_label_test = list(zip(x_test, y_test))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "        else:#本地\n",
        "            x_train, y_train = load_cifar10_batch(self.cifar_folder, 1)\n",
        "            for i in range(2,6):\n",
        "                features,labels = load_cifar10_batch(self.cifar_folder, i)\n",
        "                x_train, y_train = np.concatenate([x_train, features]),np.concatenate([y_train, labels])\n",
        "            self.data_label_train = list(zip(x_train, y_train))\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "    #批次读取训练集数据\n",
        "    def next_train_data(self, batch_size=100):\n",
        "        if self.batch_index < len(self.data_label_train) // batch_size:\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "            np.random.shuffle(self.data_label_train)\n",
        "            datum = self.data_label_train[self.batch_index * batch_size:(self.batch_index + 1) * batch_size]\n",
        "            self.batch_index += 1\n",
        "            return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有训练集数据\n",
        "    def all_train_data(self):\n",
        "        np.random.shuffle(self.data_label_train)\n",
        "        datum = self.data_label_train\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #独热编码\n",
        "    def _decode(self, datum, onehot):\n",
        "        rdata = list()\n",
        "        rlabel = list()\n",
        "        if onehot:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                hot = np.zeros(10)\n",
        "                hot[int(l)] = 1\n",
        "                rlabel.append(hot)\n",
        "        else:\n",
        "            for d, l in datum:\n",
        "                rdata.append(d)\n",
        "                rlabel.append(int(l))\n",
        "        return rdata, rlabel\n",
        "\n",
        "    #批次读取测试集数据\n",
        "    def next_test_data(self, batch_size=100):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test[0:batch_size]\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "    #读取所有测试集数据\n",
        "    def all_test_data(self):\n",
        "        if self.data_label_test is None:\n",
        "            with open(self.cifar_folder + '/test_batch', mode = 'rb') as file:\n",
        "                batch = pickle.load(file, encoding='latin1')\n",
        "                data = batch['data'].reshape((len(batch['data']),3,32,32)).transpose(0,2,3,1)\n",
        "                labels = batch['labels']\n",
        "            self.data_label_test = list(zip(data, labels))\n",
        "\n",
        "        np.random.shuffle(self.data_label_test)\n",
        "        datum = self.data_label_test\n",
        "\n",
        "        return self._decode(datum, self.onehot)\n",
        "\n",
        "\n",
        "#网络结构\n",
        "def weight_variable(shape):\n",
        "    weights = tf.get_variable(\"weights\", shape, initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    #加入l2正则化\n",
        "    regular=tf.multiply(tf.nn.l2_loss(weights),0.005,name='regular')\n",
        "    tf.add_to_collection('losses',regular)\n",
        "    return weights\n",
        "def bias_variable(shape):\n",
        "    biases = tf.get_variable(\"biases\", shape, initializer=tf.constant_initializer(0.0))\n",
        "    return biases\n",
        "def conv_layer(x, w, b, name, strides, padding = 'SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = weight_variable(w)\n",
        "        b = bias_variable(b)\n",
        "        conv_and_biased = tf.nn.conv2d(x, w, strides = strides, padding = padding, name = name) + b\n",
        "    return conv_and_biased\n",
        "def batch_normalization(inputs, scope, is_training=True, need_relu=True):\n",
        "    bn = tf.contrib.layers.batch_norm(inputs,\n",
        "        decay=0.999,\n",
        "        center=True, # 是否有beta\n",
        "        scale=True,  # 是否有gamma\n",
        "        epsilon=0.001,\n",
        "        activation_fn=None,\n",
        "        param_initializers=None, # beta, gamma, moving mean and moving variance的优化初始化\n",
        "        param_regularizers=None, # beta, gamma的正则化优化\n",
        "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
        "        is_training=is_training,  # 可以让学生实验True和False的区别。\n",
        "        reuse=None,\n",
        "        variables_collections=None,\n",
        "        outputs_collections=None,\n",
        "        trainable=True,\n",
        "        batch_weights=None,\n",
        "        fused=False,\n",
        "        data_format='NHWC',\n",
        "        zero_debias_moving_mean=True,\n",
        "        scope=scope,\n",
        "        renorm=False,\n",
        "        renorm_clipping=None,\n",
        "        renorm_decay=0.99)\n",
        "    if need_relu:\n",
        "        bn = tf.nn.relu(bn, name='relu')\n",
        "    return bn\n",
        "def maxpooling(x,kernal_size, strides, name):  # 最大池化，前面的是核大小，一般为[1, 2, 2, 1]，后面的strides指的是步长，如[1, 2, 2, 1]。\n",
        "    return tf.nn.max_pool(x, ksize=kernal_size, strides=strides, padding='SAME',name = name)\n",
        "def avg_pool(input_feats, k):\n",
        "    ksize = [1, k, k, 1]\n",
        "    strides = [1, k, k, 1]\n",
        "    padding = 'VALID'\n",
        "    output = tf.nn.avg_pool(input_feats, ksize, strides, padding)  # 平均池化\n",
        "    return output\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, train_flag,stride2=False):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters  # nb_filter1/2/3分别是64/64/256，三个整数。\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'  # 'res2a_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'  # 'bn2a_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    if stride2==False:\n",
        "        stride_for_first_conv = [1, 1, 1, 1]  # 。\n",
        "    else:\n",
        "        stride_for_first_conv = [1, 2, 2, 1]\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    shortcut = conv_layer(input_tensor, [1, 1, input_layers, nb_filter3], [nb_filter3], strides=stride_for_first_conv, padding='SAME', name=conv_name_base + '1')\n",
        "    shortcut = batch_normalization(shortcut, scope=bn_name_base + '1', is_training=train_flag)\n",
        "    x = tf.add(x, shortcut)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block, train_flag):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    _, _, _, input_layers = input_tensor.get_shape().as_list()\n",
        "    x = conv_layer(input_tensor, [1, 1, input_layers, nb_filter1], [nb_filter1], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2a')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2a', is_training=train_flag)\n",
        "    x = conv_layer(x, [kernel_size, kernel_size, nb_filter1, nb_filter2], [nb_filter2], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2b')\n",
        "    x =  batch_normalization(x, scope=bn_name_base + '2b', is_training=train_flag)\n",
        "    x = conv_layer(x, [1, 1, nb_filter2, nb_filter3], [nb_filter3], strides=[1, 1, 1, 1], padding='SAME', name=conv_name_base + '2c')\n",
        "    x = batch_normalization(x, scope=bn_name_base + '2c', is_training=train_flag, need_relu=False)\n",
        "    x = tf.add(x, input_tensor)\n",
        "    x = tf.nn.relu(x, name='res' + str(stage) + block + '_out')\n",
        "    #x = layers.dropout(x, rate=0.4, training=train_flag)\n",
        "    return x\n",
        "def resnet_graph(input_image, architecture,train_flag=True, stage5=False,ttf=True):  # 残差网络函数\n",
        "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
        "    # Stage 1\n",
        "    paddings = tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])  # 上下左右各补3个0。\n",
        "    x = tf.pad(input_image, paddings, \"CONSTANT\")\n",
        "    w = weight_variable([7, 7, 3, 64])\n",
        "    b = bias_variable([64])  #\n",
        "    x = tf.nn.conv2d(x, w, strides = [1, 2, 2, 1], padding = 'VALID', name = 'conv_1') + b\n",
        "    x = batch_normalization(x, scope='bn_conv1', is_training=train_flag, need_relu=True)\n",
        "    C1 = x = maxpooling(x, [1, 3, 3, 1], [1, 2, 2, 1], name='stage1')\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', train_flag=train_flag)  # 结构块。\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_flag=train_flag)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_flag=train_flag, stride2=True)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_flag=train_flag)\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_flag=train_flag)\n",
        "    x=tf.layers.dropout(x,rate=0.5,training=ttf)#加入dropout\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_flag=train_flag)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_flag=train_flag, stride2=True)\n",
        "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  # block_count=22，即，下句for循环的range是0~22，正好是加23个层。\n",
        "    for i in range(block_count):  # 加22个层，都是用identity_block函数加。\n",
        "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_flag=train_flag)\n",
        "    C4 = x  #\n",
        "    # Stage 5\n",
        "    if stage5:\n",
        "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_flag=train_flag, stride2=True)\n",
        "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_flag=train_flag)\n",
        "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_flag=train_flag)\n",
        "        output=avg_pool(C5,1)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)#加入dropout\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "    else:\n",
        "        C5 = None\n",
        "        output=avg_pool(C4,2)\n",
        "        output=tf.layers.dropout(output,rate=0.5,training=ttf)\n",
        "        fo= layers.Dense(10)\n",
        "\n",
        "        output=tf.layers.flatten(output)\n",
        "        output = fo(output)\n",
        "\n",
        "    return [C1, C2, C3, C4, C5,output]\n",
        "\n",
        "\n",
        "\n",
        "filepath='C:/Users/zy109/Desktop/1/data/cifar-10-batches-py'\n",
        "model_save_path='C:/Users/zy109/Desktop/1/cifar10'\n",
        "data=Cifar10DataReader()\n",
        "sess=tf.InteractiveSession()\n",
        "\n",
        "#指数衰减学习率\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(learning_rate=0.001, global_step=global_step, decay_steps=50, decay_rate=0.99, staircase=False)\n",
        "\n",
        "#占位符\n",
        "input_image=tf.placeholder(tf.float32,[None,32,32,3])\n",
        "y_=tf.placeholder(tf.float32,[None,10])\n",
        "tf_is_train=tf.placeholder(tf.bool,None)\n",
        "input_image=data_enhace(input_image)#数据增强\n",
        "input_image,y_=criterion(input_image,y_,batch_size)#mixup\n",
        "\n",
        "[C1,C2,C3,C4,C5,output]=resnet_graph(input_image,'resnet50',ttf=tf_is_train)\n",
        "\n",
        "#交叉熵损失\n",
        "cross_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_,logits=output))\n",
        "\n",
        "tf.add_to_collection('losses',cross_loss)\n",
        "loss=tf.add_n(tf.get_collection('losses'))\n",
        "train_step=tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "\n",
        "#正确率\n",
        "correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y_,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "sess.run(tf.initialize_all_variables())\n",
        "saver = tf.train.Saver()\n",
        "#读取已经训练好的权重\n",
        "ckpt = tf.train.get_checkpoint_state(model_save_path)\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "#加载测试集数据\n",
        "test_data=data.all_test_data()\n",
        "x_test=np.array(test_data[0])/255\n",
        "y_test=np.array(test_data[1])\n",
        "\n",
        "total_loss=[]\n",
        "total_acc=[]\n",
        "total_step=[]\n",
        "\n",
        "\n",
        "# 训练\n",
        "for i in range(20000):\n",
        "\n",
        "    #逐批加载训练集数据\n",
        "    train_data=data.next_train_data(batch_size)\n",
        "    x_train=np.array(train_data[0])/255\n",
        "    y_train=np.array(train_data[1])\n",
        "\n",
        "    train_step.run(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        # 测试准确率\n",
        "        train_accuracy = accuracy.eval(feed_dict={input_image: x_train,y_: y_train,tf_is_train:False})\n",
        "        test_accuracy = accuracy.eval(feed_dict={input_image: x_test,y_: y_test,tf_is_train:False})\n",
        "        # 该次训练的损失\n",
        "        loss_value = loss.eval(feed_dict = {input_image: x_train,\n",
        "                                    y_: y_train,tf_is_train:True})\n",
        "\n",
        "        global_var=sess.run(global_step)\n",
        "        lr_val=sess.run(lr)\n",
        "        print(\"step %d, trainning accuracy， %g , testing accuracy， %g , loss %g  ,lr %g\" % (global_var, train_accuracy,test_accuracy, loss_value,lr_val))\n",
        "        total_step.append(i)\n",
        "        total_loss.append(loss_value)\n",
        "        total_acc.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "save_path = saver.save(sess,\"./cifar10/model.ckpt\")\n",
        "print(\"save model:{0} Finished\".format(save_path))\n",
        "np.save(\"cifar10_step\",total_step)\n",
        "np.save(\"cifar10_loss\",total_loss)\n",
        "np.save(\"cifar10_acc\",total_acc)\n",
        "\n",
        "#测试\n",
        "y_log_predict=output.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "test_accuracy = accuracy.eval(feed_dict = {input_image: x_test, y_: y_test,tf_is_train:False})\n",
        "print(\"test accuracy %g\" % test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test.argmax(axis=1), y_log_predict.argmax(axis=1))\n",
        "\n",
        "\n",
        "for i in range(len(y_log_predict)):\n",
        "        max_value=max(y_log_predict[i])\n",
        "        for j in range(len(y_log_predict[i])):\n",
        "            if max_value==y_log_predict[i][j]:\n",
        "                y_log_predict[i][j]=1\n",
        "            else:\n",
        "                y_log_predict[i][j]=0\n",
        "\n",
        "# 精确率\n",
        "from sklearn.metrics import precision_score\n",
        "ps=precision_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_precision_score\",ps)\n",
        "\n",
        "#召回率\n",
        "from sklearn.metrics import recall_score\n",
        "rs=recall_score(y_test, y_log_predict,average=None)\n",
        "np.save(\"cifar10_recall_score\",rs)\n",
        "\n",
        "print(\"precision score:\",ps)\n",
        "print(\"recall score:\",rs)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "ax1.plot(total_step, total_loss, color=\"blue\", label=\"loss\")\n",
        "ax1.set_xlabel(\"step\")\n",
        "\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_step, total_acc, color=\"red\", label=\"accuary\")\n",
        "ax2.set_ylabel(\"accuary\")\n",
        "\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-1-648f0d8455e6>:249: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-648f0d8455e6>:280: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "step 1, trainning accuracy， 0.160156 , testing accuracy， 0.1232 , loss 80.9494  ,lr 0.000999799\n",
            "step 101, trainning accuracy， 0.351562 , testing accuracy， 0.324 , loss 50.8969  ,lr 0.000979903\n",
            "step 201, trainning accuracy， 0.410156 , testing accuracy， 0.4074 , loss 30.2323  ,lr 0.000960403\n",
            "step 301, trainning accuracy， 0.402344 , testing accuracy， 0.425 , loss 18.8853  ,lr 0.000941291\n",
            "step 401, trainning accuracy， 0.519531 , testing accuracy， 0.4524 , loss 13.3933  ,lr 0.000922559\n",
            "step 501, trainning accuracy， 0.457031 , testing accuracy， 0.4596 , loss 9.98305  ,lr 0.0009042\n",
            "step 601, trainning accuracy， 0.550781 , testing accuracy， 0.5223 , loss 7.67291  ,lr 0.000886207\n",
            "step 701, trainning accuracy， 0.554688 , testing accuracy， 0.5295 , loss 6.19726  ,lr 0.000868571\n",
            "step 801, trainning accuracy， 0.601562 , testing accuracy， 0.5648 , loss 5.32091  ,lr 0.000851287\n",
            "step 901, trainning accuracy， 0.609375 , testing accuracy， 0.5818 , loss 4.53854  ,lr 0.000834346\n",
            "step 1001, trainning accuracy， 0.621094 , testing accuracy， 0.5943 , loss 4.16466  ,lr 0.000817743\n",
            "step 1101, trainning accuracy， 0.613281 , testing accuracy， 0.6011 , loss 3.92599  ,lr 0.00080147\n",
            "step 1201, trainning accuracy， 0.640625 , testing accuracy， 0.6232 , loss 3.34669  ,lr 0.00078552\n",
            "step 1301, trainning accuracy， 0.695312 , testing accuracy， 0.629 , loss 3.08288  ,lr 0.000769889\n",
            "step 1401, trainning accuracy， 0.667969 , testing accuracy， 0.6152 , loss 2.91771  ,lr 0.000754568\n",
            "step 1501, trainning accuracy， 0.613281 , testing accuracy， 0.6287 , loss 2.85865  ,lr 0.000739552\n",
            "step 1601, trainning accuracy， 0.664062 , testing accuracy， 0.6424 , loss 2.52806  ,lr 0.000724835\n",
            "step 1701, trainning accuracy， 0.722656 , testing accuracy， 0.6681 , loss 2.258  ,lr 0.000710411\n",
            "step 1801, trainning accuracy， 0.714844 , testing accuracy， 0.6708 , loss 2.26822  ,lr 0.000696274\n",
            "step 1901, trainning accuracy， 0.742188 , testing accuracy， 0.6728 , loss 2.1624  ,lr 0.000682418\n",
            "step 2001, trainning accuracy， 0.777344 , testing accuracy， 0.6802 , loss 1.98634  ,lr 0.000668838\n",
            "step 2101, trainning accuracy， 0.699219 , testing accuracy， 0.6702 , loss 2.29619  ,lr 0.000655528\n",
            "step 2201, trainning accuracy， 0.734375 , testing accuracy， 0.6926 , loss 1.9645  ,lr 0.000642483\n",
            "step 2301, trainning accuracy， 0.753906 , testing accuracy， 0.7107 , loss 1.86033  ,lr 0.000629697\n",
            "step 2401, trainning accuracy， 0.714844 , testing accuracy， 0.6869 , loss 2.00123  ,lr 0.000617166\n",
            "step 2501, trainning accuracy， 0.746094 , testing accuracy， 0.7051 , loss 1.80786  ,lr 0.000604885\n",
            "step 2601, trainning accuracy， 0.667969 , testing accuracy， 0.63 , loss 2.29604  ,lr 0.000592848\n",
            "step 2701, trainning accuracy， 0.710938 , testing accuracy， 0.6843 , loss 1.90744  ,lr 0.00058105\n",
            "step 2801, trainning accuracy， 0.761719 , testing accuracy， 0.7054 , loss 1.79034  ,lr 0.000569487\n",
            "step 2901, trainning accuracy， 0.742188 , testing accuracy， 0.7104 , loss 1.84531  ,lr 0.000558154\n",
            "step 3001, trainning accuracy， 0.714844 , testing accuracy， 0.6849 , loss 2.03728  ,lr 0.000547047\n",
            "step 3101, trainning accuracy， 0.730469 , testing accuracy， 0.6549 , loss 2.16065  ,lr 0.000536161\n",
            "step 3201, trainning accuracy， 0.746094 , testing accuracy， 0.6901 , loss 1.89051  ,lr 0.000525491\n",
            "step 3301, trainning accuracy， 0.714844 , testing accuracy， 0.7042 , loss 1.86611  ,lr 0.000515034\n",
            "step 3401, trainning accuracy， 0.789062 , testing accuracy， 0.7238 , loss 1.60102  ,lr 0.000504785\n",
            "step 3501, trainning accuracy， 0.753906 , testing accuracy， 0.7354 , loss 1.56652  ,lr 0.00049474\n",
            "step 3601, trainning accuracy， 0.761719 , testing accuracy， 0.7076 , loss 1.69859  ,lr 0.000484894\n",
            "step 3701, trainning accuracy， 0.738281 , testing accuracy， 0.6885 , loss 1.80982  ,lr 0.000475245\n",
            "step 3801, trainning accuracy， 0.769531 , testing accuracy， 0.7393 , loss 1.60366  ,lr 0.000465788\n",
            "step 3901, trainning accuracy， 0.789062 , testing accuracy， 0.745 , loss 1.40991  ,lr 0.000456518\n",
            "step 4001, trainning accuracy， 0.828125 , testing accuracy， 0.7539 , loss 1.31476  ,lr 0.000447434\n",
            "step 4101, trainning accuracy， 0.84375 , testing accuracy， 0.7498 , loss 1.19164  ,lr 0.00043853\n",
            "step 4201, trainning accuracy， 0.777344 , testing accuracy， 0.7279 , loss 1.48745  ,lr 0.000429803\n",
            "step 4301, trainning accuracy， 0.847656 , testing accuracy， 0.7448 , loss 1.1899  ,lr 0.00042125\n",
            "step 4401, trainning accuracy， 0.832031 , testing accuracy， 0.7598 , loss 1.27979  ,lr 0.000412867\n",
            "step 4501, trainning accuracy， 0.59375 , testing accuracy， 0.5861 , loss 2.25606  ,lr 0.000404651\n",
            "step 4601, trainning accuracy， 0.734375 , testing accuracy， 0.7029 , loss 1.84219  ,lr 0.000396598\n",
            "step 4701, trainning accuracy， 0.835938 , testing accuracy， 0.7472 , loss 1.35411  ,lr 0.000388706\n",
            "step 4801, trainning accuracy， 0.71875 , testing accuracy， 0.6641 , loss 1.76447  ,lr 0.000380971\n",
            "step 4901, trainning accuracy， 0.839844 , testing accuracy， 0.7407 , loss 1.3905  ,lr 0.00037339\n",
            "step 5001, trainning accuracy， 0.808594 , testing accuracy， 0.7482 , loss 1.39718  ,lr 0.000365959\n",
            "step 5101, trainning accuracy， 0.8125 , testing accuracy， 0.76 , loss 1.30865  ,lr 0.000358677\n",
            "step 5201, trainning accuracy， 0.820312 , testing accuracy， 0.7626 , loss 1.15604  ,lr 0.000351539\n",
            "step 5301, trainning accuracy， 0.855469 , testing accuracy， 0.755 , loss 1.15778  ,lr 0.000344543\n",
            "step 5401, trainning accuracy， 0.835938 , testing accuracy， 0.774 , loss 1.1922  ,lr 0.000337687\n",
            "step 5501, trainning accuracy， 0.859375 , testing accuracy， 0.7738 , loss 1.11806  ,lr 0.000330967\n",
            "step 5601, trainning accuracy， 0.578125 , testing accuracy， 0.5315 , loss 2.13337  ,lr 0.000324381\n",
            "step 5701, trainning accuracy， 0.765625 , testing accuracy， 0.6954 , loss 1.67856  ,lr 0.000317926\n",
            "step 5801, trainning accuracy， 0.796875 , testing accuracy， 0.7286 , loss 1.45646  ,lr 0.000311599\n",
            "step 5901, trainning accuracy， 0.792969 , testing accuracy， 0.7455 , loss 1.5123  ,lr 0.000305398\n",
            "step 6001, trainning accuracy， 0.785156 , testing accuracy， 0.7561 , loss 1.39049  ,lr 0.000299321\n",
            "step 6101, trainning accuracy， 0.839844 , testing accuracy， 0.7625 , loss 1.41487  ,lr 0.000293364\n",
            "step 6201, trainning accuracy， 0.789062 , testing accuracy， 0.7691 , loss 1.29029  ,lr 0.000287526\n",
            "step 6301, trainning accuracy， 0.871094 , testing accuracy， 0.7702 , loss 1.15738  ,lr 0.000281804\n",
            "step 6401, trainning accuracy， 0.859375 , testing accuracy， 0.7701 , loss 1.13285  ,lr 0.000276196\n",
            "step 6501, trainning accuracy， 0.863281 , testing accuracy， 0.7717 , loss 1.13326  ,lr 0.0002707\n",
            "step 6601, trainning accuracy， 0.90625 , testing accuracy， 0.7776 , loss 1.0063  ,lr 0.000265313\n",
            "step 6701, trainning accuracy， 0.875 , testing accuracy， 0.7792 , loss 1.07629  ,lr 0.000260034\n",
            "step 6801, trainning accuracy， 0.871094 , testing accuracy， 0.7752 , loss 1.05636  ,lr 0.000254859\n",
            "step 6901, trainning accuracy， 0.910156 , testing accuracy， 0.7799 , loss 0.914037  ,lr 0.000249787\n",
            "step 7001, trainning accuracy， 0.886719 , testing accuracy， 0.7786 , loss 0.997586  ,lr 0.000244816\n",
            "step 7101, trainning accuracy， 0.882812 , testing accuracy， 0.7793 , loss 1.03698  ,lr 0.000239945\n",
            "step 7201, trainning accuracy， 0.921875 , testing accuracy， 0.7733 , loss 0.899686  ,lr 0.00023517\n",
            "step 7301, trainning accuracy， 0.921875 , testing accuracy， 0.7778 , loss 0.860182  ,lr 0.00023049\n",
            "step 7401, trainning accuracy， 0.917969 , testing accuracy， 0.7793 , loss 0.899366  ,lr 0.000225903\n",
            "step 7501, trainning accuracy， 0.902344 , testing accuracy， 0.7799 , loss 0.885725  ,lr 0.000221408\n",
            "step 7601, trainning accuracy， 0.910156 , testing accuracy， 0.7782 , loss 0.817566  ,lr 0.000217002\n",
            "step 7701, trainning accuracy， 0.925781 , testing accuracy， 0.7824 , loss 0.832422  ,lr 0.000212683\n",
            "step 7801, trainning accuracy， 0.941406 , testing accuracy， 0.7825 , loss 0.789421  ,lr 0.000208451\n",
            "step 7901, trainning accuracy， 0.957031 , testing accuracy， 0.7866 , loss 0.730585  ,lr 0.000204303\n",
            "step 8001, trainning accuracy， 0.953125 , testing accuracy， 0.7869 , loss 0.721524  ,lr 0.000200237\n",
            "step 8101, trainning accuracy， 0.914062 , testing accuracy， 0.7796 , loss 0.800153  ,lr 0.000196252\n",
            "step 8201, trainning accuracy， 0.949219 , testing accuracy， 0.7879 , loss 0.730777  ,lr 0.000192347\n",
            "step 8301, trainning accuracy， 0.957031 , testing accuracy， 0.7863 , loss 0.668004  ,lr 0.000188519\n",
            "step 8401, trainning accuracy， 0.957031 , testing accuracy， 0.784 , loss 0.759646  ,lr 0.000184768\n",
            "step 8501, trainning accuracy， 0.953125 , testing accuracy， 0.7845 , loss 0.693267  ,lr 0.000181091\n",
            "step 8601, trainning accuracy， 0.976562 , testing accuracy， 0.787 , loss 0.609841  ,lr 0.000177487\n",
            "step 8701, trainning accuracy， 0.949219 , testing accuracy， 0.7877 , loss 0.698694  ,lr 0.000173955\n",
            "step 8801, trainning accuracy， 0.972656 , testing accuracy， 0.7847 , loss 0.654394  ,lr 0.000170493\n",
            "step 8901, trainning accuracy， 0.976562 , testing accuracy， 0.788 , loss 0.628742  ,lr 0.000167101\n",
            "step 9001, trainning accuracy， 0.988281 , testing accuracy， 0.783 , loss 0.606934  ,lr 0.000163775\n",
            "step 9101, trainning accuracy， 0.957031 , testing accuracy， 0.7841 , loss 0.679589  ,lr 0.000160516\n",
            "step 9201, trainning accuracy， 0.96875 , testing accuracy， 0.7844 , loss 0.679705  ,lr 0.000157322\n",
            "step 9301, trainning accuracy， 0.980469 , testing accuracy， 0.7864 , loss 0.602973  ,lr 0.000154191\n",
            "step 9401, trainning accuracy， 0.976562 , testing accuracy， 0.7846 , loss 0.549697  ,lr 0.000151123\n",
            "step 9501, trainning accuracy， 0.972656 , testing accuracy， 0.7857 , loss 0.606274  ,lr 0.000148115\n",
            "step 9601, trainning accuracy， 0.980469 , testing accuracy， 0.7857 , loss 0.602104  ,lr 0.000145168\n",
            "step 9701, trainning accuracy， 0.953125 , testing accuracy， 0.787 , loss 0.625122  ,lr 0.000142279\n",
            "step 9801, trainning accuracy， 0.992188 , testing accuracy， 0.7868 , loss 0.583083  ,lr 0.000139448\n",
            "step 9901, trainning accuracy， 0.992188 , testing accuracy， 0.7847 , loss 0.565457  ,lr 0.000136673\n",
            "step 10001, trainning accuracy， 0.980469 , testing accuracy， 0.7872 , loss 0.562542  ,lr 0.000133953\n",
            "step 10101, trainning accuracy， 0.96875 , testing accuracy， 0.793 , loss 0.578956  ,lr 0.000131287\n",
            "step 10201, trainning accuracy， 0.992188 , testing accuracy， 0.7884 , loss 0.55298  ,lr 0.000128675\n",
            "step 10301, trainning accuracy， 0.996094 , testing accuracy， 0.7841 , loss 0.502042  ,lr 0.000126114\n",
            "step 10401, trainning accuracy， 0.992188 , testing accuracy， 0.7863 , loss 0.523734  ,lr 0.000123604\n",
            "step 10501, trainning accuracy， 0.988281 , testing accuracy， 0.7869 , loss 0.556616  ,lr 0.000121145\n",
            "step 10601, trainning accuracy， 0.996094 , testing accuracy， 0.7891 , loss 0.487493  ,lr 0.000118734\n",
            "step 10701, trainning accuracy， 0.984375 , testing accuracy， 0.7899 , loss 0.47048  ,lr 0.000116371\n",
            "step 10801, trainning accuracy， 1 , testing accuracy， 0.7824 , loss 0.472159  ,lr 0.000114055\n",
            "step 10901, trainning accuracy， 0.984375 , testing accuracy， 0.7884 , loss 0.514913  ,lr 0.000111786\n",
            "step 11001, trainning accuracy， 0.992188 , testing accuracy， 0.7887 , loss 0.49457  ,lr 0.000109561\n",
            "step 11101, trainning accuracy， 1 , testing accuracy， 0.7834 , loss 0.482442  ,lr 0.000107381\n",
            "step 11201, trainning accuracy， 0.996094 , testing accuracy， 0.7857 , loss 0.448309  ,lr 0.000105244\n",
            "step 11301, trainning accuracy， 0.996094 , testing accuracy， 0.7864 , loss 0.482768  ,lr 0.00010315\n",
            "step 11401, trainning accuracy， 1 , testing accuracy， 0.7903 , loss 0.436235  ,lr 0.000101097\n",
            "step 11501, trainning accuracy， 1 , testing accuracy， 0.787 , loss 0.468244  ,lr 9.90851e-05\n",
            "step 11601, trainning accuracy， 1 , testing accuracy， 0.7875 , loss 0.47119  ,lr 9.71133e-05\n",
            "step 11701, trainning accuracy， 0.996094 , testing accuracy， 0.7946 , loss 0.448293  ,lr 9.51808e-05\n",
            "step 11801, trainning accuracy， 1 , testing accuracy， 0.794 , loss 0.424347  ,lr 9.32867e-05\n",
            "step 11901, trainning accuracy， 1 , testing accuracy， 0.7881 , loss 0.41508  ,lr 9.14303e-05\n",
            "step 12001, trainning accuracy， 0.996094 , testing accuracy， 0.7887 , loss 0.434446  ,lr 8.96108e-05\n",
            "step 12101, trainning accuracy， 1 , testing accuracy， 0.791 , loss 0.42031  ,lr 8.78276e-05\n",
            "step 12201, trainning accuracy， 1 , testing accuracy， 0.7881 , loss 0.419997  ,lr 8.60798e-05\n",
            "step 12301, trainning accuracy， 1 , testing accuracy， 0.792 , loss 0.398076  ,lr 8.43668e-05\n",
            "step 12401, trainning accuracy， 0.988281 , testing accuracy， 0.7916 , loss 0.46689  ,lr 8.26879e-05\n",
            "step 12501, trainning accuracy， 1 , testing accuracy， 0.7883 , loss 0.432648  ,lr 8.10424e-05\n",
            "step 12601, trainning accuracy， 1 , testing accuracy， 0.7886 , loss 0.394597  ,lr 7.94297e-05\n",
            "step 12701, trainning accuracy， 0.996094 , testing accuracy， 0.7891 , loss 0.406605  ,lr 7.7849e-05\n",
            "step 12801, trainning accuracy， 1 , testing accuracy， 0.7869 , loss 0.402018  ,lr 7.62998e-05\n",
            "step 12901, trainning accuracy， 0.992188 , testing accuracy， 0.7908 , loss 0.425049  ,lr 7.47815e-05\n",
            "step 13001, trainning accuracy， 0.996094 , testing accuracy， 0.7877 , loss 0.397754  ,lr 7.32933e-05\n",
            "step 13101, trainning accuracy， 1 , testing accuracy， 0.792 , loss 0.403176  ,lr 7.18348e-05\n",
            "step 13201, trainning accuracy， 0.996094 , testing accuracy， 0.7861 , loss 0.390585  ,lr 7.04053e-05\n",
            "step 13301, trainning accuracy， 1 , testing accuracy， 0.7899 , loss 0.388537  ,lr 6.90042e-05\n",
            "step 13401, trainning accuracy， 1 , testing accuracy， 0.7909 , loss 0.399769  ,lr 6.7631e-05\n",
            "step 13501, trainning accuracy， 1 , testing accuracy， 0.7913 , loss 0.390719  ,lr 6.62852e-05\n",
            "step 13601, trainning accuracy， 0.996094 , testing accuracy， 0.7855 , loss 0.385218  ,lr 6.49661e-05\n",
            "step 13701, trainning accuracy， 1 , testing accuracy， 0.7894 , loss 0.376235  ,lr 6.36733e-05\n",
            "step 13801, trainning accuracy， 1 , testing accuracy， 0.7908 , loss 0.378782  ,lr 6.24062e-05\n",
            "step 13901, trainning accuracy， 1 , testing accuracy， 0.7922 , loss 0.35914  ,lr 6.11643e-05\n",
            "step 14001, trainning accuracy， 1 , testing accuracy， 0.7905 , loss 0.364562  ,lr 5.99471e-05\n",
            "step 14101, trainning accuracy， 1 , testing accuracy， 0.7895 , loss 0.365347  ,lr 5.87542e-05\n",
            "step 14201, trainning accuracy， 0.996094 , testing accuracy， 0.7936 , loss 0.371799  ,lr 5.7585e-05\n",
            "step 14301, trainning accuracy， 1 , testing accuracy， 0.7895 , loss 0.370091  ,lr 5.6439e-05\n",
            "step 14401, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.355425  ,lr 5.53159e-05\n",
            "step 14501, trainning accuracy， 1 , testing accuracy， 0.7905 , loss 0.357128  ,lr 5.42151e-05\n",
            "step 14601, trainning accuracy， 1 , testing accuracy， 0.7916 , loss 0.366582  ,lr 5.31362e-05\n",
            "step 14701, trainning accuracy， 1 , testing accuracy， 0.7899 , loss 0.34536  ,lr 5.20788e-05\n",
            "step 14801, trainning accuracy， 1 , testing accuracy， 0.7908 , loss 0.372556  ,lr 5.10425e-05\n",
            "step 14901, trainning accuracy， 1 , testing accuracy， 0.7921 , loss 0.356744  ,lr 5.00267e-05\n",
            "step 15001, trainning accuracy， 1 , testing accuracy， 0.7889 , loss 0.362499  ,lr 4.90312e-05\n",
            "step 15101, trainning accuracy， 1 , testing accuracy， 0.7887 , loss 0.355861  ,lr 4.80555e-05\n",
            "step 15201, trainning accuracy， 1 , testing accuracy， 0.7874 , loss 0.359275  ,lr 4.70992e-05\n",
            "step 15301, trainning accuracy， 1 , testing accuracy， 0.7895 , loss 0.332877  ,lr 4.61619e-05\n",
            "step 15401, trainning accuracy， 1 , testing accuracy， 0.7939 , loss 0.359973  ,lr 4.52433e-05\n",
            "step 15501, trainning accuracy， 1 , testing accuracy， 0.7917 , loss 0.335086  ,lr 4.43429e-05\n",
            "step 15601, trainning accuracy， 1 , testing accuracy， 0.7902 , loss 0.332232  ,lr 4.34605e-05\n",
            "step 15701, trainning accuracy， 1 , testing accuracy， 0.7892 , loss 0.335699  ,lr 4.25956e-05\n",
            "step 15801, trainning accuracy， 1 , testing accuracy， 0.7893 , loss 0.330327  ,lr 4.1748e-05\n",
            "step 15901, trainning accuracy， 0.996094 , testing accuracy， 0.7907 , loss 0.331929  ,lr 4.09172e-05\n",
            "step 16001, trainning accuracy， 0.996094 , testing accuracy， 0.7909 , loss 0.347259  ,lr 4.0103e-05\n",
            "step 16101, trainning accuracy， 0.996094 , testing accuracy， 0.7917 , loss 0.345061  ,lr 3.93049e-05\n",
            "step 16201, trainning accuracy， 1 , testing accuracy， 0.7901 , loss 0.32277  ,lr 3.85227e-05\n",
            "step 16301, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.319328  ,lr 3.77561e-05\n",
            "step 16401, trainning accuracy， 1 , testing accuracy， 0.7945 , loss 0.343942  ,lr 3.70048e-05\n",
            "step 16501, trainning accuracy， 1 , testing accuracy， 0.7944 , loss 0.332989  ,lr 3.62684e-05\n",
            "step 16601, trainning accuracy， 1 , testing accuracy， 0.7898 , loss 0.313526  ,lr 3.55467e-05\n",
            "step 16701, trainning accuracy， 1 , testing accuracy， 0.7902 , loss 0.331331  ,lr 3.48393e-05\n",
            "step 16801, trainning accuracy， 1 , testing accuracy， 0.7929 , loss 0.315347  ,lr 3.4146e-05\n",
            "step 16901, trainning accuracy， 1 , testing accuracy， 0.789 , loss 0.324255  ,lr 3.34665e-05\n",
            "step 17001, trainning accuracy， 1 , testing accuracy， 0.7902 , loss 0.307821  ,lr 3.28005e-05\n",
            "step 17101, trainning accuracy， 1 , testing accuracy， 0.7952 , loss 0.310602  ,lr 3.21478e-05\n",
            "step 17201, trainning accuracy， 1 , testing accuracy， 0.7943 , loss 0.315291  ,lr 3.1508e-05\n",
            "step 17301, trainning accuracy， 1 , testing accuracy， 0.7911 , loss 0.305976  ,lr 3.0881e-05\n",
            "step 17401, trainning accuracy， 1 , testing accuracy， 0.7938 , loss 0.306672  ,lr 3.02665e-05\n",
            "step 17501, trainning accuracy， 1 , testing accuracy， 0.7938 , loss 0.309416  ,lr 2.96642e-05\n",
            "step 17601, trainning accuracy， 1 , testing accuracy， 0.7954 , loss 0.30327  ,lr 2.90739e-05\n",
            "step 17701, trainning accuracy， 1 , testing accuracy， 0.7939 , loss 0.300963  ,lr 2.84953e-05\n",
            "step 17801, trainning accuracy， 1 , testing accuracy， 0.7953 , loss 0.314641  ,lr 2.79282e-05\n",
            "step 17901, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.299279  ,lr 2.73725e-05\n",
            "step 18001, trainning accuracy， 1 , testing accuracy， 0.7953 , loss 0.301546  ,lr 2.68278e-05\n",
            "step 18101, trainning accuracy， 1 , testing accuracy， 0.7933 , loss 0.305293  ,lr 2.62939e-05\n",
            "step 18201, trainning accuracy， 1 , testing accuracy， 0.7943 , loss 0.294731  ,lr 2.57706e-05\n",
            "step 18301, trainning accuracy， 1 , testing accuracy， 0.7953 , loss 0.325117  ,lr 2.52578e-05\n",
            "step 18401, trainning accuracy， 1 , testing accuracy， 0.7937 , loss 0.298575  ,lr 2.47552e-05\n",
            "step 18501, trainning accuracy， 1 , testing accuracy， 0.7943 , loss 0.301606  ,lr 2.42625e-05\n",
            "step 18601, trainning accuracy， 1 , testing accuracy， 0.7928 , loss 0.293253  ,lr 2.37797e-05\n",
            "step 18701, trainning accuracy， 1 , testing accuracy， 0.7932 , loss 0.306508  ,lr 2.33065e-05\n",
            "step 18801, trainning accuracy， 1 , testing accuracy， 0.7924 , loss 0.318978  ,lr 2.28427e-05\n",
            "step 18901, trainning accuracy， 1 , testing accuracy， 0.7909 , loss 0.293668  ,lr 2.23881e-05\n",
            "step 19001, trainning accuracy， 1 , testing accuracy， 0.7936 , loss 0.299605  ,lr 2.19426e-05\n",
            "step 19101, trainning accuracy， 1 , testing accuracy， 0.7936 , loss 0.29226  ,lr 2.1506e-05\n",
            "step 19201, trainning accuracy， 1 , testing accuracy， 0.7935 , loss 0.297171  ,lr 2.1078e-05\n",
            "step 19301, trainning accuracy， 1 , testing accuracy， 0.7946 , loss 0.289475  ,lr 2.06585e-05\n",
            "step 19401, trainning accuracy， 1 , testing accuracy， 0.7975 , loss 0.288612  ,lr 2.02474e-05\n",
            "step 19501, trainning accuracy， 1 , testing accuracy， 0.7983 , loss 0.288342  ,lr 1.98445e-05\n",
            "step 19601, trainning accuracy， 1 , testing accuracy， 0.7979 , loss 0.2849  ,lr 1.94496e-05\n",
            "step 19701, trainning accuracy， 1 , testing accuracy， 0.7977 , loss 0.283173  ,lr 1.90626e-05\n",
            "step 19801, trainning accuracy， 1 , testing accuracy， 0.7938 , loss 0.283421  ,lr 1.86832e-05\n",
            "step 19901, trainning accuracy， 1 , testing accuracy， 0.7959 , loss 0.281515  ,lr 1.83114e-05\n",
            "save model:./cifar10/model.ckpt Finished\n",
            "test accuracy 0.7959\n",
            "precision score: [0.77932961 0.88306452 0.74814815 0.62921348 0.78134403 0.72204807\n",
            " 0.84174085 0.81267739 0.88294652 0.87061184]\n",
            "recall score: [0.837 0.876 0.707 0.616 0.779 0.691 0.851 0.859 0.875 0.868]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}